import json
from typing import Any, Dict, List, Tuple, Optional
import ulid
from datetime import datetime
from zoneinfo import ZoneInfo

from src.state_of_mind.config import config
from src.state_of_mind.utils.constants import PARALLEL, SERIAL, PREPROCESSING, CATEGORY_RAW, SuggestionType
from src.state_of_mind.utils.ip_timezone import IPBasedTimezoneResolver
from src.state_of_mind.utils.logger import LoggerManager as logger
from src.state_of_mind.utils.network import get_public_ip
from static.prompts.prompt import LLM_PROMPTS_SCHEMA


class Prompter:
    """
    Prompt æ„é€ å™¨
    """
    CHINESE_NAME = "Promptæ„é€ å™¨"

    def build_raw(self, template_name: str, **template_vars: Any) -> Dict[str, Any]:
        """
        æ„å»º Prompt å¹¶è¿”å›å®Œæ•´ä¸Šä¸‹æ–‡æ•°æ®
        """
        logger.info("ğŸ”„ å¼€å§‹æ„å»º build_raw Prompt", module_name=self.CHINESE_NAME)

        # 1. éªŒè¯è¾“å…¥
        if "user_input" not in template_vars:
            error_msg = "ç¼ºå¤±å¿…éœ€å­—æ®µ: user_input"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        user_input = template_vars["user_input"]
        llm_model = template_vars["llm_model"]

        # 2. è·å–æ¨¡æ¿å®šä¹‰
        raw_schema = LLM_PROMPTS_SCHEMA.get(template_name)
        if not raw_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {template_name}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        schema_version = raw_schema.get("version")
        core_iron_law = raw_schema.get("core_iron_law")
        pipeline = raw_schema.get("pipeline")

        # 3. ä¸‰è·¯åˆ†ç¦» pipeline
        preprocessing_steps, parallel_steps, serial_steps = Prompter._split_pipeline(pipeline)

        # 4. æ„é€ ä¸‰ç±» prompts
        preprocessing_prompts = Prompter._build_step_prompts(
            steps=preprocessing_steps,
            core_iron_law=core_iron_law,
            step_type=PREPROCESSING
        )

        parallel_prompts = Prompter._build_step_prompts(
            steps=parallel_steps,
            core_iron_law=core_iron_law,
            step_type=PARALLEL
        )

        serial_prompts = Prompter._build_step_prompts(
            steps=serial_steps,
            core_iron_law=core_iron_law,
            step_type=SERIAL
        )

        # 5. ç”ŸæˆåŸºç¡€å…ƒæ•°æ®
        basic_data = Prompter.create_raw_basic_data(user_input, llm_model, schema_version)

        # 6. è®°å½•å®Œæˆæ—¥å¿—
        logger.info(
            f"âœ… Prompt æ„å»ºå®Œæˆ, preprocessing_count = {len(preprocessing_prompts)} | "
            f"parallel_count = {len(parallel_prompts)} | serial_count = {len(serial_prompts)} | "
            f"record_id = {basic_data['id']}",
            module_name=Prompter.CHINESE_NAME
        )

        # âœ… è¿”å›å®Œæ•´ç»“æ„ï¼Œä¾¿äºä¸Šå±‚ç»„è£…
        return {
            "template_name": template_name,
            "preprocessing_prompts": preprocessing_prompts,  # æ–°å¢
            "parallel_prompts": parallel_prompts,
            "serial_prompts": serial_prompts,
            "basic_data": basic_data
        }

    def build_suggestion(self, template_name: str, user_input: str, suggestion_type: str) -> str:
        logger.info("ğŸ”„ å¼€å§‹æ„å»º build_suggestion Prompt", module_name=self.CHINESE_NAME)

        suggestion_schema = LLM_PROMPTS_SCHEMA.get(template_name)
        if not suggestion_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {template_name}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        # âœ… ä½¿ç”¨ SuggestionType å®šä¹‰çš„åˆæ³•ç±»å‹åšæ ¡éªŒ
        valid_types = {
            SuggestionType.COMMON_SUGGESTION,
            SuggestionType.CONSISTENCY_SUGGESTION
        }

        if suggestion_type not in valid_types:
            error_msg = f"ä¸æ”¯æŒçš„å»ºè®®ç±»å‹: '{suggestion_type}'ã€‚å¯ç”¨ç±»å‹: {sorted(valid_types)}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        prompt_template = suggestion_schema.get(suggestion_type)
        if not prompt_template:
            error_msg = f"æ¨¡æ¿ '{template_name}' ä¸­ç¼ºå°‘å»ºè®®ç±»å‹ '{suggestion_type}' çš„å®šä¹‰"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        try:
            final_prompt = prompt_template.format(user_input=user_input)
        except KeyError as e:
            error_msg = f"æ¨¡æ¿ä¸­åŒ…å«æœªæä¾›çš„å­—æ®µ: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        logger.info("âœ… build_suggestion Prompt æ„å»ºæˆåŠŸ", module_name=self.CHINESE_NAME)
        return final_prompt

    @staticmethod
    def _split_pipeline(pipeline: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        åˆ†ç¦» pipeline ä¸­çš„é¢„å¤„ç†ã€å¹¶è¡Œã€ä¸²è¡Œä»»åŠ¡
        è¿”å›: (preprocessing_steps, parallel_steps, serial_steps)
        """
        if not isinstance(pipeline, list):
            error_msg = "pipeline å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹"
            logger.error(error_msg)
            raise ValueError(error_msg)

        preprocessing = []
        parallel = []
        serial = []

        for idx, step in enumerate(pipeline):
            if not isinstance(step, dict):
                logger.warning(f"è·³è¿‡éæ³• pipeline æ­¥éª¤ï¼ˆéå­—å…¸ï¼‰: ç´¢å¼•={idx}")
                continue

            step_type = step.get("type", SERIAL)  # é»˜è®¤ä¸ºä¸²è¡Œ

            if step_type == PREPROCESSING:
                preprocessing.append(step)
            elif step_type == PARALLEL:
                parallel.append(step)
            else:
                serial.append(step)  # åŒ…æ‹¬ SERIAL å’Œ æœªçŸ¥ type éƒ½å½’ä¸ºä¸²è¡Œï¼ˆå®‰å…¨å…œåº•ï¼‰

        logger.info(
            f"ğŸ“Š pipeline ä¸‰è·¯åˆ†ç¦»å®Œæˆ, preprocessing_count = {len(preprocessing)} | parallel_count = {len(parallel)} | "
            f"serial_count = {len(serial)} | total_steps = {len(pipeline)}", module_name=Prompter.CHINESE_NAME
        )
        return preprocessing, parallel, serial

    @staticmethod
    def _build_step_prompts(
            steps: List[Dict],
            core_iron_law: str,
            step_type: str
    ) -> List[Tuple[str, str, str]]:
        """
        æ„å»ºæŒ‡å®šç±»å‹ï¼ˆå¹¶è¡Œ/ä¸²è¡Œï¼‰çš„ prompt åˆ—è¡¨ï¼Œè¿”å› (require_fields, full_prompt) å…ƒç»„åˆ—è¡¨
        """
        prompts_with_fields = []
        missing_fields = []

        for idx, step in enumerate(steps):
            try:
                step_name = step["step"]
                role = step["role"]
                sole_mission = step["sole_mission"]
                fields = step["fields"]
                driven_by = step.get("driven_by")
            except KeyError as e:
                field = e.args[0]
                missing_fields.append(f"æ­¥éª¤{idx}.{field}")
                continue

            fields_json = json.dumps(fields, ensure_ascii=False, indent=2)
            fields_escaped = fields_json.replace('{', '{{').replace('}', '}}')

            # æ„å»º promptï¼ˆä¸¥æ ¼æŒ‰ç…§ä½ ç»™çš„é¡ºåºï¼‰
            full_prompt = "\n".join([
                role.strip(),
                sole_mission.strip(),
                core_iron_law.strip(),
                fields_escaped.strip()
            ])

            prompts_with_fields.append((step_name, driven_by, full_prompt))

        if missing_fields:
            error_msg = f"{step_type} æ­¥éª¤ä¸­ç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}"
            logger.error(error_msg, module_name=Prompter.CHINESE_NAME)
            raise ValueError(error_msg)

        logger.info(f"ğŸ”§ å·²ç”Ÿæˆ {step_type} prompts æ•°é‡: {len(prompts_with_fields)}", module_name=Prompter.CHINESE_NAME)
        return prompts_with_fields

    @staticmethod
    def create_raw_basic_data(user_input: str, llm_model: str, schema_version: str = "1.0.0") -> Dict[str, Any]:
        """
        æ„é€ åŸå§‹äº‹ä»¶çš„å›ºå®šåŸºç¡€å…ƒæ•°æ®
        å¯ç”¨äºæ—¥å¿—è¿½è¸ªã€å®¡è®¡ã€æº¯æºç­‰
        """
        record_id = f"raw_{ulid.new().str}"

        public_ip = get_public_ip()
        tz_name = IPBasedTimezoneResolver.get_timezone_from_ip(public_ip) if public_ip else "UTC"

        if not public_ip:
            logger.warning("âš ï¸ æ— æ³•è·å–å…¬ç½‘IPï¼Œä½¿ç”¨ UTC æ—¶åŒº", module_name=Prompter.CHINESE_NAME)

        tz = ZoneInfo(tz_name)
        timestamp = datetime.now(tz).isoformat()

        formatter_time = ""
        try:
            dt = datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=ZoneInfo("UTC"))
            weekday = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"][dt.weekday()]
            base_time = dt.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]  # æ¯«ç§’éƒ¨åˆ†
            formatter_time = f"{base_time} {weekday}"
        except Exception as e:
            logger.warning(
                f"ğŸ•’ æ— æ³•è§£æ timestamp ä¸º formatter_time: {e}",
                module_name=Prompter.CHINESE_NAME,
                extra={"timestamp": timestamp}
            )

        data = {
            "id": record_id,
            "type": CATEGORY_RAW,
            "schema_version": schema_version,
            "timestamp": timestamp,
            "formatter_time": formatter_time,
            "source": {
                "modality": "text/narrative",
                "content": user_input,
                "input_mode": "user_input",
                # "local_ip": public_ip,
                "timezone": tz_name
            },
            "meta": {
                "library_version": config.VERSION,
                "created_by_ai": True,
                "llm_model": llm_model,
                "crystal_ids": [],
                "ontology_ids": [],
                "narrative_enriched": False,
                "privacy_scope": {
                    "allowed_modules": [],
                    "sync_to_cloud": False,
                    "notify_on_trigger": False,
                    "exportable": False
                }

            }
        }

        logger.info(f"ğŸ“¦ å·²ç”ŸæˆåŸºç¡€å…ƒæ•°æ®, id={record_id} | timezone={tz_name}", module_name=Prompter.CHINESE_NAME)
        return data

    @staticmethod
    def generate_description(context: dict, field_config: List[Tuple[str, bool, Any, str]], prefix="") -> str:
        def _is_effectively_empty(value) -> bool:
            if value is None:
                return True
            if isinstance(value, str) and not value.strip():
                return True
            if isinstance(value, (list, dict)) and len(value) == 0:
                return True
            return False

        def _format_simple_value(value):
            if isinstance(value, list):
                non_empty = [str(v) for v in value if not _is_effectively_empty(v)]
                return ", ".join(non_empty)
            return str(value)

        # é¢„å¤„ç†é€šé…è§„åˆ™ï¼šæå–æ‰€æœ‰ *. è·¯å¾„
        wildcard_rules = {}
        normal_rules = {}
        top_fields = []

        for path, required, typ, desc in field_config:
            if ".*." in path:
                prefix_path = path.split(".*.", 1)[0]  # å¦‚ "inference.events"
                field_name = path.split(".*.", 1)[1]  # å¦‚ "inference_type"
                if prefix_path not in wildcard_rules:
                    wildcard_rules[prefix_path] = []
                wildcard_rules[prefix_path].append((field_name, desc))
            elif "." not in path:
                top_fields.append((path, desc))
            else:
                normal_rules[path] = desc

        output_lines = []

        # å¦‚æœæ²¡æœ‰é¡¶å±‚å­—æ®µï¼Œfallback åˆ°å¹³é“ºæ¸²æŸ“
        if not top_fields:
            for path, desc in normal_rules.items():
                val = context.get(path)
                if not _is_effectively_empty(val):
                    output_lines.append(f"## {desc.rstrip('ï¼š:').strip()}")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")
            result = "\n".join(output_lines).strip()
            # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ï¼ˆæ— é¡¶å±‚ï¼‰:{result}", module_name=Prompter.CHINESE_NAME)
            return result

        # å¤„ç†æ¯ä¸ªé¡¶å±‚å­—æ®µï¼ˆæ”¯æŒå¤šä¸ªï¼‰
        for top_path, top_desc in top_fields:
            top_value = context.get(top_path)
            if _is_effectively_empty(top_value):
                continue

            clean_top_desc = top_desc.rstrip("ï¼š:").strip()
            output_lines.append(f"## {clean_top_desc}")

            if isinstance(top_value, dict):
                # æ¸²æŸ“å­—å…¸çš„æ¯ä¸ªå­å­—æ®µ
                for key, val in top_value.items():
                    if _is_effectively_empty(val):
                        continue
                    full_sub_path = f"{top_path}.{key}"
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ list[dict] ä¸”æœ‰é€šé…è§„åˆ™
                    if isinstance(val, list) and val and isinstance(val[0], dict):
                        if full_sub_path in wildcard_rules:
                            # è·å–è¯¥åˆ—è¡¨å­—æ®µçš„å®Œæ•´æè¿°ï¼ˆå¦‚ "eventsï¼ˆæ¨ç†äº‹ä»¶åˆ—è¡¨ï¼‰ï¼š"ï¼‰
                            list_desc = normal_rules.get(full_sub_path, f"{key}ï¼ˆåˆ—è¡¨ï¼‰ï¼š")
                            for item in val:
                                item_lines = []
                                for field_name, field_desc in wildcard_rules[full_sub_path]:
                                    item_val = item.get(field_name)
                                    if not _is_effectively_empty(item_val):
                                        item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                                if item_lines:
                                    output_lines.append(f"  - {list_desc}")
                                    output_lines.extend(item_lines)
                            continue  # å·²å¤„ç†ï¼Œè·³è¿‡é»˜è®¤é€»è¾‘

                    # é»˜è®¤ï¼šç®€å•æ ¼å¼åŒ–
                    desc = normal_rules.get(full_sub_path, f"{key}: ")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")

            elif isinstance(top_value, list):
                # é¡¶å±‚æ˜¯åˆ—è¡¨ï¼ˆå¦‚ participantsï¼‰
                if top_path in wildcard_rules:
                    for item in top_value:
                        if not isinstance(item, dict):
                            continue
                        item_lines = []
                        for field_name, field_desc in wildcard_rules[top_path]:
                            item_val = item.get(field_name)
                            if not _is_effectively_empty(item_val):
                                item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                        if item_lines:
                            output_lines.append("  - åˆ—è¡¨é¡¹ï¼š")
                            output_lines.extend(item_lines)
                else:
                    output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")
            else:
                output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")

        # æ¸…ç†ç©ºè¡Œ
        while output_lines and output_lines[-1] == "":
            output_lines.pop()

        result = "\n".join(output_lines).strip()
        # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡:{result}", module_name=Prompter.CHINESE_NAME)
        return result

    @staticmethod
    def extract_top_level_description(fields_spec: List[Tuple[str, bool, Any, str]]) -> Optional[str]:
        """
        ä»å­—æ®µè§„èŒƒåˆ—è¡¨ä¸­æå–é¡¶å±‚å­—æ®µï¼ˆè·¯å¾„ä¸­ä¸å« '.' çš„å­—æ®µï¼‰çš„æè¿°ã€‚
        è‹¥å­˜åœ¨å¤šä¸ªé¡¶å±‚å­—æ®µï¼ˆå¦‚ inference + context_clueï¼‰ï¼Œä¼˜å…ˆå–ç¬¬ä¸€ä¸ªéé€šé…ã€éåˆ—è¡¨é¡¹çš„ã€‚
        """
        for field_path, _, _, description in fields_spec:
            # è·³è¿‡å¸¦é€šé…ç¬¦çš„è·¯å¾„ï¼ˆå¦‚ participants.*.roleï¼‰
            if ".*." in field_path or field_path.startswith("*."):
                continue
            parts = field_path.split(".")
            if len(parts) == 1:
                # è¿™æ˜¯ä¸€ä¸ªé¡¶å±‚å­—æ®µï¼Œå¦‚ "participants", "inference", "context_clue"
                return description
        return None
