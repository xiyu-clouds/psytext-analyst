import asyncio
import time
import uuid
from copy import deepcopy
from pathlib import Path
from typing import List, Any, Tuple, Dict, Optional
from src.state_of_mind.utils.registry import GlobalSingletonRegistry
from src.state_of_mind.cache.base import BaseCache
from src.state_of_mind.cache.redis import RedisLLMCache
from .prompter import Prompter
from ..cache.llm_cache import LLMCache
from ..config import config
from ..utils.constants import ModelName, REQUIRED_FIELDS_BY_CATEGORY, LLM_SOURCE_EXTRACTION, \
    PREPROCESSING, PARALLEL, SERIAL, SEMANTIC_MODULES_L1, CATEGORY_SUGGESTION
from ..utils.file_util import FileUtil
from ..utils.timeUtil import timed
from src.state_of_mind.utils.logger import LoggerManager as logger


class MetaCognitiveEngine:
    CHINESE_NAME = "MetaCognitiveEngine"
    _init_lock = asyncio.Lock()  # ç”¨äºä¿æŠ¤å¹¶å‘åˆå§‹åŒ–

    def __init__(self, backend_name: str, llm_model: str, recommended_params: dict):
        # å‚æ•°æ ¡éªŒ
        if not backend_name:
            raise ValueError("backend_name ä¸èƒ½ä¸ºç©º")
        if not llm_model:
            raise ValueError("llm_model ä¸èƒ½ä¸ºç©º")
        if not isinstance(recommended_params, dict):
            raise TypeError("recommended_params å¿…é¡»æ˜¯å­—å…¸ç±»å‹")

        # æ ¡éªŒ backend_name æ˜¯å¦æ”¯æŒ
        if backend_name not in self.supported_backends():
            logger.warning(
                f"æœªçŸ¥çš„ backend_name: {backend_name}ï¼Œå½“å‰ä»…æ”¯æŒ: {self.supported_backends()}"
            )
            # å¯é€‰æ‹©æŠ›å‡ºå¼‚å¸¸æˆ–ç»§ç»­ï¼ˆå–å†³äºä½ çš„ç­–ç•¥ï¼‰
            raise ValueError(f"ä¸æ”¯æŒçš„ backend_name: {backend_name}")

        # æ ¡éªŒ llm_model æ˜¯å¦åœ¨ MODEL_CONFIG ä¸­å­˜åœ¨
        if llm_model not in self.supported_models():
            logger.warning(
                f"æœªçŸ¥çš„ llm_model: {llm_model}ï¼Œå½“å‰ä»…æ”¯æŒ: {self.supported_models()}"
            )
            # åŒæ ·ï¼Œå¯é€‰æ‹©æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"ä¸æ”¯æŒçš„ llm_model: {llm_model}")

        self.backend_name = backend_name
        self.llm_model = llm_model
        self.recommended_params = recommended_params
        self.prompter = Prompter()
        self.llm_cache = self._create_cache_backend(config)
        self.file_util = FileUtil()
        self._backend = None
        self._backend_configs = {"api_key": config.LLM_API_KEY, "timeout": 120} \
            if backend_name == ModelName.QWEN \
            else {"api_key": config.LLM_API_KEY, "base_url": config.LLM_API_URL, "timeout": 120}
        self._top_field_to_step_types = self._build_top_field_to_step_types()
        self._step_type_to_config = self._build_step_type_to_config()
        self.current_parallel_concurrency = config.get("CURRENT_PARALLEL_CONCURRENCY", 3)  # é»˜è®¤3
        self._parallel_semaphore = asyncio.Semaphore(self.current_parallel_concurrency)
        logger.info(
            f"MetaCognitiveEngine åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ backend: {backend_name}, model: {llm_model}"
        )

    @property
    async def backend(self):
        """æƒ°æ€§åŠ è½½ backend å®ä¾‹"""
        if self._backend is None:
            async with self._init_lock:
                if self._backend is None:  # double-check
                    logger.info("ğŸ”„ é¦–æ¬¡è·å– backendï¼Œæ­£åœ¨å¼‚æ­¥åˆå§‹åŒ–...")
                    self._backend = await GlobalSingletonRegistry.get_backend_async(
                        self.backend_name,
                        self._backend_configs
                    )
                    if self._backend is None:
                        raise RuntimeError(f"æ— æ³•è·å– backend: {self.backend_name}")
        return self._backend

    @classmethod
    def supported_backends(cls) -> set:
        return {"qwen", "deepseek"}

    @classmethod
    def supported_models(cls) -> set:
        return {
            "qwen-max", "qwen3-max", "qwen-plus", "qwen-flash",
            "deepseek-chat"
        }

    @staticmethod
    def _create_cache_backend(c) -> BaseCache:
        storage = c.STORAGE_BACKEND
        if storage == c.STORAGE_LOCAL:
            return LLMCache(
                max_size=c.LLM_CACHE_MAX_SIZE,
                ttl_seconds=c.LLM_CACHE_TTL
            )
        elif storage == c.STORAGE_REDIS:
            return RedisLLMCache(config=c, default_ttl=c.LLM_CACHE_TTL)
        else:
            raise ValueError(f"Unsupported storage backend: {storage}")

    def extract(self, template_name: str, user_input: str, suggestion_type: str, title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                **template_vars):
        """
        åŒæ­¥æå–å…¥å£ã€‚ä»…é€‚ç”¨äºæ—  asyncio äº‹ä»¶å¾ªç¯çš„ç¯å¢ƒï¼ˆå¦‚æ™®é€šè„šæœ¬ï¼‰ã€‚
        åœ¨ Jupyterã€FastAPIã€å¼‚æ­¥æµ‹è¯•ç­‰ç¯å¢ƒä¸­ï¼Œè¯·ä½¿ç”¨ `await async_extract(...)`
        """
        return asyncio.run(self._async_extract(template_name, user_input, suggestion_type, title, **template_vars))

    async def async_extract(self, template_name: str, user_input: str, suggestion_type: str, title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                            **template_vars) -> Dict[str, Any]:
        """å¼‚æ­¥æå–å…¥å£ï¼Œé€‚ç”¨äºæ‰€æœ‰å¼‚æ­¥ç¯å¢ƒ"""
        return await self._async_extract(template_name, user_input, suggestion_type, title, **template_vars)

    async def _async_extract(self, template_name: str, user_input: str, suggestion_type: str,
                             title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š", **template_vars) -> Dict[str, Any]:
        """å¼‚æ­¥æ ¸å¿ƒæµç¨‹"""
        context = template_vars.copy()
        context["user_input"] = user_input
        context["llm_model"] = self.llm_model

        # å‚æ•°æ ¡éªŒï¼ˆåŒæ­¥ï¼‰
        if not template_name or not isinstance(template_name, str):
            raise ValueError("template_name å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
        if user_input is not None and not isinstance(user_input, str):
            raise TypeError("user_input å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ– None")

        cache_key = self.llm_cache.make_key(template_name, **context)
        logger.info(f"æ•´ä½“ç¼“å­˜ key: {cache_key}")
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                report_url = cached_data.get("meta", {}).get("report_url", "")
                res = {"report_url": report_url}
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={"template": template_name, "report_url": report_url})
                return res

        prompt_result = self.prompter.build_raw(template_name, **context)
        preprocessing_prompts = prompt_result["preprocessing_prompts"]
        parallel_prompts = prompt_result["parallel_prompts"]
        serial_prompts = prompt_result["serial_prompts"]
        basic_data = prompt_result["basic_data"]

        # æ”¶é›†æ‰€æœ‰æ­¥éª¤ç»“æœ
        all_step_results = []
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„æœ€ç»ˆprompt
        prompt_records = {PREPROCESSING: [], PARALLEL: [], SERIAL: []}
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„åŸå§‹å“åº”
        raw_response_records = {PREPROCESSING: [], PARALLEL: [], SERIAL: []}
        # æ”¶é›†å¯å¤ç”¨çš„åŠ¨æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æè¿°ä¿¡æ¯
        context_desc_info = []

        # âœ… é¢„å¤„ç†ï¼šå¿…é¡»ç­‰å¾…å®Œæˆ
        await self._run_preprocessing_async(
            preprocessing_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # âœ… å¹¶è¡Œä»»åŠ¡ï¼šå¹¶å‘æ‰§è¡Œ
        await self._run_parallel_async(
            parallel_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # âœ… ä¸²è¡Œä»»åŠ¡ï¼šå¿…é¡»ç­‰å¾…å®Œæˆ
        await self._run_serial_async(
            serial_prompts, context, template_name, cache_key, all_step_results, prompt_records, context_desc_info
        )

        # âœ… ä½¿ç”¨ basic_data ç»„è£…æœ€ç»ˆç»“æœ
        result = self._assemble_final_data(context, basic_data)
        aggregation = self._aggregate_step_results(all_step_results, raw_response_records)
        valid_result = self._validate_final_result(result)
        aggregation["__errors_summary"]["final_validation_errors"] = [
            {"step": "final_validation", "errors": valid_result["__final_validation_errors"]}
        ] if valid_result["__final_validation_errors"] else []
        result["meta"]["validity_level"] = valid_result["__validity_level"]

        report_url = ""

        # åªç¼“å­˜å®Œå…¨æˆåŠŸçš„ç»“æœ
        if valid_result.get("__success"):
            try:
                await self._inject_suggestion_into_result(result, user_input, suggestion_type, title)

                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                filename = self.file_util.generate_filename(prefix=template_name, suffix=".json")

                # === å†™å…¥ DATA_YUAN_RAW_DIR ===
                raw_file_path = config.DATA_YUAN_RAW_DIR / filename
                if self.file_util.write_json(result, raw_file_path):
                    logger.info("ğŸ’¾ å·²ä¿å­˜ç»“æ„åŒ–æ•°æ®", extra={"path": str(raw_file_path), "category": template_name})

                # === å†™å…¥ DATA_YUAN_DYE_VAT_DIRï¼ˆéªŒè¯è¯Šæ–­ä¿¡æ¯ï¼‰===
                dye_data = {
                    "success": valid_result.get("__success", False),
                    "partial_success": aggregation.get("__partial_success", False),
                    "__valid_structure": aggregation.get("__valid_structure", False),
                    "errors_summary": aggregation.get("__errors_summary", {}),
                    "prompt_records": prompt_records,
                    "raw_response_records": raw_response_records,
                    "model": self.llm_model,
                    "category": template_name,
                    "timestamp": int(time.time()),
                }
                dye_file_path = config.DATA_YUAN_DYE_VAT_DIR / filename
                if self.file_util.write_json(dye_data, dye_file_path):
                    logger.info("ğŸ’‰ å·²ä¿å­˜éªŒè¯è¯Šæ–­ä¿¡æ¯", extra={"path": str(dye_file_path), "category": template_name})

            except Exception as e:
                logger.exception("æŒä¹…åŒ– extract ç»“æœå¤±è´¥", extra={"category": template_name, "error": str(e)})

            outpath = self._render_report_to_html(result)
            # âœ… æå–æ–‡ä»¶åï¼ˆå¦‚ analysis_abc123.htmlï¼‰
            report_filename = outpath.name  # outpath æ˜¯ Path å¯¹è±¡
            # âœ… æ„é€ å¯è®¿é—®çš„ URLï¼ˆå‰ç«¯èƒ½æ‰“å¼€çš„ï¼‰
            report_url = f"/reports/{report_filename}"
            logger.info("âœ… æ„é€ HTMLæŠ¥å‘ŠæˆåŠŸ", extra={"report_url": report_url})

            result["meta"]["report_url"] = report_url
            await self.llm_cache.set(cache_key, result)
            logger.info("âœ… ç¼“å­˜æœ‰æ•ˆç»“æœ", extra={"cache_key": cache_key})
            # self._open_report_in_browser(outpath)
        else:
            logger.info("ğŸŸ¡ éƒ¨åˆ†æ­¥éª¤æˆåŠŸä½†æœ€ç»ˆæ ¡éªŒå¤±è´¥ï¼Œä¸ç¼“å­˜",
                        extra={"cache_key": cache_key, "errors_summary": aggregation.get("__errors_summary", {})})

        return {"report_url": report_url}

    @timed
    async def _run_preprocessing_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """å¼‚æ­¥æ‰§è¡Œé¢„å¤„ç†ï¼ˆè™½ç„¶æ˜¯é¡ºåºï¼Œä½†ä¸ºæœªæ¥æ‰©å±•ï¼‰"""
        logger.info("ğŸ”§ æ‰§è¡Œé¢„å¤„ç†ä»»åŠ¡", extra={"count": len(prompts)})
        for idx, (step_name, driven_by, prompt_template) in enumerate(prompts):
            cache_key = f"{cache_key_base}:{step_name}:{idx}"

            rendered_prompt = self.build_user_input_only(prompt_template, context, context_desc_info)
            # è®°å½• prompt
            prompt_records[PREPROCESSING].append({"step_name": step_name, "prompt": rendered_prompt})

            result = await self._execute_single_step_async(rendered_prompt, template_name, step_name, cache_key,
                                                           PREPROCESSING)
            all_step_results.append(result)
            self._update_context_from_result(result, context, step_name)
            await self.llm_cache.set(cache_key, result)

    @timed
    async def _run_parallel_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """å¹¶å‘æ‰§è¡Œå¹¶è¡Œä»»åŠ¡"""
        if not prompts:
            logger.info("â­ï¸ æ— å¹¶è¡Œä»»åŠ¡")
            return

        logger.info("âš¡ æ‰§è¡Œå¹¶è¡Œä»»åŠ¡", extra={"count": len(prompts)})
        rendered_prompt = self.build_parallel_context(
            step_name=LLM_SOURCE_EXTRACTION,
            prompt_template="",
            context=context,
            context_desc_info=context_desc_info
        )

        # æ˜¾å¼å®šä¹‰è¿”å›ç±»å‹ï¼Œè®©ç±»å‹æ£€æŸ¥å™¨çŸ¥é“å¯èƒ½è¿”å› Exception
        async def _task(index: int, step_name: str, driven_by: str, prompt_template: str) -> Dict[str, Any]:
            async with self._parallel_semaphore:  # â† é™åˆ¶å¹¶å‘
                cache_key = f"{cache_key_base}:{step_name}:{index}"
                prompt_template += rendered_prompt
                prompt_records[PARALLEL].append({"step_name": step_name, "prompt": rendered_prompt})
                data = await self._execute_single_step_async(prompt_template, template_name, step_name, cache_key,
                                                             PARALLEL)
                await self.llm_cache.set(cache_key, data)
                return data

        # âœ… æ‰€æœ‰ task éƒ½è¿”å› dictï¼Œä¸å†å¯èƒ½è¿”å› Exception
        tasks = [
            _task(idx, step_name, driven_by, prompt)
            for idx, (step_name, driven_by, prompt) in enumerate(prompts)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=False)

        for idx, result in enumerate(results):
            all_step_results.append(result)
            self._update_context_from_result(result, context, result.get("step_name"))

    @timed
    async def _run_serial_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """ä¸²è¡Œæ‰§è¡Œä»»åŠ¡ï¼Œåç»­æ­¥éª¤å¯ä½¿ç”¨å‰é¢æ­¥éª¤æ³¨å…¥çš„å­—æ®µ"""
        if not prompts:
            logger.info("â­ï¸ æ— ä¸²è¡Œä»»åŠ¡")
            return

        logger.info("ğŸ” æ‰§è¡Œä¸²è¡Œä»»åŠ¡", extra={"count": len(prompts)})
        # === å…³é”®ï¼šåŠ¨æ€ç”Ÿæˆæ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡æè¿° ===
        dynamic_desc = self.build_serial_context_batch(context)
        context_desc_info.append(dynamic_desc)
        total_steps = len(prompts)

        for idx, (step_name, driven_by, prompt_template) in enumerate(prompts):
            cache_key = f"{cache_key_base}:{step_name}:{idx}"
            rendered_prompt = f"{prompt_template}"
            if idx == 0:
                for item in context_desc_info:
                    rendered_prompt += f"{item}"
            else:
                if context_desc_info:
                    rendered_prompt += context_desc_info[0]  # åˆå§‹ä¸Šä¸‹æ–‡
                    if len(context_desc_info) > 1:
                        rendered_prompt += context_desc_info[-1]  # æœ€æ–°ä¸Šä¸‹æ–‡

            prompt_records[SERIAL].append({"step_name": step_name, "prompt": rendered_prompt})
            result = await self._execute_single_step_async(rendered_prompt, template_name, step_name,
                                                           cache_key, SERIAL)
            all_step_results.append(result)
            # 4. æ›´æ–° contextï¼ˆåç»­æ­¥éª¤å¯ç”¨ï¼‰
            self._update_context_from_result(result, context, step_name)
            # ä»…åœ¨éæœ€åä¸€æ¬¡è¿­ä»£æ—¶ç”Ÿæˆå¹¶æ³¨å…¥å¹¶è¡Œä¸Šä¸‹æ–‡æè¿°
            if idx < total_steps - 1:
                temp_context = {driven_by: context.get(driven_by)}
                self.build_parallel_context(step_name, "", temp_context, context_desc_info)

            await self.llm_cache.set(cache_key, result)

    @timed
    async def _execute_single_step_async(
            self,
            prompt_template: str,
            template_name: str,
            step_name: str,
            cache_key: str,
            prompt_type: str
    ) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ª LLM è°ƒç”¨ï¼Œæ”¯æŒç¼“å­˜"""
        # æŸ¥ç¼“å­˜ï¼ˆåŒæ­¥ï¼‰
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={"template": template_name})
                return cached_data

        try:
            backend = await self.backend
            result = await backend.async_call(
                prompt=prompt_template,
                model=self.llm_model,
                template_name=template_name,
                step_name=step_name,
                params=self.recommended_params,
                prompt_type=prompt_type
            )
            return result
        except Exception as e:
            import traceback
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(
                f"[{step_name}] LLM è°ƒç”¨å¼‚å¸¸",
                extra={"step": step_name}
            )
            return {
                "__success": False,
                "__valid_structure": False,
                "data": {},
                "__raw_response": None,
                "__validation_errors": [],
                "__api_error": None,
                "__system_error": error_msg,  # â† ç»Ÿä¸€ä½¿ç”¨ __system_error
                "model": self.llm_model,
                "template_name": template_name,
                "step_name": step_name,
                "prompt_type": prompt_type,
                "__traceback": traceback.format_exc()  # å¯é€‰ï¼Œç”¨äºè°ƒè¯•
            }

    @staticmethod
    def _update_context_from_result(
            result: Dict[str, Any],
            context: Dict[str, Any],
            step_name: str
    ):
        if not result.get("__success"):
            error_detail = result.get("__system_error") or result.get("__api_error") or "Unknown error"
            extra = {
                "step": step_name,
                "error": error_detail,  # â† çœŸå®é”™è¯¯
                "system_error": result.get("__system_error"),
                "api_error": result.get("__api_error")
            }
            logger.warning(
                f"âš ï¸ æ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡æ›´æ–°: {extra}",
                module_name=MetaCognitiveEngine.CHINESE_NAME,
                extra=extra
            )
            return

        if not result.get("__valid_structure"):
            val_errors = result.get("__validation_errors", [])
            extra = {
                "step_name": result.get("step_name"),
                "template_name": result.get("template_name"),
                "validation_errors": val_errors,
                "raw_response": result.get("__raw_response")[:200] if result.get("__raw_response") else None
            }
            logger.warning(
                f"å½“å‰æ­¥éª¤ {step_name} ç»“æ„æ ¡éªŒå¤±è´¥",
                module_name=MetaCognitiveEngine.CHINESE_NAME,
                extra=extra
            )
            return

        data = result.get("data")
        if data and isinstance(data, dict):
            clean_data = {k: v for k, v in data.items() if not k.startswith("__")}
            if clean_data:
                injected_keys = list(clean_data.keys())
                context.update(clean_data)
                logger.info("ğŸŸ¢ æˆåŠŸæ³¨å…¥ä¸Šä¸‹æ–‡å­—æ®µ", module_name=MetaCognitiveEngine.CHINESE_NAME,
                            extra={"step": step_name, "keys": injected_keys})
        elif data:
            raise ValueError(f"[{step_name}] data å¿…é¡»æ˜¯ dictï¼Œå®é™…ä¸º {type(data)}")
        else:
            logger.info("âšª data ä¸ºç©ºï¼Œè·³è¿‡æ³¨å…¥", module_name=MetaCognitiveEngine.CHINESE_NAME, extra={"step": step_name})

    @staticmethod
    def _assemble_final_data(
            context: Dict[str, Any],
            basic_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç»„è£…æœ€ç»ˆçš„æœ‰æ•ˆæ•°æ®ç»“æ„ã€‚
        - ä¿ç•™ basic_data çš„å®Œæ•´éª¨æ¶ï¼ˆå°¤å…¶æ˜¯ meta ç»“æ„ï¼‰
        - ä» context ä¸­æå–éæ’é™¤å­—æ®µæ³¨å…¥ result é¡¶å±‚
        - ä¸º result ä¸­çš„ participants æ¯ä¸ªå®ä½“ç”Ÿæˆ entity_id
        - è®¡ç®— privacy_level å¹¶æ³¨å…¥ meta.privacy_scope
        """
        result = deepcopy(basic_data)

        excluded_fields = {"user_input", "llm_model"}

        # ç¬¬ä¸€æ­¥ï¼šæ³¨å…¥éæ’é™¤å­—æ®µ
        for key, value in context.items():
            if key.startswith("__") or key in excluded_fields:
                continue
            result[key] = value

        # ç¬¬äºŒæ­¥ï¼šå¤„ç† participantsï¼ˆç›´æ¥æ“ä½œ resultï¼‰
        participants = result.get("participants")
        if isinstance(participants, list) and participants:
            processed_participants = []
            for p in participants:
                if isinstance(p, dict) and "entity" in p and isinstance(p["entity"], str):
                    unique_suffix = uuid.uuid4().hex[:8]
                    p_new = deepcopy(p)
                    p_new["entity_id"] = f"{p['entity']}_{unique_suffix}"
                    processed_participants.append(p_new)
                else:
                    processed_participants.append(deepcopy(p))
            result["participants"] = processed_participants

        # ç¬¬ä¸‰æ­¥ï¼šéšç§åº¦è®¡ç®—ï¼ˆä»åŸºäº contextï¼Œå› ä¸º count éœ€è¦æ’é™¤é€»è¾‘ï¼‰
        count = sum(
            1 for k in context.keys()
            if not k.startswith("__") and k not in excluded_fields
        )

        privacy_score = count * 0.05

        # æ¨ç†å±‚
        inference = context.get("inference")
        if isinstance(inference, dict):
            has_inference = (
                    (isinstance(inference.get("events"), list) and len(inference["events"]) > 0) or
                    (isinstance(inference.get("summary"), str) and inference["summary"].strip()) or
                    (isinstance(inference.get("evidence"), list) and len(inference["evidence"]) > 0)
            )
            if has_inference:
                privacy_score += 0.05

        # æ·±åº¦åˆ†æå±‚ï¼ˆ+0.05ï¼‰
        deep_analysis = context.get("deep_analysis")
        if isinstance(deep_analysis, dict):
            has_deep_analysis = (
                    (isinstance(deep_analysis.get("summary"), str) and deep_analysis["summary"].strip()) or
                    (isinstance(deep_analysis.get("core_driver"), list) and len(deep_analysis["core_driver"]) > 0) or
                    (isinstance(deep_analysis.get("power_asymmetry"), dict) and deep_analysis["power_asymmetry"]) or
                    (isinstance(deep_analysis.get("narrative_distortion"), dict) and deep_analysis[
                        "narrative_distortion"])
            )
            if has_deep_analysis:
                privacy_score += 0.05

        # åˆç†å»ºè®®å±‚ï¼ˆ+0.1ï¼‰
        rational_advice = context.get("rational_advice")
        if isinstance(rational_advice, dict):
            has_rational_advice = (
                    (isinstance(rational_advice.get("summary"), str) and rational_advice["summary"].strip()) or
                    (isinstance(rational_advice.get("safety_first_intervention"), list) and len(
                        rational_advice["safety_first_intervention"]) > 0) or
                    (isinstance(rational_advice.get("incremental_strategy"), list) and len(
                        rational_advice["incremental_strategy"]) > 0)
            )
            if has_rational_advice:
                privacy_score += 0.1

        privacy_level = min(round(privacy_score, 2), 1.0)

        # æ³¨å…¥ meta
        meta = result.setdefault("meta", {})
        privacy_scope = meta.setdefault("privacy_scope", {})
        privacy_scope["privacy_level"] = float(privacy_level)

        return result

    @staticmethod
    def build_user_input_only(
            prompt_template: str,
            context: Dict[str, Any],
            context_desc_info: List
    ) -> str:
        user_input_text = context.get("user_input", "")
        build_context_desc = f"\n\n## ç”¨æˆ·è¾“å…¥\n{user_input_text}"
        context_desc_info.append(build_context_desc)
        rendered_prompt = f"{prompt_template}{build_context_desc}"
        return rendered_prompt

    def build_parallel_context(
            self,
            step_name: str,
            prompt_template: str,
            context: Dict[str, Any],
            context_desc_info: List
    ) -> str:
        """
        æ„å»ºæœ€ç»ˆæ¸²æŸ“åçš„ promptï¼Œæ”¯æŒåŠ¨æ€æè¿°ç”Ÿæˆã€‚
        """
        field_config = self._step_type_to_config.get(step_name)
        construction_input = ""
        if field_config:
            try:
                construction_input = self.prompter.generate_description(
                    context=context,
                    field_config=field_config,
                    prefix=""
                )
                construction_input = "\n" + construction_input
                context_desc_info.append(construction_input)
            except Exception as e:
                logger.error(f"[{step_name}] åŠ¨æ€æè¿°ç”Ÿæˆå¤±è´¥: {e}")

        final_prompt = f"{prompt_template}{context_desc_info[0]}{construction_input}".strip()
        return final_prompt

    def build_serial_context_batch(self, context: Dict[str, Any]) -> str:
        """
        éå† context ä¸­æ‰€æœ‰é¡¶çº§å­—æ®µï¼ˆé __ å¼€å¤´ï¼Œéç³»ç»Ÿå­—æ®µï¼‰ï¼Œ
        æŸ¥æ‰¾å…¶å¯¹åº”çš„ step_typeï¼Œç”¨å®Œæ•´ field_tuples ç”Ÿæˆæè¿°ã€‚
        """
        excluded = {"user_input", "llm_model", "participants"}
        descriptions = []

        for key, value in context.items():
            if key.startswith("__") or key in excluded:
                continue

            # æŸ¥æ‰¾è¯¥é¡¶çº§å­—æ®µå…³è”çš„æ‰€æœ‰ step_type
            step_types = self._top_field_to_step_types.get(key)
            if not step_types:
                continue

            for st in step_types:
                field_tuples = self._step_type_to_config.get(st)
                if not field_tuples:
                    continue

                try:
                    desc = self.prompter.generate_description(
                        context=context,
                        field_config=field_tuples,
                        prefix=""
                    )
                    if desc.strip():
                        descriptions.append(desc.strip())
                except Exception as e:
                    logger.error(f"ç”Ÿæˆå­—æ®µ {key} çš„æè¿°å¤±è´¥ (step_type={st}): {e}")

        return "\n".join(descriptions)

    @staticmethod
    def _validate_l0(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒåŸå§‹è¾“å…¥æœ‰æ•ˆæ€§ï¼ˆL0ï¼‰"""
        errors = []
        required_top = {"id", "type", "timestamp", "source", "meta"}
        for field in required_top:
            if field not in result:
                errors.append(f"L0ç¼ºå¤±é¡¶å±‚å­—æ®µ: {field}")

        source = result.get("source", {})
        content = source.get("content")
        if not isinstance(content, str) or len(content.strip()) < 10:
            errors.append("L0: source.content å¿…é¡»ä¸ºéç©ºå­—ç¬¦ä¸²ä¸”é•¿åº¦â‰¥10")

        return len(errors) == 0, errors

    @staticmethod
    def _validate_l1(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒè¯­ä¹‰ç»“æ„æœ‰æ•ˆæ€§ï¼ˆL1ï¼‰"""
        errors = []
        l1_valid = False
        present_but_empty = []
        missing_or_invalid = []

        for mod_name in SEMANTIC_MODULES_L1:
            mod = result.get(mod_name)
            if mod is None:
                missing_or_invalid.append(f"{mod_name} (ç¼ºå¤±)")
            elif isinstance(mod, dict):
                token = mod.get("semantic_notation")
                if token and isinstance(token, str) and token.strip():
                    l1_valid = True
            elif isinstance(mod, list):
                if len(mod) == 0:
                    present_but_empty.append(f"{mod_name} (ç©ºåˆ—è¡¨)")
                else:
                    found_valid = False
                    for item in mod:
                        if isinstance(item, dict):
                            token = item.get("semantic_notation")
                            if token and isinstance(token, str) and token.strip():
                                found_valid = True
                                break
                    if found_valid:
                        l1_valid = True
                    else:
                        present_but_empty.append(f"{mod_name} (æ— æœ‰æ•ˆ semantic_notation)")
            else:
                missing_or_invalid.append(f"{mod_name} (ç±»å‹é”™è¯¯: {type(mod)})")

        if not l1_valid:
            if present_but_empty:
                errors.append("å­˜åœ¨è¯­ä¹‰æ¨¡å—ä½†æœªæå– semantic_notation: " + ", ".join(present_but_empty))
            if missing_or_invalid:
                errors.append("å…³é”®è¯­ä¹‰æ¨¡å—ç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯: " + ", ".join(missing_or_invalid))

        return l1_valid, errors

    @staticmethod
    def _validate_l2(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒè®¤çŸ¥å¹²é¢„æœ‰æ•ˆæ€§ï¼ˆL2ï¼‰â€”â€”å®½æ¾ä½†æœ‰åº•çº¿"""
        errors = []
        l2_ok = True

        # --- 1. inference ---
        inference = result.get("inference")
        if not isinstance(inference, dict):
            errors.append("L2: inference ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            sn = inference.get("semantic_notation")
            if not (isinstance(sn, str) and sn.strip()):
                errors.append("L2: inference.semantic_notation ç¼ºå¤±æˆ–ä¸ºç©º")
                l2_ok = False
            # ä¸æ ¡éªŒ events / evidence â€”â€” å…è®¸ç¼ºå¤±

        # --- 2. deep_analysis ---
        deep_analysis = result.get("deep_analysis")
        if not isinstance(deep_analysis, dict):
            errors.append("L2: deep_analysis ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            sn = deep_analysis.get("semantic_notation")
            if not (isinstance(sn, str) and sn.strip()):
                errors.append("L2: deep_analysis.semantic_notation ç¼ºå¤±æˆ–ä¸ºç©º")
                l2_ok = False
            has_content = any(
                v not in (None, "", [], {}, False)
                for k, v in deep_analysis.items()
                if k not in ("semantic_notation", "summary")
            )
            if not has_content:
                errors.append("L2: deep_analysis æ— å®è´¨æ€§åˆ†æå†…å®¹")
                l2_ok = False

        # --- 3. rational_advice ---
        rational_advice = result.get("rational_advice")
        if not isinstance(rational_advice, dict):
            errors.append("L2: rational_advice ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            sn = rational_advice.get("semantic_notation")
            if not (isinstance(sn, str) and sn.strip()):
                errors.append("L2: rational_advice.semantic_notation ç¼ºå¤±æˆ–ä¸ºç©º")
                l2_ok = False
            has_content = any(
                v not in (None, "", [], {}, False)
                for k, v in rational_advice.items()
                if k not in ("semantic_notation", "summary")
            )
            if not has_content:
                errors.append("L2: rational_advice æ— å®è´¨æ€§å»ºè®®å†…å®¹")
                l2_ok = False

        return l2_ok, errors

    def _validate_final_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸è¿›å¼ä¸‰çº§æœ‰æ•ˆæ€§æ ¡éªŒï¼ˆå•æ¬¡è¯­ä¹‰æ¨¡å—éå†ï¼‰ï¼Œè¿”å› validity_level ä¸åˆ†å±‚é”™è¯¯ã€‚
        """
        # L0: åŸå§‹è¾“å…¥
        l0_valid, errors_l0 = self._validate_l0(result)
        if not l0_valid:
            return {
                "__success": False,
                "__validity_level": "invalid",
                "__final_validation_errors": {"L0": errors_l0, "L1": [], "L2": []}
            }

        # L1: è¯­ä¹‰ç»“æ„
        l1_valid, errors_l1 = self._validate_l1(result)

        # L2: è®¤çŸ¥å¹²é¢„ï¼ˆä»…å½“ L1 æœ‰æ•ˆæ—¶å°è¯•ï¼‰
        l2_valid, errors_l2 = self._validate_l2(result) if l1_valid else (False, [])

        # ç¡®å®šæœ€ç»ˆçº§åˆ«
        level = (
            "L2_actionable" if l2_valid else
            "L1_structured" if l1_valid else
            "L0_raw"
        )

        return {
            "__success": True,
            "__validity_level": level,
            "__final_validation_errors": {
                "L0": errors_l0,
                "L1": errors_l1,
                "L2": errors_l2
            }
        }

    @staticmethod
    def _aggregate_step_results(all_step_results: List[Dict[str, Any]], raw_response_records: Dict) -> Dict[str, Any]:
        """
        èšåˆæ‰€æœ‰æ­¥éª¤çš„ç»“æœï¼Œä»…æ”¶é›†é”™è¯¯ä¿¡æ¯å’Œæ‰§è¡ŒçŠ¶æ€ã€‚
        æ³¨æ„ï¼š__success å­—æ®µä¸åœ¨æ­¤å¤„è®¾ç½®ï¼Œç•™ç©ºï¼ˆç”± _validate_final_result å†³å®šï¼‰ã€‚
        """
        system_errors = []
        api_errors = []
        validation_errors_all = []
        all_valid = True
        partial_success = False

        for step in all_step_results:
            if step.get("__success"):
                partial_success = True

            if not step.get("__valid_structure"):
                all_valid = False

            sys_err = step.get("__system_error")
            if sys_err:
                system_errors.append({
                    "step": step.get("step_name"),
                    "error": sys_err
                })

            api_err = step.get("__api_error")
            if api_err:
                api_errors.append({
                    "step": step.get("step_name"),
                    "error": api_err
                })

            val_errs = step.get("__validation_errors")
            if val_errs:
                validation_errors_all.append({
                    "step": step.get("step_name"),
                    "errors": val_errs
                })

            prompt_type = step.get("prompt_type")
            if prompt_type and prompt_type == PREPROCESSING:
                raw_response_records[PREPROCESSING].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })
            elif prompt_type and prompt_type == PARALLEL:
                raw_response_records[PARALLEL].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })
            else:
                raw_response_records[SERIAL].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })

        return {
            "__valid_structure": all_valid,
            "__partial_success": partial_success,
            "__errors_summary": {
                "system_errors": system_errors,
                "api_errors": api_errors,
                "validation_errors": validation_errors_all,
                # é¢„ç•™ä½ç½®ç»™æœ€ç»ˆæ ¡éªŒé”™è¯¯ï¼Œç”¨ä¸åŒ key é¿å…å†²çª
                "final_validation_errors": []  # åç»­ç”± _validate_final_result å¡«å…¥
            }
        }

    @staticmethod
    def _build_top_field_to_step_types() -> Dict[str, List[str]]:
        """
        ä» REQUIRED_FIELDS_BY_CATEGORY ä¸­æå–æ‰€æœ‰é¡¶çº§å­—æ®µï¼ˆå¦‚ 'participants'ï¼‰ï¼Œ
        å¹¶è®°å½•å®ƒä»¬æ‰€å±çš„ step_typeï¼ˆå¦‚ LLM_SOURCE_EXTRACTIONï¼‰ã€‚
        """
        mapping: Dict[str, List[str]] = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, field_tuples in steps.items():
                for field_path, *_ in field_tuples:
                    # æå–é¡¶çº§å­—æ®µåï¼šå–ç¬¬ä¸€ä¸ª '.' ä¹‹å‰çš„éƒ¨åˆ†
                    top_field = field_path.split('.')[0]
                    if top_field not in mapping:
                        mapping[top_field] = []
                    if step_name not in mapping[top_field]:
                        mapping[top_field].append(step_name)
        return mapping

    @staticmethod
    def _build_step_type_to_config() -> Dict[str, List[Tuple]]:
        config_map = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, tuples in steps.items():
                if step_name not in config_map:
                    config_map[step_name] = []
                config_map[step_name].extend(tuples)
        return config_map

    def _render_report_to_html(self, data: Dict[str, Any]) -> Optional[Path]:
        """
        å°† result æ•°æ®æ³¨å…¥ HTML æ¨¡æ¿ï¼Œç”ŸæˆæŠ¥å‘Šã€‚
        - è¾“å‡ºç›®å½•ï¼šconfig.STATIC_REPORTS_DIR
        - æ–‡ä»¶åï¼šé€šè¿‡ self.file_util.generate_filename ç”Ÿæˆ
        - å‰ç¼€ï¼š"æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š"
        - åç¼€ï¼š".html"
        - æ¨¡æ¿è¯»å–ï¼šå¤ç”¨ self.file_util.read_file
        - æ–‡ä»¶å†™å…¥ï¼šå¤ç”¨ self.file_util.write_file
        - ä¸Šä¸‹æ–‡å˜é‡åï¼šdata
        """
        try:
            if not data or not isinstance(data, dict):
                return None

            # 1. ç”Ÿæˆæ–‡ä»¶å
            filename = self.file_util.generate_filename(
                prefix="æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                suffix=".html",
                include_timestamp=True
            )

            # 2. ç¡®å®šè¾“å‡ºè·¯å¾„
            output_path = config.REPORTS_DIR / filename

            # 3. è¯»å–æ¨¡æ¿ï¼ˆå¤ç”¨å·¥å…·ï¼‰
            template_content = self.file_util.read_file(
                str(config.FILE_DEFAULT_TEMPLATE_PATH),
                encoding="utf-8",
                auto_decode=False
            )
            if not template_content:
                logger.error("âŒ æ¨¡æ¿æ–‡ä»¶ä¸ºç©ºæˆ–è¯»å–å¤±è´¥", extra={"template_path": str(config.FILE_DEFAULT_TEMPLATE_PATH)})
                return None

            # 4. æ¸²æŸ“æ¨¡æ¿
            from jinja2 import Template
            html_output = Template(template_content).render(data=data)

            # 5. âœ… å†™å…¥ HTMLï¼ˆå¤ç”¨ file_util.write_fileï¼‰
            success = self.file_util.write_file(
                file_path=str(output_path),
                content=html_output,
                encoding="utf-8",
                as_json=False,
                file_type="html"  # â† è®©æ—¥å¿—æ˜¾ç¤ºâ€œHTMLâ€ï¼ˆè‹¥ write_file æ”¯æŒè¯¥å‚æ•°ï¼‰
            )

            if not success:
                logger.error("âŒ HTML æŠ¥å‘Šå†™å…¥å¤±è´¥", extra={"path": str(output_path)})
                return None

            logger.info("ğŸ“„ HTML æŠ¥å‘Šå·²ç”Ÿæˆ", extra={"path": str(output_path)})
            return output_path

        except Exception as e:
            logger.exception("ğŸ’¥ HTML æŠ¥å‘Šç”Ÿæˆå¤±è´¥", extra={"error": str(e)})
            return None

    async def _execute_suggestion(self, prompt: str) -> str:
        logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆ LLM å»ºè®®å†…å®¹", module_name=self.CHINESE_NAME)
        try:
            backend = await self.backend
            result = await backend.generate_text(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
            )
            if result.startswith("ç”Ÿæˆå¤±è´¥"):
                logger.warning("âš ï¸ LLM å»ºè®®ç”Ÿæˆå¤±è´¥", extra={"error": result})
            else:
                logger.info("âœ… LLM å»ºè®®ç”ŸæˆæˆåŠŸ", module_name=self.CHINESE_NAME)
            return result
        except Exception as e:
            error_msg = f"LLM å»ºè®®ç”Ÿæˆå¼‚å¸¸: {type(e).__name__}: {str(e)}"
            logger.exception(error_msg, module_name=self.CHINESE_NAME)
            return error_msg

    async def _inject_suggestion_into_result(
            self,
            result: Dict[str, Any],
            user_input: str,
            suggestion_type: str,
            title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š"
    ) -> None:
        """
        ä¸ºå·²éªŒè¯é€šè¿‡çš„ result æ³¨å…¥ LLM ç”Ÿæˆçš„å»ºè®®å†…å®¹ã€‚
        - æ ‡é¢˜æ³¨å…¥åˆ° result['meta']['title']
        - å»ºè®®å†…å®¹æ³¨å…¥åˆ° result['analysis']['suggestion']ï¼ˆå«å…ƒä¿¡æ¯ï¼‰
        - ä¿è¯å³ä½¿å¤±è´¥ä¹Ÿä¸ç ´å result ç»“æ„
        """
        # ç¡®ä¿ meta å’Œ analysis å­˜åœ¨
        result.setdefault("meta", {})
        result.setdefault("analysis", {})

        # å…ˆè®¾ç½®æ ‡é¢˜ï¼ˆæ€»æ˜¯æˆåŠŸï¼‰
        result["meta"]["title"] = title

        try:
            suggestion_prompt = self.prompter.build_suggestion(
                template_name=CATEGORY_SUGGESTION,
                user_input=user_input,
                suggestion_type=suggestion_type
            )
            suggestion_content = await self._execute_suggestion(suggestion_prompt)

            # æ„é€ å¸¦å…ƒä¿¡æ¯çš„ suggestion å¯¹è±¡
            suggestion_record = {
                "content": suggestion_content,
                "type": suggestion_type,
                "model": self.llm_model,
                "generated_at": int(time.time()),
                "success": not (suggestion_content.startswith(("ç”Ÿæˆå¤±è´¥", "[å»ºè®®ç”Ÿæˆå¤±è´¥")))
            }

            result["analysis"]["suggestion"] = suggestion_record
            logger.info(
                "âœ… LLM å»ºè®®å·²æ³¨å…¥ç»“æœ",
                extra={
                    "suggestion_type": suggestion_type,
                    "model": self.llm_model,
                    "success": suggestion_record["success"]
                }
            )
        except Exception as e:
            error_msg = f"[å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}]"
            logger.exception(
                "âš ï¸ æ³¨å…¥ LLM å»ºè®®å¤±è´¥",
                extra={
                    "suggestion_type": suggestion_type,
                    "error": str(e)
                }
            )
            # å³ä½¿å¤±è´¥ä¹Ÿå†™å…¥ç»“æ„åŒ–å ä½ï¼Œä¾¿äºå‰ç«¯/åç»­å¤„ç†
            result["analysis"]["suggestion"] = {
                "content": error_msg,
                "type": suggestion_type,
                "model": self.llm_model,
                "generated_at": int(time.time()),
                "success": False
            }

    @staticmethod
    def _open_report_in_browser(outpath: Path) -> None:
        try:
            import webbrowser
            webbrowser.open(f"file://{outpath}")
            logger.info("ğŸŒ å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š", extra={"outpath": str(outpath)})
        except Exception as e:
            logger.warning("âŒ æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨", extra={"error": str(e)})
