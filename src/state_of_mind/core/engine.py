import asyncio
import time
import uuid
from copy import deepcopy
from pathlib import Path
from typing import List, Any, Tuple, Dict, Optional, Set
from src.state_of_mind.utils.registry import GlobalSingletonRegistry
from src.state_of_mind.cache.base import BaseCache
from src.state_of_mind.cache.redis import RedisLLMCache
from .prompter import Prompter
from ..cache.llm_cache import LLMCache
from ..config import config
from ..utils.async_decorators import async_timed
from ..utils.constants import ModelName, REQUIRED_FIELDS_BY_CATEGORY, LLM_PARTICIPANTS_EXTRACTION, \
    PREPROCESSING, PARALLEL, SERIAL, SEMANTIC_MODULES_L1, CATEGORY_SUGGESTION, LLM_EXPLICIT_MOTIVATION_EXTRACTION, \
    LLM_INFERENCE, PERCEPTION_LAYERS, ALLOWED_SERIAL_MARKERS, ALLOWED_PARALLEL_MARKERS, EXCLUDED_PRONOUNS, \
    CHINESE_PRONOUNS
from ..utils.file_util import FileUtil
from src.state_of_mind.utils.logger import LoggerManager as logger


class MetaCognitiveEngine:
    CHINESE_NAME = "MetaCognitiveEngine"
    _init_lock = asyncio.Lock()  # ç”¨äºä¿æŠ¤å¹¶å‘åˆå§‹åŒ–

    def __init__(self, backend_name: str, llm_model: str, recommended_params: dict):
        # å‚æ•°æ ¡éªŒ
        if not backend_name:
            raise ValueError("backend_name ä¸èƒ½ä¸ºç©º")
        if not llm_model:
            raise ValueError("llm_model ä¸èƒ½ä¸ºç©º")
        if not isinstance(recommended_params, dict):
            raise TypeError("recommended_params å¿…é¡»æ˜¯å­—å…¸ç±»å‹")

        # æ ¡éªŒ backend_name æ˜¯å¦æ”¯æŒ
        if backend_name not in self.supported_backends():
            logger.warning(
                f"æœªçŸ¥çš„ backend_name: {backend_name}ï¼Œå½“å‰ä»…æ”¯æŒ: {self.supported_backends()}"
            )
            # å¯é€‰æ‹©æŠ›å‡ºå¼‚å¸¸æˆ–ç»§ç»­ï¼ˆå–å†³äºä½ çš„ç­–ç•¥ï¼‰
            raise ValueError(f"ä¸æ”¯æŒçš„ backend_name: {backend_name}")

        # æ ¡éªŒ llm_model æ˜¯å¦åœ¨ MODEL_CONFIG ä¸­å­˜åœ¨
        if llm_model not in self.supported_models():
            logger.warning(
                f"æœªçŸ¥çš„ llm_model: {llm_model}ï¼Œå½“å‰ä»…æ”¯æŒ: {self.supported_models()}"
            )
            # åŒæ ·ï¼Œå¯é€‰æ‹©æŠ›å‡ºå¼‚å¸¸
            raise ValueError(f"ä¸æ”¯æŒçš„ llm_model: {llm_model}")

        self.backend_name = backend_name
        self.llm_model = llm_model
        self.recommended_params = recommended_params
        self.prompter = Prompter()
        self.llm_cache = self._create_cache_backend(config)
        self.file_util = FileUtil()
        self._backend = None
        self._backend_configs = {"api_key": config.LLM_API_KEY, "timeout": 120} \
            if backend_name == ModelName.QWEN \
            else {"api_key": config.LLM_API_KEY, "base_url": config.LLM_API_URL, "timeout": 120}
        self._top_field_to_step_types = self._build_top_field_to_step_types()
        self._step_type_to_config = self._build_step_type_to_config()
        self.current_parallel_concurrency = config.get("CURRENT_PARALLEL_CONCURRENCY", 3)  # é»˜è®¤3
        self._parallel_semaphore = asyncio.Semaphore(self.current_parallel_concurrency)
        logger.info(
            f"MetaCognitiveEngine åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ backend: {backend_name}, model: {llm_model}"
        )

    @property
    async def backend(self):
        """æƒ°æ€§åŠ è½½ backend å®ä¾‹"""
        if self._backend is None:
            async with self._init_lock:
                if self._backend is None:  # double-check
                    logger.info("ğŸ”„ é¦–æ¬¡è·å– backendï¼Œæ­£åœ¨å¼‚æ­¥åˆå§‹åŒ–...")
                    self._backend = await GlobalSingletonRegistry.get_backend_async(
                        self.backend_name,
                        self._backend_configs
                    )
                    if self._backend is None:
                        raise RuntimeError(f"æ— æ³•è·å– backend: {self.backend_name}")
        return self._backend

    @classmethod
    def supported_backends(cls) -> set:
        return {"qwen", "deepseek"}

    @classmethod
    def supported_models(cls) -> set:
        return {
            "qwen-max", "qwen3-max", "qwen-plus", "qwen-flash",
            "deepseek-chat"
        }

    @staticmethod
    def _create_cache_backend(c) -> BaseCache:
        storage = c.STORAGE_BACKEND
        if storage == c.STORAGE_LOCAL:
            return LLMCache(
                max_size=c.LLM_CACHE_MAX_SIZE,
                ttl_seconds=c.LLM_CACHE_TTL
            )
        elif storage == c.STORAGE_REDIS:
            return RedisLLMCache(config=c, default_ttl=c.LLM_CACHE_TTL)
        else:
            raise ValueError(f"Unsupported storage backend: {storage}")

    def extract(self, template_name: str, user_input: str, suggestion_type: str, title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                **template_vars):
        """
        åŒæ­¥æå–å…¥å£ã€‚ä»…é€‚ç”¨äºæ—  asyncio äº‹ä»¶å¾ªç¯çš„ç¯å¢ƒï¼ˆå¦‚æ™®é€šè„šæœ¬ï¼‰ã€‚
        åœ¨ Jupyterã€FastAPIã€å¼‚æ­¥æµ‹è¯•ç­‰ç¯å¢ƒä¸­ï¼Œè¯·ä½¿ç”¨ `await async_extract(...)`
        """
        return asyncio.run(self._async_extract(template_name, user_input, suggestion_type, title, **template_vars))

    async def async_extract(self, template_name: str, user_input: str, suggestion_type: str, title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                            **template_vars) -> Dict[str, Any]:
        """å¼‚æ­¥æå–å…¥å£ï¼Œé€‚ç”¨äºæ‰€æœ‰å¼‚æ­¥ç¯å¢ƒ"""
        return await self._async_extract(template_name, user_input, suggestion_type, title, **template_vars)

    @async_timed
    async def _async_extract(self, template_name: str, user_input: str, suggestion_type: str,
                             title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š", **template_vars) -> Dict[str, Any]:
        """å¼‚æ­¥æ ¸å¿ƒæµç¨‹"""
        context = template_vars.copy()
        context["user_input"] = user_input
        context["llm_model"] = self.llm_model
        self.user_input = user_input

        # å‚æ•°æ ¡éªŒï¼ˆåŒæ­¥ï¼‰
        if not template_name or not isinstance(template_name, str):
            raise ValueError("template_name å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
        if user_input is not None and not isinstance(user_input, str):
            raise TypeError("user_input å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ– None")

        cache_key = self.llm_cache.make_key(template_name, **context)
        logger.info(f"æ•´ä½“ç¼“å­˜ key: {cache_key}")
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                report_url = cached_data.get("meta", {}).get("report_url", "")
                res = {"report_url": report_url}
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={"template": template_name, "report_url": report_url})
                return res

        prompt_result = self.prompter.build_raw(template_name, **context)
        preprocessing_prompts = prompt_result["preprocessing_prompts"]
        parallel_prompts = prompt_result["parallel_prompts"]
        serial_prompts = prompt_result["serial_prompts"]
        basic_data = prompt_result["basic_data"]

        # æ”¶é›†æ‰€æœ‰æ­¥éª¤ç»“æœ
        all_step_results = []
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„æœ€ç»ˆprompt
        prompt_records = {PREPROCESSING: [], PARALLEL: [], SERIAL: []}
        # æ”¶é›†æ‰€æœ‰æ­¥éª¤çš„åŸå§‹å“åº”
        raw_response_records = {PREPROCESSING: [], PARALLEL: [], SERIAL: []}
        # æ”¶é›†å¯å¤ç”¨çš„åŠ¨æ€ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æè¿°ä¿¡æ¯
        context_desc_info = []

        # âœ… é¢„å¤„ç†ï¼šå¿…é¡»ç­‰å¾…å®Œæˆ
        await self._run_preprocessing_async(
            preprocessing_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # âœ… å¹¶è¡Œä»»åŠ¡ï¼šå¹¶å‘æ‰§è¡Œ
        await self._run_parallel_async(
            parallel_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # âœ… ä¸²è¡Œä»»åŠ¡ï¼šå¿…é¡»ç­‰å¾…å®Œæˆ
        await self._run_serial_async(
            serial_prompts, context, template_name, cache_key, all_step_results, prompt_records, context_desc_info
        )

        # âœ… ä½¿ç”¨ basic_data ç»„è£…æœ€ç»ˆç»“æœ
        result = self._assemble_final_data(context, basic_data)
        aggregation = self._aggregate_step_results(all_step_results, raw_response_records)
        valid_result = self._validate_final_result(result)
        aggregation["__errors_summary"]["final_validation_errors"] = [
            {"step": "final_validation", "errors": valid_result["__final_validation_errors"]}
        ] if valid_result["__final_validation_errors"] else []
        result["meta"]["validity_level"] = valid_result["__validity_level"]

        report_url = ""

        # åªç¼“å­˜å®Œå…¨æˆåŠŸçš„ç»“æœ
        if valid_result.get("__success"):
            try:
                await self._inject_suggestion_into_result(result, user_input, suggestion_type, title)

                # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                filename = self.file_util.generate_filename(prefix=template_name, suffix=".json")

                # === å†™å…¥ DATA_YUAN_RAW_DIR ===
                raw_file_path = config.DATA_YUAN_RAW_DIR / filename
                if self.file_util.write_json(result, raw_file_path):
                    logger.info("ğŸ’¾ å·²ä¿å­˜ç»“æ„åŒ–æ•°æ®", extra={"path": str(raw_file_path), "category": template_name})

                # === å†™å…¥ DATA_YUAN_DYE_VAT_DIRï¼ˆéªŒè¯è¯Šæ–­ä¿¡æ¯ï¼‰===
                dye_data = {
                    "success": valid_result.get("__success", False),
                    "partial_success": aggregation.get("__partial_success", False),
                    "__valid_structure": aggregation.get("__valid_structure", False),
                    "errors_summary": aggregation.get("__errors_summary", {}),
                    "prompt_records": prompt_records,
                    "raw_response_records": raw_response_records,
                    "model": self.llm_model,
                    "category": template_name,
                    "timestamp": int(time.time()),
                }
                dye_file_path = config.DATA_YUAN_DYE_VAT_DIR / filename
                if self.file_util.write_json(dye_data, dye_file_path):
                    logger.info("ğŸ’‰ å·²ä¿å­˜éªŒè¯è¯Šæ–­ä¿¡æ¯", extra={"path": str(dye_file_path), "category": template_name})

            except Exception as e:
                logger.exception("æŒä¹…åŒ– extract ç»“æœå¤±è´¥", extra={"category": template_name, "error": str(e)})

            outpath = self._render_report_to_html(result)
            # âœ… æå–æ–‡ä»¶åï¼ˆå¦‚ analysis_abc123.htmlï¼‰
            report_filename = outpath.name  # outpath æ˜¯ Path å¯¹è±¡
            # âœ… æ„é€ å¯è®¿é—®çš„ URLï¼ˆå‰ç«¯èƒ½æ‰“å¼€çš„ï¼‰
            report_url = f"/reports/{report_filename}"
            logger.info("âœ… æ„é€ HTMLæŠ¥å‘ŠæˆåŠŸ", extra={"report_url": report_url})

            result["meta"]["report_url"] = report_url
            await self.llm_cache.set(cache_key, result)
            logger.info("âœ… ç¼“å­˜æœ‰æ•ˆç»“æœ", extra={"cache_key": cache_key})
            # self._open_report_in_browser(outpath)
        else:
            logger.info("ğŸŸ¡ éƒ¨åˆ†æ­¥éª¤æˆåŠŸä½†æœ€ç»ˆæ ¡éªŒå¤±è´¥ï¼Œä¸ç¼“å­˜",
                        extra={"cache_key": cache_key, "errors_summary": aggregation.get("__errors_summary", {})})

        return {"report_url": report_url}

    async def _run_preprocessing_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """å¼‚æ­¥æ‰§è¡Œé¢„å¤„ç†ï¼ˆè™½ç„¶æ˜¯é¡ºåºï¼Œä½†ä¸ºæœªæ¥æ‰©å±•ï¼‰"""
        logger.info("ğŸ”§ æ‰§è¡Œé¢„å¤„ç†ä»»åŠ¡", extra={"count": len(prompts)})
        for idx, (step_name, driven_by, prompt_template) in enumerate(prompts):
            cache_key = f"{cache_key_base}:{step_name}:{idx}"

            rendered_prompt = self.build_user_input_only(prompt_template, context, context_desc_info)
            # è®°å½• prompt
            prompt_records[PREPROCESSING].append({"step_name": step_name, "prompt": rendered_prompt})

            result = await self._execute_single_step_async(rendered_prompt, template_name, step_name, cache_key,
                                                           PREPROCESSING)
            all_step_results.append(result)
            self._update_context_from_result(result, context, step_name)
            await self.llm_cache.set(cache_key, result)

    async def _run_parallel_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """å¹¶å‘æ‰§è¡Œå¹¶è¡Œä»»åŠ¡"""
        if not prompts:
            logger.info("â­ï¸ æ— å¹¶è¡Œä»»åŠ¡")
            return

        logger.info("âš¡ æ‰§è¡Œå¹¶è¡Œä»»åŠ¡", extra={"count": len(prompts)})
        # æ„å»ºå‚ä¸è€…æœ‰æ•ˆä¿¡æ¯ï¼Œåªéœ€è¦ä¸€æ¬¡
        self.build_parallel_context(
            step_name=LLM_PARTICIPANTS_EXTRACTION,
            context=context,
            context_desc_info=context_desc_info
        )

        # æ˜¾å¼å®šä¹‰è¿”å›ç±»å‹ï¼Œè®©ç±»å‹æ£€æŸ¥å™¨çŸ¥é“å¯èƒ½è¿”å› Exception
        async def _task(idx: int, step_name: str, driven_by: str, prompt_template: str) -> Dict[str, Any]:
            async with self._parallel_semaphore:  # â† é™åˆ¶å¹¶å‘
                cache_key = f"{cache_key_base}:{step_name}:{idx}"
                rendered_prompt = prompt_template

                # === å…³é”®ï¼šæŒ‰ marker åŠ¨æ€ç­›é€‰è¦æ³¨å…¥çš„ä¸Šä¸‹æ–‡ ===
                allowed = ALLOWED_PARALLEL_MARKERS.get(idx, set())
                for ctx_str in context_desc_info:
                    if not ctx_str or not isinstance(ctx_str, str):
                        continue
                    # æ£€æŸ¥è¯¥æ®µæ˜¯å¦ä»¥å…è®¸çš„ marker å¼€å¤´
                    if any(ctx_str.lstrip().startswith(marker) for marker in allowed):
                        rendered_prompt += ctx_str

                prompt_records[PARALLEL].append({"step_name": step_name, "prompt": rendered_prompt})
                data = await self._execute_single_step_async(rendered_prompt, template_name, step_name, cache_key,
                                                             PARALLEL)
                await self.llm_cache.set(cache_key, data)
                return data

        # âœ… æ‰€æœ‰ task éƒ½è¿”å› dictï¼Œä¸å†å¯èƒ½è¿”å› Exception
        tasks = [
            _task(idx, step_name, driven_by, prompt)
            for idx, (step_name, driven_by, prompt) in enumerate(prompts)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=False)

        legitimate_participants = self._build_legitimate_participant_set(context)

        for idx, result in enumerate(results):
            # è°ƒç”¨å°è£…çš„åå¤„ç†å‡½æ•°
            await self._filter_perception_results_by_legitimate_participants(result, legitimate_participants)
            all_step_results.append(result)
            self._update_context_from_result(result, context, result.get("step_name"))

    async def _run_serial_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """ä¸²è¡Œæ‰§è¡Œä»»åŠ¡ï¼Œåç»­æ­¥éª¤å¯ä½¿ç”¨å‰é¢æ­¥éª¤æ³¨å…¥çš„å­—æ®µ"""
        if not prompts:
            logger.info("â­ï¸ æ— ä¸²è¡Œä»»åŠ¡")
            return

        logger.info("ğŸ” æ‰§è¡Œä¸²è¡Œä»»åŠ¡", extra={"count": len(prompts)})
        # === å…³é”®ï¼šåŠ¨æ€ç”Ÿæˆæ„ŸçŸ¥çš„ä¸Šä¸‹æ–‡æè¿° ===
        dynamic_desc = self.build_serial_context_batch(context)
        context_desc_info.append(dynamic_desc)
        # ç”Ÿæˆåˆæ³•å‚ä¸è€…æ•°æ®
        legit_participants_ctx = self._build_participants_context_desc(context)
        if legit_participants_ctx:
            context_desc_info.append(legit_participants_ctx)

        total_steps = len(prompts)

        for idx, (step_name, driven_by, prompt_template) in enumerate(prompts):
            cache_key = f"{cache_key_base}:{step_name}:{idx}"
            rendered_prompt = prompt_template

            # === å…³é”®ï¼šæŒ‰ marker åŠ¨æ€ç­›é€‰è¦æ³¨å…¥çš„ä¸Šä¸‹æ–‡ ===
            allowed = ALLOWED_SERIAL_MARKERS.get(idx, set())
            for ctx_str in context_desc_info:
                if not ctx_str or not isinstance(ctx_str, str):
                    continue
                # æ£€æŸ¥è¯¥æ®µæ˜¯å¦ä»¥å…è®¸çš„ marker å¼€å¤´
                if any(ctx_str.lstrip().startswith(marker) for marker in allowed):
                    rendered_prompt += ctx_str

            prompt_records[SERIAL].append({"step_name": step_name, "prompt": rendered_prompt})
            result = await self._execute_single_step_async(rendered_prompt, template_name, step_name,
                                                           cache_key, SERIAL)
            all_step_results.append(result)
            # 4. æ›´æ–° contextï¼ˆåç»­æ­¥éª¤å¯ç”¨ï¼‰
            self._update_context_from_result(result, context, step_name)
            # ä»…åœ¨éæœ€åä¸€æ¬¡è¿­ä»£æ—¶ç”Ÿæˆå¹¶æ³¨å…¥å¹¶è¡Œä¸Šä¸‹æ–‡æè¿°
            if idx < total_steps - 1:
                temp_context = {driven_by: context.get(driven_by)}
                self.build_parallel_context(step_name, temp_context, context_desc_info)

            await self.llm_cache.set(cache_key, result)

    async def _execute_single_step_async(
            self,
            prompt_template: str,
            template_name: str,
            step_name: str,
            cache_key: str,
            prompt_type: str
    ) -> Dict[str, Any]:
        """å¼‚æ­¥æ‰§è¡Œå•ä¸ª LLM è°ƒç”¨ï¼Œæ”¯æŒç¼“å­˜"""
        # æŸ¥ç¼“å­˜ï¼ˆåŒæ­¥ï¼‰
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={"template": template_name})
                return cached_data

        try:
            backend = await self.backend
            result = await backend.async_call(
                prompt=prompt_template,
                model=self.llm_model,
                template_name=template_name,
                step_name=step_name,
                params=self.recommended_params,
                prompt_type=prompt_type
            )
            return result
        except Exception as e:
            import traceback
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(
                f"[{step_name}] LLM è°ƒç”¨å¼‚å¸¸",
                extra={"step": step_name}
            )
            return {
                "__success": False,
                "__valid_structure": False,
                "data": {},
                "__raw_response": None,
                "__validation_errors": [],
                "__api_error": None,
                "__system_error": error_msg,  # â† ç»Ÿä¸€ä½¿ç”¨ __system_error
                "model": self.llm_model,
                "template_name": template_name,
                "step_name": step_name,
                "prompt_type": prompt_type,
                "__traceback": traceback.format_exc()  # å¯é€‰ï¼Œç”¨äºè°ƒè¯•
            }

    @staticmethod
    def _update_context_from_result(
            result: Dict[str, Any],
            context: Dict[str, Any],
            step_name: str
    ):
        if not result.get("__success"):
            error_detail = result.get("__system_error") or result.get("__api_error") or "Unknown error"
            extra = {
                "step": step_name,
                "error": error_detail,  # â† çœŸå®é”™è¯¯
                "system_error": result.get("__system_error"),
                "api_error": result.get("__api_error")
            }
            logger.warning(
                f"âš ï¸ æ­¥éª¤å¤±è´¥ï¼Œè·³è¿‡æ›´æ–°: {extra}",
                module_name=MetaCognitiveEngine.CHINESE_NAME,
                extra=extra
            )
            return

        if not result.get("__valid_structure"):
            val_errors = result.get("__validation_errors", [])
            extra = {
                "step_name": result.get("step_name"),
                "template_name": result.get("template_name"),
                "validation_errors": val_errors,
                "raw_response": result.get("__raw_response")[:200] if result.get("__raw_response") else None
            }
            logger.warning(
                f"å½“å‰æ­¥éª¤ {step_name} ç»“æ„æ ¡éªŒå¤±è´¥",
                module_name=MetaCognitiveEngine.CHINESE_NAME,
                extra=extra
            )
            return

        data = result.get("data")
        if data and isinstance(data, dict):
            clean_data = {k: v for k, v in data.items() if not k.startswith("__")}
            if clean_data:
                injected_keys = list(clean_data.keys())
                context.update(clean_data)
                logger.info("ğŸŸ¢ æˆåŠŸæ³¨å…¥ä¸Šä¸‹æ–‡å­—æ®µ", module_name=MetaCognitiveEngine.CHINESE_NAME,
                            extra={"step": step_name, "keys": injected_keys})
        elif data:
            raise ValueError(f"[{step_name}] data å¿…é¡»æ˜¯ dictï¼Œå®é™…ä¸º {type(data)}")
        else:
            logger.info("âšª data ä¸ºç©ºï¼Œè·³è¿‡æ³¨å…¥", module_name=MetaCognitiveEngine.CHINESE_NAME, extra={"step": step_name})

    def _assemble_final_data(
            self,
            context: Dict[str, Any],
            basic_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç»„è£…æœ€ç»ˆçš„æœ‰æ•ˆæ•°æ®ç»“æ„ã€‚
        - ä¿ç•™ basic_data çš„å®Œæ•´éª¨æ¶ï¼ˆå°¤å…¶æ˜¯ meta ç»“æ„ï¼‰
        - ä» context ä¸­æå–éæ’é™¤å­—æ®µæ³¨å…¥ result é¡¶å±‚
        - ä¸º result ä¸­çš„ participants æ¯ä¸ªå®ä½“ç”Ÿæˆ entity_id
        - è®¡ç®— privacy_level å¹¶æ³¨å…¥ meta.privacy_scope
        """
        result = deepcopy(basic_data)

        excluded_fields = {"user_input", "llm_model"}

        # ç¬¬ä¸€æ­¥ï¼šæ³¨å…¥éæ’é™¤å­—æ®µ
        for key, value in context.items():
            if key.startswith("__") or key in excluded_fields:
                continue
            result[key] = value

        # ç¬¬äºŒæ­¥ï¼šå¤„ç† participantsï¼ˆç›´æ¥æ“ä½œ resultï¼‰
        participants = result.get("participants")
        if isinstance(participants, list) and participants:
            processed_participants = []
            for p in participants:
                if isinstance(p, dict) and "entity" in p and isinstance(p["entity"], str):
                    unique_suffix = uuid.uuid4().hex[:8]
                    p_new = deepcopy(p)
                    p_new["entity_id"] = f"{p['entity']}_{unique_suffix}"
                    processed_participants.append(p_new)
                else:
                    processed_participants.append(deepcopy(p))
            result["participants"] = processed_participants

        # ç¬¬ä¸‰æ­¥ï¼šéšç§åº¦è®¡ç®—ï¼ˆä»åŸºäº contextï¼Œå› ä¸º count éœ€è¦æ’é™¤é€»è¾‘ï¼‰
        count = sum(
            1 for k in context.keys()
            if not k.startswith("__") and k not in excluded_fields
        )

        privacy_score = count * 0.05

        # æ¨ç†å±‚
        inference = context.get("inference")
        if isinstance(inference, dict):
            has_inference = (
                    (isinstance(inference.get("events"), list) and len(inference["events"]) > 0) or
                    (isinstance(inference.get("summary"), str) and inference["summary"].strip()) or
                    (isinstance(inference.get("evidence"), list) and len(inference["evidence"]) > 0)
            )
            if has_inference:
                privacy_score += 0.05

        # æ·±åº¦åˆ†æå±‚ï¼ˆ+0.05ï¼‰
        explicit_motivation = context.get("explicit_motivation")
        if isinstance(explicit_motivation, dict):
            has_explicit_motivation = (
                    (isinstance(explicit_motivation.get("summary"), str) and explicit_motivation["summary"].strip()) or
                    (isinstance(explicit_motivation.get("core_driver"), list) and len(
                        explicit_motivation["core_driver"]) > 0) or
                    (isinstance(explicit_motivation.get("power_asymmetry"), dict) and explicit_motivation[
                        "power_asymmetry"]) or
                    (isinstance(explicit_motivation.get("narrative_distortion"), dict) and explicit_motivation[
                        "narrative_distortion"])
            )
            if has_explicit_motivation:
                privacy_score += 0.05

        # åˆç†å»ºè®®å±‚ï¼ˆ+0.1ï¼‰
        rational_advice = context.get("rational_advice")
        if isinstance(rational_advice, dict):
            has_rational_advice = (
                    (isinstance(rational_advice.get("summary"), str) and rational_advice["summary"].strip()) or
                    (isinstance(rational_advice.get("safety_first_intervention"), list) and len(
                        rational_advice["safety_first_intervention"]) > 0) or
                    (isinstance(rational_advice.get("incremental_strategy"), list) and len(
                        rational_advice["incremental_strategy"]) > 0)
            )
            if has_rational_advice:
                privacy_score += 0.1

        privacy_level = min(round(privacy_score, 2), 1.0)

        # æ³¨å…¥ meta
        meta = result.setdefault("meta", {})
        privacy_scope = meta.setdefault("privacy_scope", {})
        privacy_scope["privacy_level"] = float(privacy_level)

        # å…³é”®ï¼šæ¸…ç† result ä¸­â€œå…¨ç©ºâ€çš„é¡¶å±‚å­—æ®µ
        keys_to_remove = []
        for key, value in result.items():
            if not self._is_value_effective(value):
                keys_to_remove.append(key)

        for key in keys_to_remove:
            del result[key]
        return result

    def _is_value_effective(self, value) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªå€¼æ˜¯å¦â€œæœ‰æ•ˆâ€ï¼ˆå³ä¸åº”è¢«è§†ä¸ºç©ºï¼‰ã€‚
        - å­—ç¬¦ä¸²ï¼šéç©ºä¸”éçº¯ç©ºç™½ â†’ æœ‰æ•ˆ
        - åˆ—è¡¨/å…ƒç»„ï¼šè‡³å°‘ä¸€ä¸ªå…ƒç´ æœ‰æ•ˆ â†’ æœ‰æ•ˆ
        - å­—å…¸ï¼šè‡³å°‘ä¸€ä¸ª value æœ‰æ•ˆ â†’ æœ‰æ•ˆ
        - None / ç©ºå­—ç¬¦ä¸² / ç©º list / ç©º dict â†’ æ— æ•ˆ
        - bool / int / float â†’ ä¸€å¾‹è§†ä¸ºæœ‰æ•ˆï¼ˆå¦‚ privacy_level=0.0 æ˜¯æœ‰æ•ˆçš„ï¼‰
        """
        if value is None:
            return False
        if isinstance(value, str):
            return bool(value.strip())
        if isinstance(value, (list, tuple)):
            return any(self._is_value_effective(item) for item in value)
        if isinstance(value, dict):
            return any(self._is_value_effective(v) for v in value.values())
        # bool, int, float, etc.
        return True

    @staticmethod
    def build_user_input_only(
            prompt_template: str,
            context: Dict[str, Any],
            context_desc_info: List
    ) -> str:
        user_input_text = context.get("user_input", "")
        build_context_desc = f"### USER_INPUT BEGINï¼ˆç”¨æˆ·åŸå§‹è¾“å…¥å¼€å§‹ï¼‰\n{user_input_text}\n### USER_INPUT ENDï¼ˆç”¨æˆ·åŸå§‹è¾“å…¥ç»“æŸï¼‰\n"
        context_desc_info.append(build_context_desc)
        rendered_prompt = f"{prompt_template}{build_context_desc}"
        return rendered_prompt

    def build_parallel_context(
            self,
            step_name: str,
            context: Dict[str, Any],
            context_desc_info: List
    ):
        """
        æ„å»ºæœ€ç»ˆæ¸²æŸ“åçš„ promptï¼Œæ”¯æŒåŠ¨æ€æè¿°ç”Ÿæˆã€‚
        """
        field_config = self._step_type_to_config.get(step_name)
        wrapped_desc = ""
        if field_config:
            try:
                raw_desc = self.prompter.generate_description(
                    context=context,
                    field_config=field_config,
                    prefix=""
                )
                start_marker = ""
                end_marker = ""
                readable = ""
                if raw_desc:
                    if step_name == LLM_PARTICIPANTS_EXTRACTION:
                        start_marker = "### PARTICIPANTS_VALID_INFORMATION BEGIN"
                        end_marker = "### PARTICIPANTS_VALID_INFORMATION END"
                        readable = "å‚ä¸è€…æœ‰æ•ˆä¿¡æ¯ä¸Šä¸‹æ–‡"
                    elif step_name == LLM_INFERENCE:
                        start_marker = "### INFERENCE_CONTEXT BEGIN"
                        end_marker = "### INFERENCE_CONTEXT END"
                        readable = "åˆç†æ¨æ¼”æœ‰æ•ˆä¿¡æ¯ä¸Šä¸‹æ–‡"
                    elif step_name == LLM_EXPLICIT_MOTIVATION_EXTRACTION:
                        start_marker = "### EXPLICIT_MOTIVATION_CONTEXT BEGIN"
                        end_marker = "### EXPLICIT_MOTIVATION_CONTEXT END"
                        readable = "æ˜¾æ€§åŠ¨æœºæœ‰æ•ˆä¿¡æ¯ä¸Šä¸‹æ–‡"

                    wrapped_desc = self._wrap_with_context_markers(
                        raw_desc, start_marker, end_marker, readable
                    )
                    context_desc_info.append(wrapped_desc)
            except Exception as e:
                logger.error(f"[{step_name}] åŠ¨æ€æè¿°ç”Ÿæˆå¤±è´¥: {e}")

    def build_serial_context_batch(self, context: Dict[str, Any]) -> str:
        """
        éå† context ä¸­æ‰€æœ‰é¡¶çº§å­—æ®µï¼ˆé __ å¼€å¤´ï¼Œéç³»ç»Ÿå­—æ®µï¼‰ï¼Œ
        æŸ¥æ‰¾å…¶å¯¹åº”çš„ step_typeï¼Œç”¨å®Œæ•´ field_tuples ç”Ÿæˆæè¿°ã€‚
        """
        excluded = {"user_input", "llm_model", "participants"}
        descriptions = []

        for key, value in context.items():
            if key.startswith("__") or key in excluded:
                continue

            # æŸ¥æ‰¾è¯¥é¡¶çº§å­—æ®µå…³è”çš„æ‰€æœ‰ step_type
            step_types = self._top_field_to_step_types.get(key)
            if not step_types:
                continue

            for st in step_types:
                field_tuples = self._step_type_to_config.get(st)
                if not field_tuples:
                    continue

                try:
                    desc = self.prompter.generate_description(
                        context=context,
                        field_config=field_tuples,
                        prefix=""
                    )
                    if desc.strip():
                        descriptions.append(desc.strip())
                except Exception as e:
                    logger.error(f"ç”Ÿæˆå­—æ®µ {key} çš„æè¿°å¤±è´¥ (step_type={st}): {e}")

        full_content = "\n".join(descriptions)
        return self._wrap_with_context_markers(
            full_content,
            "### PERCEPTUAL_CONTEXT_BATCH BEGIN",
            "### PERCEPTUAL_CONTEXT_BATCH END",
            "æ‰¹é‡æ„ŸçŸ¥å±‚ä¸Šä¸‹æ–‡"
        )

    @staticmethod
    def _validate_l0(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒåŸå§‹è¾“å…¥æœ‰æ•ˆæ€§ï¼ˆL0ï¼‰"""
        errors = []
        required_top = {"id", "type", "timestamp", "source", "meta"}
        for field in required_top:
            if field not in result:
                errors.append(f"L0ç¼ºå¤±é¡¶å±‚å­—æ®µ: {field}")

        source = result.get("source", {})
        content = source.get("content")
        if not isinstance(content, str) or len(content.strip()) < 10:
            errors.append("L0: source.content å¿…é¡»ä¸ºéç©ºå­—ç¬¦ä¸²ä¸”é•¿åº¦â‰¥10")

        return len(errors) == 0, errors

    @staticmethod
    def _validate_l1(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒè¯­ä¹‰ç»“æ„æœ‰æ•ˆæ€§ï¼ˆL1ï¼‰â€”â€”æ¯ä¸ªæ¨¡å—å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
        (A) é¡¶å±‚ summaryï¼ˆéç©ºstrï¼‰+ evidenceï¼ˆéç©ºlistï¼‰
        (B) events ä¸­è‡³å°‘ä¸€ä¸ª item å« semantic_notationï¼ˆéç©ºstrï¼‰+ evidenceï¼ˆéç©ºlistï¼‰
        åªè¦ä»»ä¸€æ¨¡å—åŒæ—¶æ»¡è¶³ A å’Œ Bï¼ŒL1 å³æœ‰æ•ˆã€‚
        """
        errors = []
        l1_valid = False
        present_but_empty = []
        missing_or_invalid = []

        for mod_name in SEMANTIC_MODULES_L1:
            mod = result.get(mod_name)
            if mod is None:
                missing_or_invalid.append(f"{mod_name} (ç¼ºå¤±)")
                continue

            if isinstance(mod, dict):
                # --- (A) é¡¶å±‚å•å…ƒå¿…é¡»æœ‰æ•ˆ ---
                top_summary = mod.get("summary")
                top_evidence = mod.get("evidence")
                top_valid = (
                        isinstance(top_summary, str) and top_summary.strip() and
                        isinstance(top_evidence, list) and len(top_evidence) > 0
                )

                # --- (B) events ä¸­å¿…é¡»è‡³å°‘æœ‰ä¸€ä¸ªå®Œæ•´äº‹ä»¶é¡¹ ---
                events = mod.get("events")
                event_valid = False
                if isinstance(events, list) and len(events) > 0:
                    for item in events:
                        if isinstance(item, dict):
                            notation = item.get("semantic_notation")
                            evi = item.get("evidence")
                            if (
                                    isinstance(notation, str) and notation.strip() and
                                    isinstance(evi, list) and len(evi) > 0
                            ):
                                event_valid = True
                                break

                # --- æ¨¡å—æœ‰æ•ˆæ¡ä»¶ï¼šA AND B ---
                if top_valid and event_valid:
                    l1_valid = True
                else:
                    reasons = []
                    if not top_valid:
                        reasons.append("é¡¶å±‚ summary/evidence æ— æ•ˆ")
                    if not event_valid:
                        reasons.append("events ä¸­ç¼ºå°‘å« semantic_notation+evidence çš„æœ‰æ•ˆé¡¹")
                    present_but_empty.append(f"{mod_name} ({'; '.join(reasons)})")

            elif isinstance(mod, list):
                # å…¼å®¹æ—§ç»“æ„ï¼ˆè™½å·²ç»Ÿä¸€ä¸º dictï¼Œä½†ä¿ç•™ï¼‰
                # æ³¨æ„ï¼šlist ç»“æ„æ— æ³•åŒæ—¶æ»¡è¶³ A+Bï¼ˆæ— é¡¶å±‚å­—æ®µï¼‰ï¼Œæ•…è§†ä¸ºæ— æ•ˆ
                present_but_empty.append(f"{mod_name} (æ¨¡å—ä¸ºåˆ—è¡¨ï¼Œæ— æ³•æ»¡è¶³åŒé‡è¦æ±‚)")
            else:
                missing_or_invalid.append(f"{mod_name} (ç±»å‹é”™è¯¯: {type(mod)})")

        if not l1_valid:
            if present_but_empty:
                errors.append("å­˜åœ¨è¯­ä¹‰æ¨¡å—ä½†æœªåŒæ—¶æ»¡è¶³é¡¶å±‚ä¸äº‹ä»¶æœ‰æ•ˆæ€§: " + ", ".join(present_but_empty))
            if missing_or_invalid:
                errors.append("å…³é”®è¯­ä¹‰æ¨¡å—ç¼ºå¤±æˆ–æ ¼å¼é”™è¯¯: " + ", ".join(missing_or_invalid))

        return l1_valid, errors

    @staticmethod
    def _validate_l2(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """æ ¡éªŒè®¤çŸ¥å¹²é¢„æœ‰æ•ˆæ€§ï¼ˆL2ï¼‰â€”â€”å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
        1. inference æ¨¡å—ï¼šé¡¶å±‚ summary+evidence æœ‰æ•ˆ AND events ä¸­è‡³å°‘ä¸€é¡¹å« semantic_notation+evidence
        2. explicit_motivation æ¨¡å—ï¼šåŒä¸Šç»“æ„ï¼ŒåŒæ ·åŒé‡è¦æ±‚
        3. rational_advice æ¨¡å—ï¼šsummary+evidence æœ‰æ•ˆ AND è‡³å°‘ä¸€ä¸ªå»ºè®®å­—æ®µæœ‰å®è´¨å†…å®¹
        """
        errors = []
        l2_ok = True

        # -----------------------------
        # 1. Validate inference
        # -----------------------------
        inference = result.get("inference")
        if not isinstance(inference, dict):
            errors.append("L2: inference ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            # (A) Top-level
            top_summary = inference.get("summary")
            top_evidence = inference.get("evidence")
            top_valid = (
                    isinstance(top_summary, str) and top_summary.strip() and
                    isinstance(top_evidence, list) and len(top_evidence) > 0
            )

            # (B) Events
            events = inference.get("events")
            event_valid = False
            if isinstance(events, list):
                for item in events:
                    if isinstance(item, dict):
                        sn = item.get("semantic_notation")
                        evi = item.get("evidence")
                        if (
                                isinstance(sn, str) and sn.strip() and
                                isinstance(evi, list) and len(evi) > 0
                        ):
                            event_valid = True
                            break

            if not (top_valid and event_valid):
                reasons = []
                if not top_valid:
                    reasons.append("é¡¶å±‚ summary/evidence æ— æ•ˆ")
                if not event_valid:
                    reasons.append("events ä¸­æ—  semantic_notation+evidence æœ‰æ•ˆé¡¹")
                errors.append(f"L2: inference æœªåŒæ—¶æ»¡è¶³åŒé‡è¦æ±‚ ({'; '.join(reasons)})")
                l2_ok = False

        # -----------------------------
        # 2. Validate explicit_motivation
        # -----------------------------
        explicit_motivation = result.get("explicit_motivation")
        if not isinstance(explicit_motivation, dict):
            errors.append("L2: explicit_motivation ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            # (A) Top-level
            top_summary = explicit_motivation.get("summary")
            top_evidence = explicit_motivation.get("evidence")
            top_valid = (
                    isinstance(top_summary, str) and top_summary.strip() and
                    isinstance(top_evidence, list) and len(top_evidence) > 0
            )

            # (B) Events
            events = explicit_motivation.get("events")
            event_valid = False
            if isinstance(events, list):
                for item in events:
                    if isinstance(item, dict):
                        sn = item.get("semantic_notation")
                        evi = item.get("evidence")
                        if (
                                isinstance(sn, str) and sn.strip() and
                                isinstance(evi, list) and len(evi) > 0
                        ):
                            event_valid = True
                            break

            if not (top_valid and event_valid):
                reasons = []
                if not top_valid:
                    reasons.append("é¡¶å±‚ summary/evidence æ— æ•ˆ")
                if not event_valid:
                    reasons.append("events ä¸­æ—  semantic_notation+evidence æœ‰æ•ˆé¡¹")
                errors.append(f"L2: explicit_motivation æœªåŒæ—¶æ»¡è¶³åŒé‡è¦æ±‚ ({'; '.join(reasons)})")
                l2_ok = False

        # -----------------------------
        # 3. Validate rational_advice
        # -----------------------------
        rational_advice = result.get("rational_advice")
        if not isinstance(rational_advice, dict):
            errors.append("L2: rational_advice ç¼ºå¤±æˆ–éå­—å…¸")
            l2_ok = False
        else:
            # rational_advice æ—  eventsï¼Œåªæœ‰é¡¶å±‚å­—æ®µ
            summary = rational_advice.get("summary")
            evidence = rational_advice.get("evidence")
            has_summary_evidence = (
                    isinstance(summary, str) and summary.strip() and
                    isinstance(evidence, list) and len(evidence) > 0
            )

            # Check if any substantive advice field is non-empty
            substantive_fields = {
                "safety_first_intervention",
                "systemic_leverage_point",
                "incremental_strategy",
                "stakeholder_tradeoffs",
                "long_term_exit_path",
                "cultural_adaptation_needed",
                "fallback_plan"
            }
            has_substantive_content = False
            for field in substantive_fields:
                val = rational_advice.get(field)
                if val not in (None, "", [], {}):
                    # For stakeholder_tradeoffs (dict), check if it has non-empty subfields
                    if isinstance(val, dict):
                        if any(v not in (None, "", [], {}) for v in val.values()):
                            has_substantive_content = True
                            break
                    else:
                        has_substantive_content = True
                        break

            if not (has_summary_evidence and has_substantive_content):
                reasons = []
                if not has_summary_evidence:
                    reasons.append("summary æˆ– evidence æ— æ•ˆ")
                if not has_substantive_content:
                    reasons.append("æ— å®è´¨æ€§å»ºè®®å†…å®¹")
                errors.append(f"L2: rational_advice æ— æ•ˆ ({'; '.join(reasons)})")
                l2_ok = False

        return l2_ok, errors

    def _validate_final_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸è¿›å¼ä¸‰çº§æœ‰æ•ˆæ€§æ ¡éªŒï¼ˆå•æ¬¡è¯­ä¹‰æ¨¡å—éå†ï¼‰ï¼Œè¿”å› validity_level ä¸åˆ†å±‚é”™è¯¯ã€‚
        """
        # L0: åŸå§‹è¾“å…¥
        l0_valid, errors_l0 = self._validate_l0(result)
        if not l0_valid:
            return {
                "__success": False,
                "__validity_level": "invalid",
                "__final_validation_errors": {"L0": errors_l0, "L1": [], "L2": []}
            }

        # L1: è¯­ä¹‰ç»“æ„
        l1_valid, errors_l1 = self._validate_l1(result)

        # L2: è®¤çŸ¥å¹²é¢„ï¼ˆä»…å½“ L1 æœ‰æ•ˆæ—¶å°è¯•ï¼‰
        l2_valid, errors_l2 = self._validate_l2(result) if l1_valid else (False, [])

        # ç¡®å®šæœ€ç»ˆçº§åˆ«
        level = (
            "L2_actionable" if l2_valid else
            "L1_structured" if l1_valid else
            "L0_raw"
        )

        return {
            "__success": True,
            "__validity_level": level,
            "__final_validation_errors": {
                "L0": errors_l0,
                "L1": errors_l1,
                "L2": errors_l2
            }
        }

    @staticmethod
    def _aggregate_step_results(all_step_results: List[Dict[str, Any]], raw_response_records: Dict) -> Dict[str, Any]:
        """
        èšåˆæ‰€æœ‰æ­¥éª¤çš„ç»“æœï¼Œä»…æ”¶é›†é”™è¯¯ä¿¡æ¯å’Œæ‰§è¡ŒçŠ¶æ€ã€‚
        æ³¨æ„ï¼š__success å­—æ®µä¸åœ¨æ­¤å¤„è®¾ç½®ï¼Œç•™ç©ºï¼ˆç”± _validate_final_result å†³å®šï¼‰ã€‚
        """
        system_errors = []
        api_errors = []
        validation_errors_all = []
        all_valid = True
        partial_success = False

        for step in all_step_results:
            if step.get("__success"):
                partial_success = True

            if not step.get("__valid_structure"):
                all_valid = False

            sys_err = step.get("__system_error")
            if sys_err:
                system_errors.append({
                    "step": step.get("step_name"),
                    "error": sys_err
                })

            api_err = step.get("__api_error")
            if api_err:
                api_errors.append({
                    "step": step.get("step_name"),
                    "error": api_err
                })

            val_errs = step.get("__validation_errors")
            if val_errs:
                validation_errors_all.append({
                    "step": step.get("step_name"),
                    "errors": val_errs
                })

            prompt_type = step.get("prompt_type")
            if prompt_type and prompt_type == PREPROCESSING:
                raw_response_records[PREPROCESSING].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })
            elif prompt_type and prompt_type == PARALLEL:
                raw_response_records[PARALLEL].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })
            else:
                raw_response_records[SERIAL].append({
                    "step_name": step.get("step_name"),
                    "raw_response": step.get("__raw_response")
                })

        return {
            "__valid_structure": all_valid,
            "__partial_success": partial_success,
            "__errors_summary": {
                "system_errors": system_errors,
                "api_errors": api_errors,
                "validation_errors": validation_errors_all,
                # é¢„ç•™ä½ç½®ç»™æœ€ç»ˆæ ¡éªŒé”™è¯¯ï¼Œç”¨ä¸åŒ key é¿å…å†²çª
                "final_validation_errors": []  # åç»­ç”± _validate_final_result å¡«å…¥
            }
        }

    @staticmethod
    def _build_top_field_to_step_types() -> Dict[str, List[str]]:
        """
        ä» REQUIRED_FIELDS_BY_CATEGORY ä¸­æå–æ‰€æœ‰é¡¶çº§å­—æ®µï¼ˆå¦‚ 'participants'ï¼‰ï¼Œ
        å¹¶è®°å½•å®ƒä»¬æ‰€å±çš„ step_typeï¼ˆå¦‚ LLM_SOURCE_EXTRACTIONï¼‰ã€‚
        """
        mapping: Dict[str, List[str]] = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, field_tuples in steps.items():
                for field_path, *_ in field_tuples:
                    # æå–é¡¶çº§å­—æ®µåï¼šå–ç¬¬ä¸€ä¸ª '.' ä¹‹å‰çš„éƒ¨åˆ†
                    top_field = field_path.split('.')[0]
                    if top_field not in mapping:
                        mapping[top_field] = []
                    if step_name not in mapping[top_field]:
                        mapping[top_field].append(step_name)
        return mapping

    @staticmethod
    def _build_step_type_to_config() -> Dict[str, List[Tuple]]:
        config_map = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, tuples in steps.items():
                if step_name not in config_map:
                    config_map[step_name] = []
                config_map[step_name].extend(tuples)
        return config_map

    def _render_report_to_html(self, data: Dict[str, Any]) -> Optional[Path]:
        """
        å°† result æ•°æ®æ³¨å…¥ HTML æ¨¡æ¿ï¼Œç”ŸæˆæŠ¥å‘Šã€‚
        - è¾“å‡ºç›®å½•ï¼šconfig.STATIC_REPORTS_DIR
        - æ–‡ä»¶åï¼šé€šè¿‡ self.file_util.generate_filename ç”Ÿæˆ
        - å‰ç¼€ï¼š"æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š"
        - åç¼€ï¼š".html"
        - æ¨¡æ¿è¯»å–ï¼šå¤ç”¨ self.file_util.read_file
        - æ–‡ä»¶å†™å…¥ï¼šå¤ç”¨ self.file_util.write_file
        - ä¸Šä¸‹æ–‡å˜é‡åï¼šdata
        """
        try:
            if not data or not isinstance(data, dict):
                return None

            # 1. ç”Ÿæˆæ–‡ä»¶å
            filename = self.file_util.generate_filename(
                prefix="æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š",
                suffix=".html",
                include_timestamp=True
            )

            # 2. ç¡®å®šè¾“å‡ºè·¯å¾„
            output_path = config.REPORTS_DIR / filename

            # 3. è¯»å–æ¨¡æ¿ï¼ˆå¤ç”¨å·¥å…·ï¼‰
            template_content = self.file_util.read_file(
                str(config.FILE_DEFAULT_TEMPLATE_PATH),
                encoding="utf-8",
                auto_decode=False
            )
            if not template_content:
                logger.error("âŒ æ¨¡æ¿æ–‡ä»¶ä¸ºç©ºæˆ–è¯»å–å¤±è´¥", extra={"template_path": str(config.FILE_DEFAULT_TEMPLATE_PATH)})
                return None

            # 4. æ¸²æŸ“æ¨¡æ¿
            from jinja2 import Template
            html_output = Template(template_content).render(data=data)

            # 5. âœ… å†™å…¥ HTMLï¼ˆå¤ç”¨ file_util.write_fileï¼‰
            success = self.file_util.write_file(
                file_path=str(output_path),
                content=html_output,
                encoding="utf-8",
                as_json=False,
                file_type="html"  # â† è®©æ—¥å¿—æ˜¾ç¤ºâ€œHTMLâ€ï¼ˆè‹¥ write_file æ”¯æŒè¯¥å‚æ•°ï¼‰
            )

            if not success:
                logger.error("âŒ HTML æŠ¥å‘Šå†™å…¥å¤±è´¥", extra={"path": str(output_path)})
                return None

            logger.info("ğŸ“„ HTML æŠ¥å‘Šå·²ç”Ÿæˆ", extra={"path": str(output_path)})
            return output_path

        except Exception as e:
            logger.exception("ğŸ’¥ HTML æŠ¥å‘Šç”Ÿæˆå¤±è´¥", extra={"error": str(e)})
            return None

    async def _execute_suggestion(self, prompt: str) -> str:
        logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆ LLM å»ºè®®å†…å®¹", module_name=self.CHINESE_NAME)
        try:
            backend = await self.backend
            result = await backend.generate_text(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
            )
            if result.startswith("ç”Ÿæˆå¤±è´¥"):
                logger.warning("âš ï¸ LLM å»ºè®®ç”Ÿæˆå¤±è´¥", extra={"error": result})
            else:
                logger.info("âœ… LLM å»ºè®®ç”ŸæˆæˆåŠŸ", module_name=self.CHINESE_NAME)
            return result
        except Exception as e:
            error_msg = f"LLM å»ºè®®ç”Ÿæˆå¼‚å¸¸: {type(e).__name__}: {str(e)}"
            logger.exception(error_msg, module_name=self.CHINESE_NAME)
            return error_msg

    async def _inject_suggestion_into_result(
            self,
            result: Dict[str, Any],
            user_input: str,
            suggestion_type: str,
            title: str = "æ–‡æœ¬å¤šæ¨¡æ€æ„ŸçŸ¥åˆ†ææŠ¥å‘Š"
    ) -> None:
        """
        ä¸ºå·²éªŒè¯é€šè¿‡çš„ result æ³¨å…¥ LLM ç”Ÿæˆçš„å»ºè®®å†…å®¹ã€‚
        - æ ‡é¢˜æ³¨å…¥åˆ° result['meta']['title']
        - å»ºè®®å†…å®¹æ³¨å…¥åˆ° result['analysis']['suggestion']ï¼ˆå«å…ƒä¿¡æ¯ï¼‰
        - ä¿è¯å³ä½¿å¤±è´¥ä¹Ÿä¸ç ´å result ç»“æ„
        """
        # ç¡®ä¿ meta å’Œ analysis å­˜åœ¨
        result.setdefault("meta", {})
        result.setdefault("analysis", {})

        # å…ˆè®¾ç½®æ ‡é¢˜ï¼ˆæ€»æ˜¯æˆåŠŸï¼‰
        result["meta"]["title"] = title

        try:
            suggestion_prompt = self.prompter.build_suggestion(
                template_name=CATEGORY_SUGGESTION,
                user_input=user_input,
                suggestion_type=suggestion_type
            )
            suggestion_content = await self._execute_suggestion(suggestion_prompt)

            # æ„é€ å¸¦å…ƒä¿¡æ¯çš„ suggestion å¯¹è±¡
            suggestion_record = {
                "content": suggestion_content,
                "type": suggestion_type,
                "model": self.llm_model,
                "generated_at": int(time.time()),
                "success": not (suggestion_content.startswith(("ç”Ÿæˆå¤±è´¥", "[å»ºè®®ç”Ÿæˆå¤±è´¥")))
            }

            result["analysis"]["suggestion"] = suggestion_record
            logger.info(
                "âœ… LLM å»ºè®®å·²æ³¨å…¥ç»“æœ",
                extra={
                    "suggestion_type": suggestion_type,
                    "model": self.llm_model,
                    "success": suggestion_record["success"]
                }
            )
        except Exception as e:
            error_msg = f"[å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}]"
            logger.exception(
                "âš ï¸ æ³¨å…¥ LLM å»ºè®®å¤±è´¥",
                extra={
                    "suggestion_type": suggestion_type,
                    "error": str(e)
                }
            )
            # å³ä½¿å¤±è´¥ä¹Ÿå†™å…¥ç»“æ„åŒ–å ä½ï¼Œä¾¿äºå‰ç«¯/åç»­å¤„ç†
            result["analysis"]["suggestion"] = {
                "content": error_msg,
                "type": suggestion_type,
                "model": self.llm_model,
                "generated_at": int(time.time()),
                "success": False
            }

    @staticmethod
    def _open_report_in_browser(outpath: Path) -> None:
        try:
            import webbrowser
            webbrowser.open(f"file://{outpath}")
            logger.info("ğŸŒ å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š", extra={"outpath": str(outpath)})
        except Exception as e:
            logger.warning("âŒ æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨", extra={"error": str(e)})

    def _build_participants_context_desc(self, context: Dict[str, Any]) -> str:
        """åŸºäºåˆæ³•å‚ä¸è€…é›†åˆç”Ÿæˆä¸Šä¸‹æ–‡æè¿°å­—ç¬¦ä¸²"""
        legit_items = sorted(self._build_legitimate_participant_set(context))  # æ’åºä¿è¯è¾“å‡ºç¨³å®šï¼ˆä¾¿äºç¼“å­˜/è°ƒè¯•ï¼‰
        if not legit_items:
            return ""

        prefix = "### LEGITIMATE_PARTICIPANTS BEGINï¼ˆåˆæ³•çš„å‚ä¸è€…å®ä½“æˆ–è§’è‰²å¼€å§‹ï¼‰\n"
        suffix = "\n### LEGITIMATE_PARTICIPANTS ENDï¼ˆåˆæ³•çš„å‚ä¸è€…å®ä½“æˆ–è§’è‰²ç»“æŸï¼‰\n"
        return prefix + "\n".join(legit_items) + suffix

    @staticmethod
    def _build_legitimate_participant_set(context: Dict[str, Any]) -> Set[str]:
        """ä» context['participants'] æ„å»ºåˆæ³•æ ‡è¯†é›†åˆï¼ˆentity + nameï¼‰"""
        participants = context.get("participants", [])
        if not isinstance(participants, list):
            return set()

        legit_set = set()
        for p in participants:
            if not isinstance(p, dict):
                continue
            entity = p.get("entity")
            name = p.get("name")
            if isinstance(entity, str) and entity.strip():
                legit_set.add(entity.strip())
            if isinstance(name, str) and name.strip():
                legit_set.add(name.strip())
        return legit_set

    async def _filter_perception_results_by_legitimate_participants(
            self,
            result: Dict[str, Any],
            legitimate_participants: Set[str]
    ) -> None:
        """
        è¿‡æ»¤æ„ŸçŸ¥ç»“æœä¸­çš„éæ³• experiencerã€‚
        - ä»…ä¿ç•™ experiencer å±äº legitimate_participants çš„äº‹ä»¶ï¼›
        - æ”¯æŒä¸¤é˜¶æ®µè§£æï¼š
            1. ç®€å•ä»£è¯æ˜ å°„ï¼ˆå¦‚ "ä»–" â†’ "å¼ ä¸‰"ï¼‰
            2. LLM æ‰¹é‡å…œåº•æŒ‡ä»£æ¶ˆè§£ï¼ˆæœ€åæ‰‹æ®µï¼‰
        """
        logger.info(f"â†’ è¿›å…¥æ„ŸçŸ¥ç»“æœè¿‡æ»¤æµç¨‹ï¼ˆåˆæ³•å‚ä¸è€…: {sorted(legitimate_participants)}ï¼‰", extra={"module_name": self.CHINESE_NAME})

        if not isinstance(result, dict):
            return

        step_name = result.get("step_name")
        if step_name not in PERCEPTION_LAYERS:
            return

        data = result.get("data")
        if not isinstance(data, dict) or not data:
            return

        try:
            key, block = next(iter(data.items()))
        except StopIteration:
            return

        if not (isinstance(block, dict) and isinstance(block.get("events"), list)):
            return

        original_events = block["events"]
        if not original_events:
            return

        logger.info(
            f"â†’ å¾…å¤„ç†äº‹ä»¶ experiencer åˆ—è¡¨: {[e.get('experiencer') for e in original_events if isinstance(e, dict)]}",
            extra={"module_name": self.CHINESE_NAME})

        # ç¬¬ä¸€æ­¥ï¼šæ‰«æäº‹ä»¶ï¼Œæ ‡è®°åˆæ³•é¡¹ï¼Œå¹¶æ”¶é›†éœ€ LLM æ¶ˆè§£çš„ä»£è¯
        valid_indices: Set[int] = set()
        pronoun_map: Dict[int, str] = {}  # idx -> pronoun

        for idx, evt in enumerate(original_events):
            if not isinstance(evt, dict):
                continue

            exp = evt.get("experiencer")
            if not isinstance(exp, str):
                continue

            # æƒ…å†µ1ï¼šå·²åœ¨åˆæ³•åå•ä¸­
            if exp in legitimate_participants:
                valid_indices.add(idx)
                continue

            # æƒ…å†µ2ï¼šå°è¯•ç®€å•æ˜ å°„
            resolved = self._try_simple_resolution(exp, legitimate_participants)
            if resolved is not None:
                evt["experiencer"] = resolved  # åŸåœ°æ›´æ–°
                valid_indices.add(idx)
                continue

            # æƒ…å†µ3ï¼šéœ€ LLM å…œåº•
            pronoun_map[idx] = exp

        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è°ƒç”¨ LLM å…œåº•ï¼ˆä»…å½“æœ‰æœªè§£æé¡¹ï¼‰
        llm_resolved: Dict[int, str] = {}
        if pronoun_map:
            try:
                llm_resolved = await self._perform_coreference_resolution(
                    index_to_pronoun=pronoun_map,
                    legitimate_participants=legitimate_participants
                )
            except Exception as e:
                logger.exception(
                    "LLM å…œåº•æŒ‡ä»£æ¶ˆè§£å¤±è´¥ï¼Œè·³è¿‡",
                    extra={"error": str(e), "module_name": self.CHINESE_NAME}
                )
                llm_resolved = {}

        # åº”ç”¨ LLM è§£æç»“æœï¼ˆåŸåœ°æ›´æ–°ï¼‰
        for idx, name in llm_resolved.items():
            if 0 <= idx < len(original_events) and isinstance(original_events[idx], dict):
                original_events[idx]["experiencer"] = name
                valid_indices.add(idx)

        # ç¬¬ä¸‰æ­¥ï¼šæŒ‰åŸå§‹é¡ºåºä¿ç•™æœ‰æ•ˆäº‹ä»¶
        filtered_events = [
            original_events[i] for i in range(len(original_events)) if i in valid_indices
        ]

        # æ›´æ–° block
        block["events"] = filtered_events

        # æ¸…ç†ç©ºå—
        if not filtered_events:
            block["evidence"] = [] if isinstance(block.get("evidence"), list) else []
            block["summary"] = "" if isinstance(block.get("summary"), str) else ""

        # æ—¥å¿—
        perception_type = step_name.replace("LLM_PERCEPTION_", "").replace("_EXTRACTION", "").lower()
        removed = len(original_events) - len(filtered_events)
        if removed > 0:
            kept_exps = [evt.get("experiencer") for evt in filtered_events if isinstance(evt, dict)]
            removed_exps = [
                original_events[i].get("experiencer")
                for i in range(len(original_events))
                if i not in valid_indices and isinstance(original_events[i], dict)
            ]
            logger.info(
                f"ğŸ§¹ æ„ŸçŸ¥å±‚ [{perception_type}] è¿‡æ»¤å®Œæˆï¼šä¿ç•™ {kept_exps}ï¼Œä¸¢å¼ƒ {removed_exps}",
                extra={"module_name": self.CHINESE_NAME}
            )
        else:
            all_exps = [evt.get("experiencer") for evt in original_events if isinstance(evt, dict)]
            logger.info(
                f"âœ… æ„ŸçŸ¥å±‚ [{perception_type}] å…¨éƒ¨ä¿ç•™ï¼š{all_exps}",
                extra={"module_name": self.CHINESE_NAME}
            )

    def _try_simple_resolution(self, experiencer: str, legitimate_participants: Set[str]) -> Optional[str]:
        """
        å°è¯•å°†ä»£è¯æˆ–æ¨¡ç³ŠæŒ‡ç§°è§£æä¸ºå…·ä½“çš„åˆæ³•å‚ä¸è€…ã€‚

        ç­–ç•¥ï¼š
          1. è‹¥å·²æ˜¯åˆæ³•å â†’ è¿”å›è‡ªèº«
          2. è‹¥å« [uncertain] æ ‡è®° â†’ æ¸…ç†ååˆ¤æ–­
          3. è‹¥ä¸º EXCLUDED_PRONOUNS â†’ è¿”å› Noneï¼ˆä¸æ˜ å°„ï¼‰
          4. è‹¥ä¸º CHINESE_PRONOUNS ä¸”åˆæ³•å‚ä¸è€…å”¯ä¸€ â†’ æ˜ å°„åˆ°è¯¥å”¯ä¸€å‚ä¸è€…
          5. å…¶ä»–æƒ…å†µ â†’ æ— æ³•è§£æï¼Œè¿”å› None
        """
        logger.debug(f"â†’ å°è¯•ç®€å•æŒ‡ä»£è§£æ: '{experiencer}'", extra={"module_name": self.CHINESE_NAME})

        if not isinstance(experiencer, str) or not legitimate_participants:
            return None

        # å·²æ˜¯åˆæ³•å‚ä¸è€…
        if experiencer in legitimate_participants:
            return experiencer

        # æ¸…ç†å¯èƒ½çš„ uncertain æ ‡è®°ï¼ˆå…¼å®¹ LLM è¾“å‡ºï¼‰
        clean_exp = experiencer
        if "[uncertain]" in clean_exp:
            clean_exp = clean_exp.replace("[uncertain]", "").strip()
        if "(uncertain)" in clean_exp:
            clean_exp = clean_exp.replace("(uncertain)", "").strip()

        # å†ä¸€æ¬¡åˆ¤æ–­ï¼Œé¿å…æç«¯æƒ…å†µ
        if clean_exp in legitimate_participants:
            logger.debug(f"â† æ¸…ç†ååŒ¹é…åˆæ³•å‚ä¸è€…: '{clean_exp}'", extra={"module_name": self.CHINESE_NAME})
            return clean_exp

        # æ˜ç¡®æ’é™¤çš„ä»£è¯ï¼ˆå¦‚â€œåˆ«äººâ€ï¼‰â†’ ä¸æ˜ å°„
        if clean_exp in EXCLUDED_PRONOUNS:
            return None

        # å¯å°è¯•æ˜ å°„çš„ä»£è¯
        if clean_exp in CHINESE_PRONOUNS:
            # ä»…å½“åˆæ³•å‚ä¸è€…å”¯ä¸€æ—¶ï¼Œæ‰å®‰å…¨æ˜ å°„
            if len(legitimate_participants) == 1:
                resolved = next(iter(legitimate_participants))
                logger.debug(f"â† ä»£è¯æ˜ å°„æˆåŠŸ: '{experiencer}' â†’ '{resolved}'", extra={"module_name": self.CHINESE_NAME})
                return resolved
            else:
                # å¤šäººåœºæ™¯ï¼Œæ— æ³•ç¡®å®š â†’ ä¸æ˜ å°„
                return None

        # éä»£è¯ä¸”éåˆæ³•å â†’ æ— æ³•å¤„ç†
        return None

    async def _perform_coreference_resolution(
            self,
            index_to_pronoun: Dict[int, str],
            legitimate_participants: Set[str]
    ) -> Dict[int, str]:
        """
        æ‰§è¡Œæ‰¹é‡æŒ‡ä»£æ¶ˆè§£ï¼Œç›´æ¥è°ƒç”¨ bottom_dissolving_pronounsã€‚

        è¾“å…¥ï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ä»£è¯}
        è¾“å‡ºï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ç¡®å®šçš„åˆæ³•å‚ä¸è€…å}ï¼ˆä¸ç¡®å®šçš„ä¸è¿”å›ï¼‰
        """
        logger.info(f"â†’ å¯åŠ¨ LLM æŒ‡ä»£æ¶ˆè§£ï¼ˆå¾…è§£æä»£è¯: {list(index_to_pronoun.values())}ï¼‰",
                    extra={"module_name": self.CHINESE_NAME})

        if not index_to_pronoun or not legitimate_participants:
            return {}

        # æ„é€  prompt
        try:
            prompt = self.prompter._build_coref_prompt(
                user_input=self.user_input,
                legitimate_participants=legitimate_participants,
                index_to_pronoun=index_to_pronoun
            )
        except Exception as e:
            logger.warning(
                "æ„å»ºæŒ‡ä»£æ¶ˆè§£ prompt å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        try:
            backend = await self.backend
            resolved_from_llm: Dict[int, str] = await backend.bottom_dissolving_pronouns(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params
            )
        except Exception as e:
            logger.exception(
                "è°ƒç”¨ bottom_dissolving_pronouns å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        # åº•å±‚å·²ä¿è¯ï¼šresolved_from_llm æ˜¯åˆæ³• dictï¼Œå¤±è´¥æ—¶è¿”å› {}
        # æˆ‘ä»¬åªéœ€åšæœ€ç»ˆæ ¡éªŒï¼škey æ˜¯å¦åœ¨è¾“å…¥ä¸­ï¼Œvalue æ˜¯å¦åœ¨åˆæ³•åå•é‡Œ
        resolved_map: Dict[int, str] = {}
        for idx, name in resolved_from_llm.items():
            if isinstance(idx, int) and isinstance(name, str):
                if idx in index_to_pronoun and name in legitimate_participants:
                    resolved_map[idx] = name

        logger.info(f"â† LLM æ¶ˆè§£ç»“æœ: {resolved_map}", extra={"module_name": self.CHINESE_NAME})
        return resolved_map

    @staticmethod
    def _wrap_with_context_markers(
            content: str,
            start_marker: str,
            end_marker: str,
            human_readable_name: str = ""
    ) -> str:
        """ç»Ÿä¸€åŒ…è£…ä¸Šä¸‹æ–‡ç‰‡æ®µï¼Œå¸¦å¯é…ç½®è¾¹ç•Œ"""
        if not content.strip():
            return ""
        readable_start = f"ï¼ˆ{human_readable_name}å¼€å§‹ï¼‰" if human_readable_name else ""
        readable_end = f"ï¼ˆ{human_readable_name}ç»“æŸï¼‰" if human_readable_name else ""
        return (
            f"{start_marker}{readable_start}\n"
            f"{content.strip()}\n"
            f"{end_marker}{readable_end}\n"
        )
