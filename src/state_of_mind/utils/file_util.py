import os
import re
import shutil
import threading
import time
import uuid
import zipfile
from pathlib import Path
from typing import Set, List, Union, Tuple, Dict
import glob
import json
import pandas as pd
import chardet

from src.state_of_mind.utils.logger import LoggerManager as logger


class FileUtil:
    """
    æ–‡ä»¶æ“ä½œå·¥å…·ç±»ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    æä¾›æ–‡ä»¶è¯»å†™ã€ç¼–ç æ£€æµ‹ã€ç›®å½•æ“ä½œã€JSON å¤„ç†ã€åœç”¨è¯åŠ è½½ç­‰å¸¸ç”¨åŠŸèƒ½
    """

    CHINESE_NAME = "FileUtil"
    _instance = None
    _lock = threading.Lock()

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if not self._initialized:
            logger.info("ğŸ“ FileUtil åˆå§‹åŒ–å®Œæˆï¼ˆå•ä¾‹æ¨¡å¼ï¼‰")
            self._initialized = True

    # ===================== æ–‡ä»¶è¯»å†™ç›¸å…³ =====================

    @staticmethod
    def read_file(file_path: str, encoding: str = "utf-8", auto_decode: bool = False) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒè‡ªåŠ¨ç¼–ç æ£€æµ‹"""
        try:
            if auto_decode:
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                result = chardet.detect(raw_data)
                detected_encoding = result['encoding'] or 'utf-8'
                content = raw_data.decode(detected_encoding, errors='ignore')
                logger.info(f"ğŸ” è‡ªåŠ¨æ£€æµ‹ç¼–ç è¯»å–: {file_path} -> {detected_encoding}")
                return content
            else:
                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                    content = f.read()
                logger.info(f"ğŸ“– æŒ‡å®šç¼–ç è¯»å–: {file_path} ({encoding})", module_name=FileUtil.CHINESE_NAME)
                return content
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {e}", exc_info=True)
            return ""

    @staticmethod
    def file_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
            result = chardet.detect(raw_data)
            encoding = result['encoding']
            logger.debug(f"ğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {file_path} -> {encoding}")
            return encoding
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æ–‡ä»¶ç¼–ç å¤±è´¥: {file_path} - {e}")
            return "unknown"

    def write_file(self, file_path: str, content, encoding: str = "utf-8", as_json: bool = False,
                   file_type: str = "text") -> bool:
        """
        å†™å…¥æ–‡ä»¶ï¼Œè‡ªåŠ¨åˆ›å»ºçˆ¶ç›®å½•ã€‚

        :param file_path: æ–‡ä»¶è·¯å¾„
        :param content: å†…å®¹ï¼ˆstr æˆ– dict/list å½“ as_json=Trueï¼‰
        :param encoding: ç¼–ç 
        :param as_json: æ˜¯å¦ä»¥ JSON æ ¼å¼å†™å…¥
        :param file_type: å†…å®¹ç±»å‹æç¤ºï¼Œç”¨äºæ—¥å¿—ï¼ˆå¦‚ "html", "text", "log"ï¼‰ï¼Œä¸å½±å“å†™å…¥é€»è¾‘
        """
        try:
            self.ensure_directory(os.path.dirname(file_path))
            with open(file_path, 'w', encoding=encoding) as f:
                if as_json:
                    json.dump(content, f, ensure_ascii=False, indent=4)
                    logger.debug(f"ğŸ“¦ å†™å…¥ JSON æ–‡ä»¶: {file_path}")
                else:
                    f.write(str(content))
                    # æ ¹æ® file_type æ˜¾ç¤ºæ›´å‹å¥½çš„æ—¥å¿—
                    type_display = file_type.upper() if file_type != "text" else "æ–‡æœ¬"
                    logger.debug(f"ğŸ“ å†™å…¥ {type_display} æ–‡ä»¶: {file_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {file_path} - {e}", exc_info=True)
            return False

    # ===================== æ–‡ä»¶åç”Ÿæˆ =====================
    @staticmethod
    def generate_filename(prefix: str, suffix: str = ".json", include_timestamp: bool = True) -> str:
        """
        ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„å”¯ä¸€æ–‡ä»¶å
        æ ¼å¼: {prefix}_{uuid8}_{timestamp}.xxx
        :param prefix: å‰ç¼€ï¼Œå¦‚ category å
        :param suffix: åç¼€ï¼Œé»˜è®¤ .json
        :param include_timestamp: æ˜¯å¦åŒ…å«æ—¶é—´æˆ³
        :return: æ–‡ä»¶åå­—ç¬¦ä¸²
        """
        safe_prefix = re.sub(r'[<>:"/\\|?*\x00-\x1F]', '_', prefix)  # è¿‡æ»¤éæ³•å­—ç¬¦
        unique_id = str(uuid.uuid4())[:8]
        timestamp = str(int(time.time()))
        if include_timestamp:
            filename = f"{safe_prefix}_{unique_id}_{timestamp}{suffix}"
        else:
            filename = f"{safe_prefix}_{unique_id}{suffix}"
        return filename

    # ===================== ä¸“ç”¨ JSON å†™å…¥æ–¹æ³• =====================
    def write_json(self, data: dict, file_path: Union[str, Path], indent: int = 4,
                   ensure_ascii: bool = False) -> bool:
        """
        ä¸“ç”¨ JSON å†™å…¥æ–¹æ³•ï¼Œè‡ªåŠ¨åˆ›å»ºç›®å½•ï¼Œæ”¯æŒ Path å¯¹è±¡
        :param data: è¦å†™å…¥çš„å­—å…¸æ•°æ®
        :param file_path: æ–‡ä»¶è·¯å¾„ï¼ˆstr æˆ– Pathï¼‰
        :param indent: JSON ç¼©è¿›
        :param ensure_ascii: æ˜¯å¦è½¬ä¹‰é ASCII å­—ç¬¦
        :return: æ˜¯å¦æˆåŠŸ
        """
        path = None
        try:
            path = Path(file_path)
            self.ensure_directory(path.parent)  # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆå·²æœ‰æ–¹æ³•ï¼‰
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=ensure_ascii, indent=indent)
            logger.debug(f"âœ… æˆåŠŸå†™å…¥ JSON æ–‡ä»¶: {path}")
            return True
        except Exception as e:
            logger.error(f"âŒ å†™å…¥ JSON æ–‡ä»¶å¤±è´¥: {path} - {e}", exc_info=True)
            return False

    @staticmethod
    def ensure_directory(dir_path: Union[str, Path]) -> bool:
        """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {dir_path} - {e}", exc_info=True)
            return False

    # ===================== æ–‡ä»¶/ç›®å½•æ“ä½œ =====================

    @staticmethod
    def delete_file(file_path: str) -> bool:
        """åˆ é™¤æŒ‡å®šæ–‡ä»¶"""
        try:
            if os.path.isfile(file_path):
                os.remove(file_path)
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤æ–‡ä»¶: {file_path}")
                return True
            logger.debug(f"â„¹ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
            return False
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
            return False

    @staticmethod
    def delete_dir(dir_path: str) -> bool:
        """é€’å½’åˆ é™¤ç›®å½•åŠå†…å®¹"""
        try:
            if os.path.isdir(dir_path):
                shutil.rmtree(dir_path)
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç›®å½•: {dir_path}")
                return True
            logger.debug(f"â„¹ï¸ ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {dir_path}")
            return False
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç›®å½•å¤±è´¥: {dir_path} - {e}", exc_info=True)
            return False

    @staticmethod
    def list_files(dir_path: str, ext_filter: Union[str, None] = None) -> List[str]:
        """åˆ—å‡ºç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼ˆæ”¯æŒåç¼€è¿‡æ»¤ï¼‰"""
        files = []
        try:
            for root, _, filenames in os.walk(dir_path):
                for filename in filenames:
                    if ext_filter is None or filename.endswith(ext_filter):
                        file_path = os.path.join(root, filename)
                        files.append(file_path)
            logger.debug(f"ğŸ“„ æ‰«æç›®å½•: {dir_path}, æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {dir_path} - {e}")
        return files

    def replace_in_file(self, file_path: str, old_str: str, new_str: str) -> bool:
        """æ›¿æ¢æ–‡ä»¶ä¸­çš„æŸæ®µæ–‡å­—"""
        try:
            content = self.read_file(file_path)
            if not content:
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸ºç©ºæˆ–è¯»å–å¤±è´¥ï¼Œæ— æ³•æ›¿æ¢: {file_path}")
                return False
            new_content = content.replace(old_str, new_str)
            if content == new_content:
                logger.debug(f"ğŸ”„ æ›¿æ¢å†…å®¹æœªå˜åŒ–: {file_path}")
            else:
                logger.info(f"ğŸ”„ æ›¿æ¢æˆåŠŸ: '{old_str}' -> '{new_str}' in {file_path}")
            return self.write_file(file_path, new_content)
        except Exception as e:
            logger.error(f"âŒ æ›¿æ¢æ–‡ä»¶å†…å®¹å¤±è´¥: {file_path} - {e}")
            return False

    @staticmethod
    def ensure_newline_at_end(file_path: str) -> bool:
        """ç¡®ä¿æ–‡ä»¶ç»“å°¾æœ‰æ¢è¡Œç¬¦"""
        try:
            if not os.path.exists(file_path):
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            with open(file_path, 'ab+') as f:
                f.seek(-1, os.SEEK_END)
                last_char = f.read(1)
                if last_char != b'\n':
                    f.write(b'\n')
                    logger.debug(f"â†©ï¸ è¡¥å……æ¢è¡Œç¬¦: {file_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ç¡®ä¿æ–‡ä»¶ç»“å°¾æ¢è¡Œå¤±è´¥: {file_path} - {e}")
            return False

    # ===================== JSON æ“ä½œ =====================

    def read_json_file(self, file_path: str) -> Dict:
        """è¯»å– JSON æ–‡ä»¶"""
        try:
            content = self.read_file(file_path)
            if not content:
                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸ºç©º: {file_path}")
                return {}
            data = json.loads(content)
            logger.info(f"ğŸ“¥ æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {file_path}")
            return data
        except Exception as e:
            logger.error(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {file_path} - {e}")
            return {}

    def read_all_json_files_in_dir(self, dir_path: str) -> List:
        """è¯»å–ç›®å½•ä¸‹æ‰€æœ‰ JSON æ–‡ä»¶å¹¶åˆå¹¶"""
        all_data = []
        files = self.list_files(dir_path, ".json")
        logger.info(f"ğŸ“‚ å¼€å§‹è¯»å– {len(files)} ä¸ª JSON æ–‡ä»¶...")
        for file in files:
            data = self.read_json_file(file)
            if isinstance(data, list):
                all_data.extend(data)
            else:
                all_data.append(data)
        logger.info(f"âœ… åˆå¹¶å®Œæˆï¼Œå…± {len(all_data)} æ¡æ•°æ®")
        return all_data

    # ===================== DataFrame æ“ä½œ =====================

    def read_json_to_dataframe(self, file_path: str) -> pd.DataFrame:
        """è¯»å– JSON æ–‡ä»¶è½¬ä¸º DataFrame"""
        data = self.read_json_file(file_path)
        df = pd.DataFrame(data)
        logger.info(f"ğŸ“Š å·²åŠ è½½ DataFrame: {file_path} -> {df.shape}")
        return df

    @staticmethod
    def save_dataframe_to_json(df: pd.DataFrame, file_path: str) -> bool:
        """ä¿å­˜ DataFrame ä¸º JSON"""
        try:
            df.to_json(file_path, orient='records', ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ä¿å­˜ DataFrame ä¸º JSON: {file_path} ({len(df)} æ¡)")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ DataFrame å¤±è´¥: {file_path} - {e}")
            return False

    # ===================== åœç”¨è¯ä¸å“ç‰Œè¯åŠ è½½ =====================

    def load_stopwords(self, filepath: str) -> Set[str]:
        """åŠ è½½åœç”¨è¯æ–‡ä»¶"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = (line.strip() for line in f)
                stopwords = {line for line in lines if line}
            logger.info(f"âœ… æˆåŠŸåŠ è½½åœç”¨è¯: {filepath} -> {len(stopwords)} ä¸ª")
            return stopwords
        except FileNotFoundError:
            logger.error(f"âŒ åœç”¨è¯æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
            return set()
        except Exception as e:
            logger.error(f"âŒ è¯»å–åœç”¨è¯æ–‡ä»¶å¤±è´¥: {filepath} - {e}", exc_info=True)
            return set()

    def load_brands(self, filepath: str) -> Set[str]:
        """åŠ è½½å“ç‰Œè¯æ–‡ä»¶ï¼ˆè‹±æ–‡æˆ–å¸¸è§ç¼©å†™ï¼‰"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = (line.strip().lower() for line in f)
                brands = {line for line in lines if line and not line.startswith('#')}
            logger.info(f"âœ… æˆåŠŸåŠ è½½å“ç‰Œè¯: {filepath} -> {len(brands)} ä¸ª")
            return brands
        except FileNotFoundError:
            logger.error(f"âŒ è‹±æ–‡å“ç‰Œè¯æ–‡ä»¶æœªæ‰¾åˆ°: {filepath}")
            return set()
        except Exception as e:
            logger.error(f"âŒ è¯»å–è‹±æ–‡å“ç‰Œè¯æ–‡ä»¶å¤±è´¥: {filepath} - {e}", exc_info=True)
            return set()

    # ===================== ç›®å½•æœç´¢ =====================

    def find_directories(self, base_dir: str, keywords: Union[str, List[str], Tuple[str, ...]]) -> List[str]:
        """æœç´¢åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®å­—çš„ç›®å½•ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰"""
        base_dir = os.path.normpath(base_dir)
        logger.info(f"ğŸ” å¼€å§‹åœ¨ç›®å½• {base_dir} ä¸­æœç´¢åŒ…å«ä»¥ä¸‹ä»»ä¸€å…³é”®å­—çš„å­ç›®å½•: {keywords}")

        if not os.path.exists(base_dir):
            logger.error(f"ğŸš« åŸºç¡€ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return []

        if isinstance(keywords, str):
            keywords = [keywords]
        elif not isinstance(keywords, (list, tuple)):
            logger.error("ğŸš¨ keywords å¿…é¡»æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å…ƒç»„")
            raise TypeError("keywords å¿…é¡»æ˜¯å­—ç¬¦ä¸²ã€åˆ—è¡¨æˆ–å…ƒç»„")

        matched_dirs = []
        for d in os.listdir(base_dir):
            dir_path = os.path.join(base_dir, d)
            if os.path.isdir(dir_path) and any(kw.lower() in d.lower() for kw in keywords):
                matched_dirs.append(d)

        full_paths = [os.path.join(base_dir, d) for d in matched_dirs]
        logger.info(f"âœ… æ‰¾åˆ° {len(full_paths)} ä¸ªåŒ¹é…ç›®å½•: {full_paths}")
        return full_paths

    def read_json_files_in_dir(self, dir_path: str) -> pd.DataFrame:
        """è¯»å–æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶å¹¶åˆå¹¶ä¸ºä¸€ä¸ª DataFrame"""
        dir_path = os.path.normpath(dir_path)

        if not os.path.isdir(dir_path):
            logger.error(f"ğŸš« æ— æ•ˆç›®å½•: {dir_path}")
            return pd.DataFrame()

        logger.info(f"ğŸ“‚ å¼€å§‹è¯»å–ç›®å½• {dir_path} ä¸‹çš„æ‰€æœ‰ JSON æ–‡ä»¶")
        json_files = glob.glob(os.path.join(dir_path, "*.json"))
        logger.info(f"ğŸ“„ å…±æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶")

        if not json_files:
            logger.warning(f"âš ï¸ æœªåœ¨ç›®å½• {dir_path} ä¸­æ‰¾åˆ°ä»»ä½• .json æ–‡ä»¶")
            return pd.DataFrame()

        dfs = []
        for file in json_files:
            try:
                with open(file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                if isinstance(data, list) and all(isinstance(item, dict) for item in data):
                    df = pd.DataFrame(data)
                    dfs.append(df)
                    has_ts = any('timestamp' in item for item in data)
                    logger.debug(f"ğŸ“Š æ–‡ä»¶ {file} {'å«' if has_ts else 'ä¸å«'} timestamp å­—æ®µï¼Œå·²åŠ è½½")
                else:
                    logger.error(f"âŒ æ–‡ä»¶ {file} çš„ç»“æ„ä¸åˆæ³•ï¼Œåº”ä¸ºå­—å…¸åˆ—è¡¨")
            except Exception as e:
                logger.error(f"ğŸ’¥ è¯»å– JSON æ–‡ä»¶å¤±è´¥: {file} - {e}", exc_info=True)

        if dfs:
            combined_df = pd.concat(dfs, ignore_index=True)
            logger.info(f"âœ… æˆåŠŸåˆå¹¶ {len(dfs)} ä¸ª JSON æ–‡ä»¶ï¼Œæ€»æ•°æ®è¡Œæ•°: {len(combined_df)}")
            return combined_df
        else:
            logger.warning("ğŸ“­ æœªè¯»å–åˆ°ä»»ä½•æœ‰æ•ˆçš„ JSON æ•°æ®")
            return pd.DataFrame()

    # ===================== ZIP æ‰“åŒ… =====================

    def zip_task_dir(self, datacleaner) -> str:
        """å°†æ•´ä¸ªä»»åŠ¡ç›®å½•æ‰“åŒ…ä¸º ZIP"""
        try:
            task_dir = Path(datacleaner.task_dir)
            zip_path = task_dir.with_suffix(".zip")

            if not task_dir.exists():
                logger.error(f"âŒ æ— æ³•å‹ç¼©: ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨ {datacleaner.task_dir}")
                return ""

            if zip_path.exists():
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤å·²å­˜åœ¨çš„ ZIP æ–‡ä»¶: {zip_path}")
                zip_path.unlink()

            logger.info(f"ğŸ“¦ å¼€å§‹å‹ç¼©ç›®å½•: {datacleaner.task_dir} -> {zip_path}")

            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file in task_dir.rglob("*"):
                    if file.is_file():
                        arcname = file.relative_to(task_dir.parent)
                        zipf.write(file, arcname=arcname)
                        logger.debug(f"ğŸ“ æ·»åŠ æ–‡ä»¶åˆ° ZIP: {arcname}")

            zip_size_kb = zip_path.stat().st_size / 1024
            logger.info(f"âœ… å‹ç¼©å®Œæˆ: {zip_path} (å¤§å°: {zip_size_kb:.2f} KB)")
            return str(zip_path)

        except Exception as e:
            logger.error(f"âŒ å‹ç¼©ç›®å½•å¤±è´¥: {e}", exc_info=True)
            return ""
