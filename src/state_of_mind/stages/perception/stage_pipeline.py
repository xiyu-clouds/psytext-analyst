import asyncio
import time
import uuid
from typing import List, Any, Tuple, Dict, Optional
from src.state_of_mind.cache.base import BaseCache
from src.state_of_mind.cache.redis import RedisLLMCache
from src.state_of_mind.stages.perception.prompt_builder import PromptBuilder
from src.state_of_mind.cache.llm_cache import LLMCache
from src.state_of_mind.config import config
from src.state_of_mind.utils.async_decorators import async_timed
from .constants import REQUIRED_FIELDS_BY_CATEGORY, LLM_PARTICIPANTS_EXTRACTION, \
    CATEGORY_RAW, PARALLEL_PREPROCESSING, PARALLEL_PERCEPTION, PARALLEL_HIGH_ORDER, \
    SERIAL_SUGGESTION, OTHER, ALLOWED_PARALLEL_PERCEPTION_MARKERS, ALLOWED_SERIAL_SUGGESTION_MARKERS, \
    ALLOWED_PARALLEL_HIGH_ORDER_MARKERS, PARALLEL_PERCEPTION_KEYS
from src.state_of_mind.utils.file_util import FileUtil
from src.state_of_mind.utils.logger import LoggerManager as logger
from .context_builder import ContextBuilder
from .executor import StepExecutor
from .participant_filter import ParticipantFilter
from .report_generator import ReportGenerator
from .result_assembler import ResultAssembler
from ...common.llm_response import LLMResponse
from ...common.raw_data_factory import create_raw_basic_data
from ...utils.concurrency_manager import ConcurrencyManager
from src.state_of_mind.core.types import StageProtocol


class PerceptionPipeline(StageProtocol):
    CHINESE_NAME = "ç¬¬ä¸€é˜¶æ®µï¼šå…¨æ¯æ„ŸçŸ¥åŸºåº•"
    REPORT_URL_PREFIX = "/reports/"
    RAW_DATA_DIR = config.DATA_YUAN_RAW_DIR
    DYE_VAT_DIR = config.DATA_YUAN_DYE_VAT_DIR

    def __init__(
            self,
            backend_name: Optional[str] = None,
            llm_model: Optional[str] = None,
            recommended_params: Optional[dict] = None
    ):
        self.backend_name = backend_name or config.LLM_BACKEND
        self.llm_model = llm_model or config.LLM_MODEL
        self.recommended_params = recommended_params or config.LLM_RECOMMENDED_PARAMS or {}
        current_parallel_concurrency = config.get("CURRENT_PARALLEL_CONCURRENCY", 3)
        self.concurrency_manager = ConcurrencyManager(current_parallel_concurrency)
        self.prompt_builder = PromptBuilder()
        self.prompt_result = None
        self.llm_cache = self._create_cache_backend(config)
        self.file_util = FileUtil()
        self.report_generator = ReportGenerator(self.file_util)
        self.step_executor = StepExecutor(self.backend_name, self.llm_model, self.recommended_params, self.llm_cache,
                                          self.prompt_builder)
        self.result_assembler = ResultAssembler(self.llm_model, self.prompt_builder, self.step_executor)
        self._top_field_to_step_types = self._build_top_field_to_step_types()
        self._step_type_to_config = self._build_step_type_to_config()
        self._participant_filter = None
        self._context_builder = None
        self._participant_filter_lock = asyncio.Lock()
        self._context_builder_lock = asyncio.Lock()
        logger.info(f"PerceptionPipeline åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ backend: {self.backend_name}, model: {self.llm_model}")

    @staticmethod
    def _create_cache_backend(c) -> BaseCache:
        storage = c.STORAGE_BACKEND
        if storage == c.STORAGE_LOCAL:
            return LLMCache(
                max_size=c.LLM_CACHE_MAX_SIZE,
                ttl_seconds=c.LLM_CACHE_TTL
            )
        elif storage == c.STORAGE_REDIS:
            return RedisLLMCache(config=c, default_ttl=c.LLM_CACHE_TTL)
        else:
            raise ValueError(f"Unsupported storage backend: {storage}")

    async def _get_participant_filter(self):
        if self._participant_filter is None:
            async with self._participant_filter_lock:
                if self._participant_filter is None:
                    backend = await self.step_executor.get_backend()
                    self._participant_filter = ParticipantFilter(self.prompt_builder, backend)
        return self._participant_filter

    async def _get_context_builder(self):
        if self._context_builder is None:
            async with self._context_builder_lock:
                if self._context_builder is None:
                    participant_filter = await self._get_participant_filter()
                    self._context_builder = ContextBuilder(
                        self.prompt_builder,
                        participant_filter,
                        self._step_type_to_config,
                        self._top_field_to_step_types
                    )
        return self._context_builder

    async def run(self, user_input: str, category: str = CATEGORY_RAW, **kwargs) -> Dict[str, Any]:
        return await self.async_extract(
            user_input=user_input,
            template_name=category,
            suggestion_type=config.SUGGESTION_TYPE,
            title=config.REPORT_TITLE,
            **kwargs
        )

    async def run_batch(self, user_inputs: List[str], category: str = CATEGORY_RAW, **kwargs) -> List[Dict[str, Any]]:
        if not user_inputs:
            return []
        # å¹¶å‘æ‰§è¡Œï¼Œä¿æŒé¡ºåºï¼Œä»»ä¸€å¤±è´¥åˆ™æŠ›å‡º
        results = await asyncio.gather(
            *(self.run(inp, category, **kwargs) for inp in user_inputs)
        )
        return list(results)

    @async_timed
    async def async_extract(self, template_name: str, user_input: str, suggestion_type: str,
                            title: str = "å…¨æ¯æ„ŸçŸ¥åŸºåº•", **template_vars) -> Dict[str, Any]:
        """å¼‚æ­¥æ ¸å¿ƒæµç¨‹"""
        trace_id = str(uuid.uuid4())
        logger.set_trace_id(trace_id)
        context = template_vars.copy()
        context["user_input"] = user_input
        context["llm_model"] = self.llm_model

        if not template_name or not isinstance(template_name, str):
            raise ValueError("template_name å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
        if user_input is not None and not isinstance(user_input, str):
            raise TypeError("user_input å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")

        cache_key = self.llm_cache.make_key(template_name, **context)
        logger.info(f"æ•´ä½“ç¼“å­˜ key: {cache_key[:8]}...")
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                report_url = cached_data.get("meta", {}).get("report_url", "")
                res = {"report_url": report_url}
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={"template": template_name, "report_url": report_url})
                return res

        self.prompt_result = self.prompt_builder.build_raw()
        preprocessing_prompts = self.prompt_result["preprocessing_prompts"]
        perception_prompts = self.prompt_result["perception_prompts"]
        high_order_prompts = self.prompt_result["high_order_prompts"]
        suggestion_prompts = self.prompt_result["suggestion_prompts"]
        basic_data = create_raw_basic_data(user_input, self.llm_model)

        all_step_results = []
        prompt_records = {PARALLEL_PREPROCESSING: [], PARALLEL_PERCEPTION: [], PARALLEL_HIGH_ORDER: [], SERIAL_SUGGESTION: [], OTHER: []}
        raw_response_records = {PARALLEL_PREPROCESSING: [], PARALLEL_PERCEPTION: [], PARALLEL_HIGH_ORDER: [], SERIAL_SUGGESTION: [], OTHER: []}
        context_desc_info = []

        await self._run_preprocessing_parallel_async(
            preprocessing_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # === åŠ¨æ€è¿‡æ»¤ï¼šä»…ä½¿ç”¨ context ===
        filtered_parallel_prompts = [
            (step_name, driven_by, prompt)
            for (step_name, driven_by, prompt) in perception_prompts
            if context.get("pre_screening", {}).get(driven_by, False)
        ]

        await self._run_perception_parallel_async(
            filtered_parallel_prompts, context, template_name, cache_key, all_step_results, prompt_records,
            context_desc_info
        )

        # åˆ¤æ–­æ˜¯å¦å¯ç”¨é«˜é˜¶æ¨ç†
        eligible = context.get("eligibility", {}).get("eligible", False)
        if eligible:
            has_valid_perception = any(
                key in context and bool(context[key])
                for key in PARALLEL_PERCEPTION_KEYS
            )
            if has_valid_perception:
                await self._run_high_order_parallel_async(
                    high_order_prompts, context, template_name, cache_key,
                    all_step_results, prompt_records, context_desc_info
                )
                await self._run_suggestion_serial_async(
                    suggestion_prompts, context, template_name, cache_key,
                    all_step_results, prompt_records, context_desc_info
                )
            else:
                logger.info("â­ï¸ eligible=true ä½†æ— æœ‰æ•ˆå¹¶è¡Œæ„ŸçŸ¥æ•°æ®ï¼Œè·³è¿‡é«˜é˜¶ç­–ç•¥ã€çŸ›ç›¾ã€æ“æ§ã€å»ºè®®å››æ­¥é“¾")
        else:
            logger.info("â­ï¸ eligible=falseï¼Œè·³è¿‡é«˜é˜¶ç­–ç•¥ã€çŸ›ç›¾ã€æ“æ§ã€å»ºè®®å››æ­¥é“¾")

        result = self.result_assembler.assemble_final_data(context, basic_data)
        valid_result = self.result_assembler.validate_final_result(result)
        is_success = bool(valid_result.get("__success"))
        if is_success:
            # æ³¨å…¥åŸå§‹æ–‡æœ¬è§£è¯»å†…å®¹
            await self.result_assembler.inject_suggestion_into_result(result, user_input, suggestion_type, all_step_results, prompt_records, title)
            # æ³¨å…¥å…¨å±€è¯­ä¹‰æ ‡è¯†
            await self.result_assembler.inject_global_semantic_signature(result, user_input, all_step_results, prompt_records)

        aggregation = self.result_assembler.aggregate_step_results(all_step_results, raw_response_records)
        aggregation["__errors_summary"]["final_validation_errors"] = [
            {"step": "final_validation", "errors": valid_result["__final_validation_errors"]}
        ] if valid_result["__final_validation_errors"] else []
        result["meta"]["validity_level"] = valid_result["__validity_level"]

        # æ³¨æ„ï¼šå³ä½¿å¤±è´¥ï¼Œä¹Ÿè¦æŒä¹…åŒ– dye_vat è¯Šæ–­æ•°æ®
        report_url = await self._persist_extraction_artifacts(
            result=result,
            aggregation=aggregation,
            template_name=template_name,
            user_input=user_input,
            prompt_records=prompt_records,
            raw_response_records=raw_response_records,
            is_success=is_success
        )

        if is_success:
            await self.llm_cache.set(cache_key, result)
            logger.info("âœ… æœ€ç»ˆç»“æœå·²ç¼“å­˜", extra={"cache_key": cache_key})
        else:
            logger.warning("ğŸŸ¡ æå–æµç¨‹æœªå®Œå…¨æˆåŠŸï¼Œè·³è¿‡ç¼“å­˜", extra={
                "cache_key": cache_key,
                "validity_level": valid_result.get("__validity_level"),
                "final_errors": valid_result.get("__final_validation_errors")
            })
        return {"report_url": report_url}

    @async_timed
    async def _run_preprocessing_parallel_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        if not prompts:
            logger.info("â­ï¸ æ— é¢„å¤„ç†ä»»åŠ¡")
            return

        logger.info("âš¡ å¹¶å‘æ‰§è¡Œé¢„å¤„ç†ä»»åŠ¡", extra={"count": len(prompts)})
        context_builder = await self._get_context_builder()

        async def _task(idx: int, step_name: str, driven_by: str, prompt_template: str) -> Dict[str, Any]:
            try:
                async with self.concurrency_manager.semaphore:
                    cache_key = f"{cache_key_base}:{step_name}:{idx}"
                    logger.info(f"âš¡ [{step_name}] ç¼“å­˜ key: ...{cache_key[-10:]}")
                    rendered_prompt = context_builder.build_user_input_context(
                        prompt_template, context["user_input"], context_desc_info
                    )

                    prompt_records[PARALLEL_PREPROCESSING].append({
                        "step_name": step_name,
                        "prompt": rendered_prompt
                    })

                    result = await self.step_executor.execute_step(
                        prompt_template=rendered_prompt,
                        template_name=template_name,
                        step_name=step_name,
                        cache_key=cache_key,
                        prompt_type=PARALLEL_PREPROCESSING
                    )

                    if result.get("__success") is True:
                        try:
                            await self.llm_cache.set(cache_key, result)
                        except Exception as cache_err:
                            logger.warning(
                                f"âš ï¸ é¢„å¤„ç†ä»»åŠ¡ç¼“å­˜å†™å…¥å¤±è´¥ [{step_name}]: {type(cache_err).__name__}: {cache_err}",
                                extra={"step": step_name}
                            )

                    logger.info(f"âœ… é¢„å¤„ç†ä»»åŠ¡ [{step_name}] æ‰§è¡Œå®Œæˆ")
                    return result
            except Exception as e:
                error_msg = str(e)
                logger.error(f"[{step_name}] é¢„å¤„ç†ä»»åŠ¡å¼‚å¸¸: {error_msg}")
                failure_resp = LLMResponse.from_system_error(
                    system_error=error_msg,
                    model=self.llm_model,
                    template_name=template_name,
                    step_name=step_name,
                    prompt_type=PARALLEL_PREPROCESSING,
                    include_traceback=True
                )
                return failure_resp.to_dict()

        tasks = [
            _task(idx, step_name, driven_by, prompt)
            for idx, (step_name, driven_by, prompt) in enumerate(prompts)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=False)

        for idx, result in enumerate(results):
            try:
                step_name = result.get("step_name", f"unknown_preprocessing_{idx}")
                all_step_results.append(result)
                context_builder.update_context_from_result(result, context, step_name)
                if step_name == LLM_PARTICIPANTS_EXTRACTION:
                    context_builder.build_common_context(
                        step_name=step_name,
                        context=context,
                        context_desc_info=context_desc_info
                    )

            except Exception as e:
                system_error = str(e)
                logger.error(
                    f"âš ï¸ é¢„å¤„ç†åå¤„ç†å¤±è´¥ [idx={idx}, step={result.get('step_name', 'unknown')}]: {system_error}"
                )
                fallback_result = LLMResponse.from_system_error(
                    system_error=system_error,
                    model=self.llm_model,
                    template_name=template_name,
                    step_name=result.get("step_name", f"unknown_{idx}"),
                    prompt_type=PARALLEL_PREPROCESSING,
                    include_traceback=True
                )
                all_step_results.append(fallback_result.to_dict())

        success_count = sum(1 for r in results if r.get("__success", False))
        logger.info(
            f"å¹¶è¡Œé¢„å¤„ç†ä»»åŠ¡å®Œæˆ: {len(results)} ä¸ªä»»åŠ¡, æˆåŠŸ {success_count} ä¸ª",
            extra={"total": len(results), "success": success_count}
        )

    @async_timed
    async def _run_perception_parallel_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List,
    ):
        """å¹¶å‘æ‰§è¡Œæ„ŸçŸ¥ä»»åŠ¡"""
        if not prompts:
            logger.info("â­ï¸ æ— å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡")
            return

        logger.info("âš¡ æ‰§è¡Œå¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡", extra={"count": len(prompts)})
        context_builder = await self._get_context_builder()
        participant_filter = await self._get_participant_filter()
        legitimate_participants = participant_filter.build_legitimate_participants_set(context)

        async def _task(idx: int, step_name: str, prompt_template: str) -> Dict[str, Any]:
            try:
                async with self.concurrency_manager.semaphore:
                    cache_key = f"{cache_key_base}:{step_name}:{idx}"
                    logger.info(f"âš¡ [{step_name}] ç¼“å­˜ key: ...{cache_key[-10:]}")

                    allowed_markers = ALLOWED_PARALLEL_PERCEPTION_MARKERS.get(idx, set())
                    rendered_prompt = context_builder.inject_allowed_context(
                        prompt_template, context_desc_info, allowed_markers
                    )

                    prompt_records.setdefault(PARALLEL_PERCEPTION, []).append({
                        "step_name": step_name,
                        "prompt": rendered_prompt
                    })

                    data = await self.step_executor.execute_step(
                        prompt_template=rendered_prompt,
                        template_name=template_name,
                        step_name=step_name,
                        cache_key=cache_key,
                        prompt_type=PARALLEL_PERCEPTION
                    )

                    if data.get("__success") is True:
                        try:
                            await self.llm_cache.set(cache_key, data)
                        except Exception as cache_err:
                            logger.warning(
                                f"âš ï¸ å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡ç¼“å­˜å†™å…¥å¤±è´¥ [{step_name}]: {type(cache_err).__name__}: {cache_err}",
                                extra={"step": step_name}
                            )
                    logger.debug(f"âœ… å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡ [{step_name}] æ‰§è¡Œå®Œæˆ")
                    return data

            except Exception as e:
                error_msg = str(e)
                logger.error(f"[{step_name}] å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡å…œåº•å¼‚å¸¸: {error_msg}")
                failure_resp = LLMResponse.from_system_error(
                    system_error=error_msg,
                    model=self.llm_model,
                    template_name=template_name,
                    step_name=step_name,
                    prompt_type=PARALLEL_PERCEPTION,
                    include_traceback=True
                )
                return failure_resp.to_dict()

        tasks = [
            _task(idx, step_name, prompt)
            for idx, (step_name, driven_by, prompt) in enumerate(prompts)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=False)
        for idx, result in enumerate(results):
            try:
                await participant_filter.filter_perception_results(
                    context["user_input"], result, legitimate_participants, prompt_records, all_step_results
                )
                all_step_results.append(result)
                context_builder.update_context_from_result(
                    result, context, result.get("step_name")
                )
            except Exception as e:
                system_error = str(e)
                logger.error(
                    f"âš ï¸ å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡åå¤„ç†å¤±è´¥ [idx={idx}, step={result.get('step_name', 'unknown')}]: {system_error}"
                )
                fallback_result = LLMResponse.from_system_error(
                    system_error=system_error,
                    model=self.llm_model,
                    template_name=template_name,
                    step_name=result.get("step_name", f"unknown_{idx}"),
                    prompt_type=PARALLEL_PERCEPTION,
                    include_traceback=True
                )
                all_step_results.append(fallback_result.to_dict())

        dynamic_desc = context_builder.build_perception_context_batch(context)
        if dynamic_desc:
            context_desc_info.append(dynamic_desc)

        legit_participants_ctx = context_builder.build_legitimate_participants_context(context)
        if legit_participants_ctx:
            context_desc_info.append(legit_participants_ctx)

        success_count = sum(1 for r in results if r.get("__success", False))
        logger.info(
            f"å¹¶è¡Œæ„ŸçŸ¥ä»»åŠ¡å®Œæˆ: {len(results)} ä¸ªä»»åŠ¡, æˆåŠŸ {success_count} ä¸ª",
            extra={"total": len(results), "success": success_count}
        )

    @async_timed
    async def _run_high_order_parallel_async(
        self,
        prompts: List[Tuple[str, str, str]],
        context: Dict[str, Any],
        template_name: str,
        cache_key_base: str,
        all_step_results: List[Dict],
        prompt_records: Dict,
        context_desc_info: List[str],
    ):
        """
        å¹¶å‘æ‰§è¡Œé«˜é˜¶æ¨ç†ä¸‰æ­¥é“¾ï¼ˆç­–ç•¥é”šå®š / çŸ›ç›¾æš´éœ² / æ“æ§æœºåˆ¶è§£ç ï¼‰
        å‰æï¼šcontext å·²åŒ…å«å®Œæ•´çš„å¹¶è¡Œæ„ŸçŸ¥ç»“æœï¼Œä¸” eligible=True
        """
        if not prompts:
            logger.info("â­ï¸ æ— å¹¶è¡Œé«˜é˜¶ä»»åŠ¡")
            return

        if len(prompts) != 3:
            logger.warning(f"âš ï¸ å¹¶è¡Œé«˜é˜¶ä»»åŠ¡æ•°é‡å¼‚å¸¸ï¼ŒæœŸæœ› 3 ä¸ªï¼Œå®é™… {len(prompts)} ä¸ª")

        logger.info("âš¡ æ‰§è¡Œå¹¶è¡Œé«˜é˜¶ä»»åŠ¡", extra={"count": len(prompts)})
        context_builder = await self._get_context_builder()

        async def _task(idx: int, step_name: str, driven_by: str, prompt_template: str) -> Dict[str, Any]:
            try:
                async with self.concurrency_manager.semaphore:
                    cache_key = f"{cache_key_base}:{step_name}:{idx}"
                    logger.info(f"âš¡ [{step_name}] ç¼“å­˜ key: ...{cache_key[-10:]}")

                    allowed_markers = ALLOWED_PARALLEL_HIGH_ORDER_MARKERS.get(idx, set())
                    rendered_prompt = context_builder.inject_allowed_context(
                        prompt_template, context_desc_info, allowed_markers
                    )

                    prompt_records.setdefault(PARALLEL_HIGH_ORDER, []).append({
                        "step_name": step_name,
                        "prompt": rendered_prompt
                    })

                    result = await self.step_executor.execute_step(
                        prompt_template=rendered_prompt,
                        template_name=template_name,
                        step_name=step_name,
                        cache_key=cache_key,
                        prompt_type=PARALLEL_HIGH_ORDER
                    )

                    if result.get("__success") is True:
                        try:
                            await self.llm_cache.set(cache_key, result)
                        except Exception as cache_err:
                            logger.warning(
                                f"âš ï¸ å¹¶è¡Œé«˜é˜¶ä»»åŠ¡ç¼“å­˜å†™å…¥å¤±è´¥ [{step_name}]: {type(cache_err).__name__}: {cache_err}",
                                extra={"step": step_name}
                            )

                    logger.debug(f"âœ… å¹¶è¡Œé«˜é˜¶ä»»åŠ¡ [{step_name}] æ‰§è¡Œå®Œæˆ")
                    return result

            except Exception as e:
                error_msg = str(e)
                logger.error(f"[{step_name}] å¹¶è¡Œé«˜é˜¶ä»»åŠ¡å¼‚å¸¸: {error_msg}")
                failure_resp = LLMResponse.from_system_error(
                    system_error=error_msg,
                    model=self.llm_model,
                    template_name=template_name,
                    step_name=step_name,
                    prompt_type=PARALLEL_HIGH_ORDER,
                    include_traceback=True
                )
                return failure_resp.to_dict()

        tasks = [
            _task(idx, step_name, driven_by, prompt)
            for idx, (step_name, driven_by, prompt) in enumerate(prompts)
        ]

        results = await asyncio.gather(*tasks, return_exceptions=False)
        for idx, result in enumerate(results):
            step_name = result.get("step_name", f"unknown_high_order_{idx}")
            all_step_results.append(result)
            context_builder.update_context_from_result(result, context, step_name)
            context_builder.build_common_context(step_name, context, context_desc_info)

        success_count = sum(1 for r in results if r.get("__success", False))
        logger.info(
            f"å¹¶è¡Œé«˜é˜¶ä»»åŠ¡å®Œæˆ: {len(results)} ä¸ªä»»åŠ¡, æˆåŠŸ {success_count} ä¸ª",
            extra={"total": len(results), "success": success_count}
        )

    @async_timed
    async def _run_suggestion_serial_async(
            self,
            prompts: List[Tuple[str, str, str]],
            context: Dict[str, Any],
            template_name: str,
            cache_key_base: str,
            all_step_results: List[Dict],
            prompt_records: Dict,
            context_desc_info: List
    ):
        """ä¸²è¡Œæ‰§è¡Œä»»åŠ¡ï¼Œåç»­æ­¥éª¤å¯ä½¿ç”¨å‰é¢æ­¥éª¤æ³¨å…¥çš„å­—æ®µ"""
        if not prompts:
            logger.info("â­ï¸ æ— ä¸²è¡Œæœ€å°å¯è¡Œæ€§å»ºè®®ä»»åŠ¡")
            return

        logger.info("ğŸ” æ‰§è¡Œä¸²è¡Œæœ€å°å¯è¡Œæ€§å»ºè®®ä»»åŠ¡", extra={"count": len(prompts)})
        context_builder = await self._get_context_builder()
        total_steps = len(prompts)

        for idx, (step_name, driven_by, prompt_template) in enumerate(prompts):
            cache_key = f"{cache_key_base}:{step_name}:{idx}"
            logger.info(f"âš¡ [{step_name}] ç¼“å­˜ key: ...{cache_key[-10:]}")
            rendered_prompt = prompt_template

            # === å…³é”®ï¼šæŒ‰ marker åŠ¨æ€ç­›é€‰è¦æ³¨å…¥çš„ä¸Šä¸‹æ–‡ ===
            allowed = ALLOWED_SERIAL_SUGGESTION_MARKERS.get(idx, set())
            rendered_prompt = context_builder.inject_allowed_context(rendered_prompt, context_desc_info, allowed)

            prompt_records[SERIAL_SUGGESTION].append({"step_name": step_name, "prompt": rendered_prompt})
            result = await self.step_executor.execute_step(rendered_prompt, template_name, step_name,
                                                           cache_key, SERIAL_SUGGESTION)
            all_step_results.append(result)
            context_builder.update_context_from_result(result, context, step_name)
            if idx < total_steps - 1:
                context_builder.build_common_context(step_name, context, context_desc_info)

            if result.get("__success") is True:
                try:
                    await self.llm_cache.set(cache_key, result)
                except Exception as cache_err:
                    logger.warning(
                        f"âš ï¸ ä¸²è¡Œæœ€å°å¯è¡Œæ€§å»ºè®®ä»»åŠ¡ç¼“å­˜å†™å…¥å¤±è´¥ [{step_name}]: {type(cache_err).__name__}: {cache_err}",
                        extra={"step": step_name}
                    )
            logger.debug(f"âœ… ä¸²è¡Œæœ€å°å¯è¡Œæ€§å»ºè®®ä»»åŠ¡ [{step_name}] æ‰§è¡Œå®Œæˆ")

    @staticmethod
    def _build_top_field_to_step_types() -> Dict[str, List[str]]:
        """
        ä» REQUIRED_FIELDS_BY_CATEGORY ä¸­æå–æ‰€æœ‰é¡¶çº§å­—æ®µï¼ˆå¦‚ 'participants'ï¼‰ï¼Œ
        å¹¶è®°å½•å®ƒä»¬æ‰€å±çš„ step_typeï¼ˆå¦‚ LLM_SOURCE_EXTRACTIONï¼‰ã€‚
        """
        mapping: Dict[str, List[str]] = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, field_tuples in steps.items():
                for field_path, *_ in field_tuples:
                    # æå–é¡¶çº§å­—æ®µåï¼šå–ç¬¬ä¸€ä¸ª '.' ä¹‹å‰çš„éƒ¨åˆ†
                    top_field = field_path.split('.')[0]
                    if top_field not in mapping:
                        mapping[top_field] = []
                    if step_name not in mapping[top_field]:
                        mapping[top_field].append(step_name)
        return mapping

    @staticmethod
    def _build_step_type_to_config() -> Dict[str, List[Tuple]]:
        config_map = {}
        for category, steps in REQUIRED_FIELDS_BY_CATEGORY.items():
            for step_name, tuples in steps.items():
                if step_name not in config_map:
                    config_map[step_name] = []
                config_map[step_name].extend(tuples)
        return config_map

    @async_timed
    async def _persist_extraction_artifacts(
            self,
            result: Dict[str, Any],
            aggregation: Dict[str, Any],
            template_name: str,
            user_input: str,
            prompt_records: Dict[str, List[Dict]],
            raw_response_records: Dict[str, List[Dict]],
            is_success: bool = True
    ) -> Optional[str]:
        """
        é€šç”¨ç»“æœæŒä¹…åŒ–å‡½æ•°ï¼Œæ— è®ºæˆåŠŸä¸å¦éƒ½ä¿å­˜è¯Šæ–­æ•°æ®ï¼ˆdye vatï¼‰ï¼Œ
        æˆåŠŸæ—¶é¢å¤–ä¿å­˜ç»“æ„åŒ– raw æ•°æ®å’Œç”ŸæˆæŠ¥å‘Šã€‚
        è¿”å› report_urlï¼ˆä»…æˆåŠŸæ—¶éç©ºï¼‰ã€‚
        """
        filename = self.file_util.generate_filename(prefix=template_name, suffix=".json")
        report_url = ""

        try:
            # === 1. æ€»æ˜¯ä¿å­˜è¯Šæ–­æ•°æ®ï¼ˆdye vatï¼‰===
            dye_data = {
                "success": is_success,
                "partial_success": aggregation.get("__partial_success", False),
                "__valid_structure": aggregation.get("__valid_structure", False),
                "errors_summary": aggregation.get("__errors_summary", {}),
                "prompt_records": prompt_records,
                "raw_response_records": raw_response_records,
                "model": self.llm_model,
                "category": template_name,
                "user_input_preview": user_input[:200] if user_input else "",
                "timestamp": int(time.time()),
            }
            dye_file_path = self.DYE_VAT_DIR / filename
            if self.file_util.write_json(dye_data, dye_file_path):
                logger.info("ğŸ’‰ å·²ä¿å­˜éªŒè¯è¯Šæ–­ä¿¡æ¯", extra={"path": str(dye_file_path), "success": is_success})

            # === 2. ä»…æˆåŠŸæ—¶ä¿å­˜ raw + ç”ŸæˆæŠ¥å‘Š ===
            if is_success:
                raw_file_path = self.RAW_DATA_DIR / filename
                if self.file_util.write_json(result, raw_file_path):
                    logger.info("ğŸ’¾ å·²ä¿å­˜ç»“æ„åŒ–æ•°æ®", extra={"path": str(raw_file_path)})

                # æ³¨å…¥æ°´å°ç›¸å…³é…ç½®
                await self.result_assembler.inject_watermark_into_result(result)

                # é¢„å¤„ç†ç›¸å…³æ­¥éª¤çš„æ•°æ®
                await self.result_assembler.preprocess_for_html_rendering(result)

                outpath = self.report_generator.render_report_to_html(result)
                if outpath is None:
                    logger.error("âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ URL æ„é€ ")
                    report_url = ""
                else:
                    report_url = f"{self.REPORT_URL_PREFIX}{outpath.name}"
                    result["meta"]["report_url"] = report_url
                    logger.info("âœ… æ„é€ HTMLæŠ¥å‘ŠæˆåŠŸ", extra={"report_url": report_url})
        except Exception as e:
            logger.exception("æŒä¹…åŒ– extract ç»“æœå¤±è´¥", extra={
                "category": template_name,
                "is_success": is_success,
                "error": str(e)
            })

        return report_url

    # @staticmethod
    # def _open_report_in_browser(outpath: Path) -> None:
    #     try:
    #         import webbrowser
    #         webbrowser.open(f"file://{outpath}")
    #         logger.info("ğŸŒ å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š", extra={"outpath": str(outpath)})
    #     except Exception as e:
    #         logger.warning("âŒ æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨", extra={"error": str(e)})
