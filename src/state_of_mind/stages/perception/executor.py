import asyncio
from typing import Dict, Any, Set
from src.state_of_mind.cache.base import BaseCache
from src.state_of_mind.common.llm_response import LLMResponse
from src.state_of_mind.utils.registry import GlobalSingletonRegistry
from src.state_of_mind.utils.logger import LoggerManager as logger
from src.state_of_mind.stages.perception.prompt_builder import PromptBuilder


class StepExecutor:
    CHINESE_NAME = "å…¨æ¯æ„ŸçŸ¥åŸºåº•ï¼šé€šç”¨LLMæ‰§è¡Œå™¨"

    def __init__(
            self,
            backend_name: str,
            llm_model: str,
            recommended_params: Dict[str, Any],
            llm_cache: BaseCache,
            prompt_builder: PromptBuilder
    ):
        self.backend_name = backend_name
        self.llm_model = llm_model
        self.recommended_params = recommended_params
        self.llm_cache = llm_cache
        self.prompt_builder = prompt_builder
        self._backend = None
        self._init_lock = asyncio.Lock()

    async def get_backend(self):
        if self._backend is None:
            async with self._init_lock:
                if self._backend is None:
                    logger.info("ğŸ”„ é¦–æ¬¡è·å–LLMåç«¯ï¼Œæ­£åœ¨å¼‚æ­¥åˆå§‹åŒ–...")
                    self._backend = await GlobalSingletonRegistry.get_backend_async(self.backend_name)
                    if self._backend is None:
                        raise RuntimeError(f"æ— æ³•è·å– backend: {self.backend_name}")
        return self._backend

    """å¼‚æ­¥æ‰§è¡Œå•ä¸ª LLM è°ƒç”¨ï¼Œæ”¯æŒç¼“å­˜"""
    async def execute_step(
            self,
            prompt_template: str,
            template_name: str,
            step_name: str,
            cache_key: str,
            prompt_type: str
    ) -> Dict[str, Any]:
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={
                    "template": template_name,
                    "step": step_name,
                    "cache_key": cache_key
                })
                return cached_data

        try:
            backend = await self.get_backend()
            result = await backend.async_call(
                prompt=prompt_template,
                model=self.llm_model,
                params=self.recommended_params,
                template_name=template_name,
                step_name=step_name,
                prompt_type=prompt_type
            )
            return result
        except Exception as e:
            # ç³»ç»Ÿçº§å¼‚å¸¸ï¼šç½‘ç»œã€è¶…æ—¶ã€JSON è§£æå´©æºƒç­‰
            system_error = str(e)
            logger.error(f"[{step_name}] LLM è°ƒç”¨å¼‚å¸¸ - {str(e)}")
            return LLMResponse.from_system_error(
                system_error=system_error,
                model=self.llm_model,
                template_name=template_name,
                step_name=step_name,
                prompt_type=prompt_type,
                include_traceback=True
            ).to_dict()

    """å¼‚æ­¥æ‰§è¡Œç”ŸæˆåŸå§‹æ–‡æœ¬è§£è¯»"""
    async def execute_suggestion(self, prompt: str) -> str:
        logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆ LLM å»ºè®®å†…å®¹", module_name=self.CHINESE_NAME)
        try:
            backend = await self.get_backend()
            result = await backend.generate_text(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
            )
            if result.startswith("ç”Ÿæˆå¤±è´¥"):
                logger.warning("âš ï¸ LLM å»ºè®®ç”Ÿæˆå¤±è´¥", extra={"error": result})
            else:
                logger.info("âœ… LLM å»ºè®®ç”ŸæˆæˆåŠŸ", module_name=self.CHINESE_NAME)
            return result
        except Exception as e:
            error_msg = f"LLM å»ºè®®ç”Ÿæˆå¼‚å¸¸: {type(e).__name__}: {str(e)}"
            logger.exception(error_msg, module_name=self.CHINESE_NAME)
            return error_msg

    """
    å¼‚æ­¥æ‰§è¡Œæ‰¹é‡æŒ‡ä»£æ¶ˆè§£
    è¾“å…¥ï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ä»£è¯}
    è¾“å‡ºï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ç¡®å®šçš„åˆæ³•å‚ä¸è€…å}ï¼ˆä¸ç¡®å®šçš„ä¸è¿”å›ï¼‰
    """
    async def perform_coreference_resolution(
            self,
            user_input: str,
            index_to_pronoun: Dict[int, str],
            legitimate_participants: Set[str]
    ) -> Dict[int, str]:
        logger.info(f"â†’ å¯åŠ¨ LLM æŒ‡ä»£æ¶ˆè§£ï¼ˆå¾…è§£æä»£è¯: {list(index_to_pronoun.values())}ï¼‰",
                    extra={"module_name": self.CHINESE_NAME})

        if not index_to_pronoun or not legitimate_participants:
            return {}

        try:
            prompt = self.prompt_builder.build_coref_prompt(
                user_input=user_input,
                legitimate_participants=legitimate_participants,
                index_to_pronoun=index_to_pronoun
            )
        except Exception as e:
            logger.warning(
                "æ„å»ºæŒ‡ä»£æ¶ˆè§£ prompt å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        try:
            backend = await self.get_backend()
            resolved_from_llm: Dict[int, str] = await backend.bottom_dissolving_pronouns(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params
            )
        except Exception as e:
            logger.exception(
                "è°ƒç”¨ bottom_dissolving_pronouns å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        resolved_map: Dict[int, str] = {}
        for raw_idx, name in resolved_from_llm.items():
            try:
                idx = int(raw_idx)
            except (ValueError, TypeError):
                continue
            if idx in index_to_pronoun and name in legitimate_participants:
                resolved_map[idx] = name

        logger.info(f"â† LLM æ¶ˆè§£ç»“æœ: {resolved_map}", extra={"module_name": self.CHINESE_NAME})
        return resolved_map
