import asyncio
from typing import Dict, Any, Set, List
from src.state_of_mind.cache.base import BaseCache
from src.state_of_mind.common.llm_response import LLMResponse
from src.state_of_mind.stages.perception.constants import OTHER
from src.state_of_mind.utils.registry import GlobalSingletonRegistry
from src.state_of_mind.utils.logger import LoggerManager as logger
from src.state_of_mind.stages.perception.prompt_builder import PromptBuilder


class StepExecutor:
    CHINESE_NAME = "å…¨æ¯æ„ŸçŸ¥åŸºåº•ï¼šé€šç”¨LLMæ‰§è¡Œå™¨"

    def __init__(
            self,
            backend_name: str,
            llm_model: str,
            recommended_params: Dict[str, Any],
            llm_cache: BaseCache,
            prompt_builder: PromptBuilder
    ):
        self.backend_name = backend_name
        self.llm_model = llm_model
        self.recommended_params = recommended_params
        self.llm_cache = llm_cache
        self.prompt_builder = prompt_builder
        self._backend = None
        self._init_lock = asyncio.Lock()

    async def get_backend(self):
        if self._backend is None:
            async with self._init_lock:
                if self._backend is None:
                    logger.info("ğŸ”„ é¦–æ¬¡è·å–LLMåç«¯ï¼Œæ­£åœ¨å¼‚æ­¥åˆå§‹åŒ–...")
                    self._backend = await GlobalSingletonRegistry.get_backend_async(self.backend_name)
                    if self._backend is None:
                        raise RuntimeError(f"æ— æ³•è·å– backend: {self.backend_name}")
        return self._backend

    """å¼‚æ­¥æ‰§è¡Œå•ä¸ª LLM è°ƒç”¨ï¼Œæ”¯æŒç¼“å­˜"""
    async def execute_step(
            self,
            prompt_template: str,
            template_name: str,
            step_name: str,
            cache_key: str,
            prompt_type: str
    ) -> Dict[str, Any]:
        cache_response = await self.llm_cache.get(cache_key)
        if cache_response.get("success"):
            cached_data = cache_response.get("data")
            if cached_data is not None:
                logger.info("ğŸ” ä½¿ç”¨ç¼“å­˜ç»“æœ", extra={
                    "template": template_name,
                    "step": step_name,
                    "cache_key": cache_key
                })
                return cached_data

        try:
            backend = await self.get_backend()
            result = await backend.async_call(
                prompt=prompt_template,
                model=self.llm_model,
                params=self.recommended_params,
                template_name=template_name,
                step_name=step_name,
                prompt_type=prompt_type
            )
            return result
        except Exception as e:
            # ç³»ç»Ÿçº§å¼‚å¸¸ï¼šç½‘ç»œã€è¶…æ—¶ã€JSON è§£æå´©æºƒç­‰
            system_error = str(e)
            logger.error(f"[{step_name}] LLM è°ƒç”¨å¼‚å¸¸ - {str(e)}")
            return LLMResponse.from_system_error(
                system_error=system_error,
                model=self.llm_model,
                template_name=template_name,
                step_name=step_name,
                prompt_type=prompt_type,
                include_traceback=True
            ).to_dict()

    """å¼‚æ­¥æ‰§è¡Œç”ŸæˆåŸå§‹æ–‡æœ¬è§£è¯»"""
    async def execute_suggestion(self, prompt: str, step_name: str, prompt_type: str, all_step_results: List[Dict]) -> str:
        logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆ LLM å»ºè®®å†…å®¹", module_name=self.CHINESE_NAME)
        try:
            backend = await self.get_backend()
            result = await backend.generate_text(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
                step_name=step_name,
                prompt_type=prompt_type
            )
            all_step_results.append(result)
            if not result.get("__success", False):
                error_detail = result.get("__system_error") or result.get("__api_error") or "æœªçŸ¥é”™è¯¯"
                error_msg = f"ç”Ÿæˆå¤±è´¥: {error_detail}"
                logger.warning("âš ï¸ LLM å»ºè®®ç”Ÿæˆå¤±è´¥", extra={
                    "error": error_msg,
                    "module_name": self.CHINESE_NAME
                })
                return error_msg

            content = result.get("data", "").strip()
            if not content:
                error_msg = "ç”Ÿæˆå¤±è´¥: æ¨¡å‹è¿”å›ç©ºå†…å®¹"
                logger.warning("âš ï¸ LLM è¿”å›ç©ºå»ºè®®", extra={"module_name": self.CHINESE_NAME})
                return error_msg

            logger.info("âœ… LLM å»ºè®®ç”ŸæˆæˆåŠŸ", extra={"module_name": self.CHINESE_NAME})
            return content
        except Exception as e:
            error_msg = f"LLM å»ºè®®ç”Ÿæˆå¼‚å¸¸: {type(e).__name__}: {str(e)}"
            logger.exception(error_msg, module_name=self.CHINESE_NAME)
            return error_msg

    async def execute_global_signature(self, prompt: str, step_name: str, prompt_type: str, all_step_results: List[Dict]) -> str:
        logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆ LLM å…¨å±€è¯­ä¹‰æ ‡è¯†å†…å®¹", module_name=self.CHINESE_NAME)
        try:
            backend = await self.get_backend()
            result = await backend.guided_global_semantic_signature(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
                step_name=step_name,
                prompt_type=prompt_type
            )
            all_step_results.append(result)
            if not result.get("__success", False):
                error_detail = result.get("__system_error") or result.get("__api_error") or "æœªçŸ¥é”™è¯¯"
                error_msg = f"ç”Ÿæˆå¤±è´¥: {error_detail}"
                logger.warning("âš ï¸ LLM å…¨å±€è¯­ä¹‰æ ‡è¯†ç”Ÿæˆå¤±è´¥", extra={
                    "error": error_msg,
                    "module_name": self.CHINESE_NAME
                })
                return error_msg

            content = result.get("data", "").strip()
            if not content:
                error_msg = "ç”Ÿæˆå¤±è´¥: æ¨¡å‹è¿”å›ç©ºå†…å®¹"
                logger.warning("âš ï¸ LLM è¿”å›ç©ºç»“æœ", extra={"module_name": self.CHINESE_NAME})
                return error_msg

            logger.info("âœ… LLM å…¨å±€è¯­ä¹‰æ ‡è¯†ç”ŸæˆæˆåŠŸ", extra={"module_name": self.CHINESE_NAME})
            return content
        except Exception as e:
            error_msg = f"LLM å…¨å±€è¯­ä¹‰æ ‡è¯†ç”Ÿæˆå¼‚å¸¸: {type(e).__name__}: {str(e)}"
            logger.exception(error_msg, module_name=self.CHINESE_NAME)
            return error_msg

    """
    å¼‚æ­¥æ‰§è¡Œæ‰¹é‡æŒ‡ä»£æ¶ˆè§£
    è¾“å…¥ï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ä»£è¯}
    è¾“å‡ºï¼š{åŸå§‹äº‹ä»¶ç´¢å¼• -> ç¡®å®šçš„åˆæ³•å‚ä¸è€…å}ï¼ˆä¸ç¡®å®šçš„ä¸è¿”å›ï¼‰
    """
    async def perform_coreference_resolution(
        self,
        user_input: str,
        index_to_pronoun: Dict[int, str],
        legitimate_participants: Set[str],
        prompt_records: Dict,
        all_step_results: List[Dict],
    ) -> Dict[int, str]:
        logger.info(f"â†’ å¯åŠ¨ LLM æŒ‡ä»£æ¶ˆè§£ï¼ˆå¾…è§£æä»£è¯: {list(index_to_pronoun.values())}ï¼‰",
                    extra={"module_name": self.CHINESE_NAME})

        if not index_to_pronoun or not legitimate_participants:
            return {}

        try:
            prompt = self.prompt_builder.build_coref_prompt(
                user_input=user_input,
                legitimate_participants=legitimate_participants,
                index_to_pronoun=index_to_pronoun
            )
            prompt_records.setdefault(OTHER, []).append({
                "step_name": "coreference_resolution",
                "prompt": prompt
            })
        except Exception as e:
            logger.warning(
                "æ„å»ºæŒ‡ä»£æ¶ˆè§£ prompt å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        try:
            backend = await self.get_backend()
            result = await backend.bottom_dissolving_pronouns(
                prompt=prompt,
                model=self.llm_model,
                params=self.recommended_params,
                step_name="coreference_resolution",
                prompt_type="coreference_resolution"
            )
            all_step_results.append(result)
            if not result.get("__success", False):
                error_msg = result.get("__system_error") or result.get("__api_error") or "æœªçŸ¥é”™è¯¯"
                logger.warning(
                    "æŒ‡ä»£æ¶ˆè§£ LLM è°ƒç”¨å¤±è´¥",
                    extra={
                        "error": error_msg,
                        "module_name": self.CHINESE_NAME,
                        "raw_result": result
                    }
                )
                return {}

            raw_resolved = result.get("data", {})
            if not isinstance(raw_resolved, dict):
                logger.warning(
                    "æŒ‡ä»£æ¶ˆè§£è¿”å› data éå­—å…¸ç±»å‹",
                    extra={"type": type(raw_resolved), "module_name": self.CHINESE_NAME}
                )
                return {}
        except Exception as e:
            logger.exception(
                "è°ƒç”¨ bottom_dissolving_pronouns å¼‚å¸¸",
                extra={"error": str(e), "module_name": self.CHINESE_NAME}
            )
            return {}

        resolved_map: Dict[int, str] = {}
        for raw_idx, name in raw_resolved.items():
            try:
                idx = int(raw_idx)
            except (ValueError, TypeError):
                continue
            if idx in index_to_pronoun and name in legitimate_participants:
                resolved_map[idx] = name

        logger.info(f"â† LLM æ¶ˆè§£ç»“æœ: {resolved_map}", extra={"module_name": self.CHINESE_NAME})
        return resolved_map
