"""è´Ÿè´£åŠ¨æ€æ¸²æŸ“"""
import json
from typing import Any, Dict, List, Tuple, Optional, Set
from src.state_of_mind.prompt_templates.prompt_templates import LLM_PROMPTS_SCHEMA
from src.state_of_mind.stages.perception.constants import get_effective_policy, \
    render_iron_law_from_policy, COREFERENCE_RESOLUTION_BATCH, CATEGORY_SUGGESTION, \
    ALL_STEPS_FOR_FRONTEND, PERCEPTION_LAYERS, CATEGORY_RAW, \
    GLOBAL_SEMANTIC_SIGNATURE, PARALLEL_PREPROCESSING_STEPS, PARALLEL_PREPROCESSING, PARALLEL_PERCEPTION, \
    PARALLEL_PERCEPTION_STEPS, SERIAL_SUGGESTION_STEPS, PARALLEL_HIGH_ORDER_STEPS, SERIAL_SUGGESTION, \
    PARALLEL_HIGH_ORDER, PARALLEL_PERCEPTION_KEYS, PARALLEL_HIGH_ORDER_KEYS, SERIAL_SUGGESTION_KEYS, \
    PARALLEL_PREPROCESSING_KEYS
# from src.state_of_mind.utils.ip_timezone import IPBasedTimezoneResolver
from src.state_of_mind.utils.logger import LoggerManager as logger
# from src.state_of_mind.utils.network import get_public_ip


class PromptBuilder:
    """
    Prompt æ„é€ å™¨
    """
    CHINESE_NAME = "Promptæ„é€ å™¨"

    def build_raw(self) -> Dict[str, Any]:
        return {
            "preprocessing_prompts": self._build_step_prompts(
                list(PARALLEL_PREPROCESSING_STEPS.values()), PARALLEL_PREPROCESSING
            ),
            "perception_prompts": self._build_step_prompts(
                list(PARALLEL_PERCEPTION_STEPS.values()), PARALLEL_PERCEPTION
            ),
            "high_order_prompts": self._build_step_prompts(
                list(PARALLEL_HIGH_ORDER_STEPS.values()), PARALLEL_HIGH_ORDER
            ),
            "suggestion_prompts": self._build_step_prompts(
                list(SERIAL_SUGGESTION_STEPS.values()), SERIAL_SUGGESTION
            ),
        }

    def build_suggestion(self, template_name: str, user_input: str, suggestion_type: str) -> str:
        logger.info("ğŸ”„ å¼€å§‹æ„å»º build_suggestion Prompt", module_name=self.CHINESE_NAME)

        suggestion_schema = LLM_PROMPTS_SCHEMA.get(template_name)
        if not suggestion_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {template_name}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        valid_types = LLM_PROMPTS_SCHEMA[CATEGORY_SUGGESTION].keys()
        if suggestion_type not in valid_types:
            error_msg = f"ä¸æ”¯æŒçš„å»ºè®®ç±»å‹: '{suggestion_type}'ã€‚å¯ç”¨ç±»å‹: {sorted(valid_types)}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        prompt_template = suggestion_schema.get(suggestion_type)
        if not prompt_template:
            error_msg = f"æ¨¡æ¿ '{template_name}' ä¸­ç¼ºå°‘å»ºè®®ç±»å‹ '{suggestion_type}' çš„å®šä¹‰"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        try:
            final_prompt = prompt_template.format(user_input=user_input)
        except KeyError as e:
            error_msg = f"æ¨¡æ¿ä¸­åŒ…å«æœªæä¾›çš„å­—æ®µ: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        logger.info("âœ… build_suggestion Prompt æ„å»ºæˆåŠŸ", module_name=self.CHINESE_NAME)
        return final_prompt

    def build_global_signature_prompt(self, user_input):
        prompt_template = LLM_PROMPTS_SCHEMA.get(GLOBAL_SEMANTIC_SIGNATURE)
        if not prompt_template:
            error_msg = f"æ¨¡æ¿ä¸­ç¼ºå°‘å…¨å±€è¯­ä¹‰æ ‡è¯†çš„ prompt å®šä¹‰"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        try:
            final_prompt = prompt_template.format(user_input=user_input)
        except Exception as e:
            error_msg = f"æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        logger.info("âœ… build_global_signature_prompt Prompt æ„å»ºæˆåŠŸ", module_name=self.CHINESE_NAME)
        return final_prompt

    @staticmethod
    def build_coref_prompt(
            user_input: str,
            legitimate_participants: Set[str],
            index_to_pronoun: Dict[int, str]
    ) -> str:
        """
        æ„é€ æŒ‡ä»£æ¶ˆè§£ promptã€‚
        :param user_input:
        :param legitimate_participants:
        :param index_to_pronoun: {0: "ä»–", 2: "å¥¹", ...} â€”â€” åŸå§‹äº‹ä»¶ä¸­çš„ç´¢å¼•åˆ°ä»£è¯æ˜ å°„
        """
        participant_list_str = "\n".join(f"- {p}" for p in sorted(legitimate_participants))

        pronoun_lines = []
        for idx in sorted(index_to_pronoun.keys()):  # æŒ‰ç´¢å¼•æ’åºï¼Œä¾¿äºé˜…è¯»
            pronoun_lines.append(f"{idx} -> â€œ{index_to_pronoun[idx]}â€")
        pronoun_mapping_str = "\n".join(pronoun_lines)

        template = LLM_PROMPTS_SCHEMA[COREFERENCE_RESOLUTION_BATCH]
        return template.format(
            user_input=user_input,
            participant_list_str=participant_list_str,
            pronoun_mapping_str=pronoun_mapping_str
        )

    def pre_basic_data(self):
        raw_schema = LLM_PROMPTS_SCHEMA.get(CATEGORY_RAW)
        if not raw_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {CATEGORY_RAW}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        pipeline = raw_schema.get("pipeline")
        if not isinstance(pipeline, list):
            error_msg = f"é…ç½®é”™è¯¯: {CATEGORY_RAW}.pipeline å¿…é¡»æ˜¯åˆ—è¡¨ï¼Œå½“å‰å€¼: {repr(pipeline)}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        if len(pipeline) == 0:
            logger.warning(f"âš ï¸ è­¦å‘Š: {CATEGORY_RAW}.pipeline ä¸ºç©ºåˆ—è¡¨ï¼å°†å¯¼è‡´å‰ç«¯æ­¥éª¤ä¸ºç©ºï¼", module_name=self.CHINESE_NAME)
            raise ValueError("pipeline ä¸èƒ½ä¸ºç©º")

        pipeline = raw_schema.get("pipeline")
        self._split_pipeline(pipeline)

    @staticmethod
    def _split_pipeline(pipeline: List[Dict]) -> None:
        """
        åˆ†ç¦» pipeline ä¸­çš„é¢„å¤„ç†ã€å¹¶è¡Œã€ä¸²è¡Œä»»åŠ¡
        """
        # å„ç±»å‹æ­¥éª¤æ•°æ®
        PARALLEL_PREPROCESSING_STEPS.clear()
        PARALLEL_PERCEPTION_STEPS.clear()
        PARALLEL_HIGH_ORDER_STEPS.clear()
        SERIAL_SUGGESTION_STEPS.clear()
        # æ„ŸçŸ¥ç±»å‹æ­¥éª¤å
        PERCEPTION_LAYERS.clear()
        # å„ç±»å‹é¡¶çº§é”®
        PARALLEL_PREPROCESSING_KEYS.clear()
        PARALLEL_PERCEPTION_KEYS.clear()
        PARALLEL_HIGH_ORDER_KEYS.clear()
        SERIAL_SUGGESTION_KEYS.clear()
        # å…¨éƒ¨æ­¥éª¤ç›¸å…³æ•°æ®
        ALL_STEPS_FOR_FRONTEND.clear()

        valid_types = {
            PARALLEL_PREPROCESSING,
            PARALLEL_PERCEPTION,
            PARALLEL_HIGH_ORDER,
            SERIAL_SUGGESTION
        }

        for idx, step in enumerate(pipeline):
            if not isinstance(step, dict) or "step_name" not in step:
                continue

            step_id = step["step_name"]
            step_type = step.get("type")
            label = step.get("label", step_id)
            driven_by = step.get("driven_by")

            if step_type not in valid_types:
                raise ValueError(
                    f"æ­¥éª¤ '{step_id}' ä½¿ç”¨äº†éæ³•ç±»å‹: '{step_type}'ã€‚"
                    f"ä»…å…è®¸: {sorted(valid_types)}"
                )

            # åˆ†ç»„å­˜å‚¨
            if step_type == PARALLEL_PREPROCESSING:
                PARALLEL_PREPROCESSING_STEPS[step_id] = step
                PARALLEL_PREPROCESSING_KEYS.add(driven_by)
            elif step_type == PARALLEL_PERCEPTION:
                PARALLEL_PERCEPTION_STEPS[step_id] = step
                PERCEPTION_LAYERS.add(step_id)
                PARALLEL_PERCEPTION_KEYS.add(driven_by)
            elif step_type == PARALLEL_HIGH_ORDER:
                PARALLEL_HIGH_ORDER_STEPS[step_id] = step
                PARALLEL_HIGH_ORDER_KEYS.add(driven_by)
            elif step_type == SERIAL_SUGGESTION:
                SERIAL_SUGGESTION_STEPS[step_id] = step
                SERIAL_SUGGESTION_KEYS.add(driven_by)

            # ã€å…³é”®ã€‘æ³¨å…¥å…¨é‡å‰ç«¯é…ç½®
            ALL_STEPS_FOR_FRONTEND.append({
                "id": step_id,
                "label": label,
                "type": step_type,
                "driven_by": driven_by
            })
        logger.info(
            f"âœ… æ­¥éª¤åˆ†ç¦»å®Œæˆ | "
            f"pre={len(PARALLEL_PREPROCESSING_STEPS)} | "
            f"percep={len(PARALLEL_PERCEPTION_STEPS)} | "
            f"high={len(PARALLEL_HIGH_ORDER_STEPS)} | "
            f"sugg={len(SERIAL_SUGGESTION_STEPS)}"
        )

    @staticmethod
    def _build_step_prompts(
            steps: List[Dict],
            step_type: str
    ) -> List[Tuple[str, str, str]]:
        """
        æ„å»ºæŒ‡å®šç±»å‹ï¼ˆå¹¶è¡Œ/ä¸²è¡Œï¼‰çš„ prompt åˆ—è¡¨ï¼Œè¿”å› (step_name, driven_by, full_prompt) å…ƒç»„åˆ—è¡¨ã€‚
        æ¯ä¸ª prompt ä¸¥æ ¼æŒ‰ä»¥ä¸‹é¡ºåºç»„ç»‡ï¼š
          1. roleï¼ˆè§’è‰²ï¼‰
          2. ### æ ¸å¿ƒåŸåˆ™ï¼ˆinformation_source + é€šç”¨ç­–ç•¥ï¼‰
          3. ### æ­¥éª¤ä¸“å±è§„åˆ™ï¼ˆæ¥è‡ª step_rulesï¼‰
          4. è¾“å‡ºå‰ç¼€ï¼ˆå¯é€‰ æ¥è‡ªoutput_prefixï¼‰
          5. å­—æ®µç»“æ„ï¼ˆæ¥è‡ªfields JSON schemaï¼‰
          6. ç©ºç»“æœå…œåº•ï¼ˆæ¥è‡ªempty_result_fallbackï¼‰
          7. è¾“å‡ºåç¼€ï¼ˆå¯é€‰ æ¥è‡ªoutput_suffixï¼‰
        """
        prompts_with_fields = []
        missing_fields = []

        for idx, step in enumerate(steps):
            try:
                step_name = step["step_name"]
                role = step["role"]
                information_source = step["information_source"]
                fields = step["fields"]
                driven_by = step.get("driven_by")
                constraint_profile = step.get("constraint_profile")
                empty_fallback = step.get("empty_result_fallback", "")
                # æ–°ï¼šä½¿ç”¨æ‰å¹³åŒ–çš„ step_rules
                step_rules = step.get("step_rules", [])
                output_prefix = step.get("output_prefix", [])
                output_suffix = step.get("output_suffix", [])
            except KeyError as e:
                field = e.args[0]
                missing_fields.append(f"æ­¥éª¤{idx}.{field}")
                continue

            # === æ¸²æŸ“é€šç”¨ç­–ç•¥é“å¾‹ï¼ˆæ ¸å¿ƒåŸåˆ™ï¼‰===
            effective_policy = get_effective_policy(step_name)
            dynamic_iron_law = render_iron_law_from_policy(effective_policy).strip()

            # === æ„å»º prompt å„éƒ¨åˆ† ===
            parts = [role.strip()]

            # æ ¸å¿ƒåŸåˆ™
            core_principle_text = "### æ ¸å¿ƒåŸåˆ™\n" + information_source.strip() + dynamic_iron_law
            parts.append(core_principle_text)

            # æ­¥éª¤ä¸“å±è§„åˆ™
            if step_rules:
                rules_text = "\n".join(step_rules)
                parts.append(rules_text)

            # è¾“å‡ºå‰ç¼€
            if output_prefix:
                parts.append("\n".join(output_prefix))

            # å­—æ®µç»“æ„ï¼ˆschemaï¼‰
            fields_json_str = json.dumps(fields, ensure_ascii=False, indent=2)
            parts.append(fields_json_str)

            # ç©ºç»“æœå…œåº•
            if empty_fallback.strip():
                parts.append(empty_fallback.strip())

            # è¾“å‡ºåç¼€
            if output_suffix:
                parts.append("\n".join(output_suffix))

            # æ‹¼æ¥å®Œæ•´ prompt
            full_prompt = "\n\n".join(parts).strip()
            prompts_with_fields.append((step_name, driven_by, full_prompt))

            # logger.info(
            #     f"ğŸ“Œ æ­¥éª¤ {step_name} ä½¿ç”¨çº¦æŸé…ç½®: {constraint_profile}",
            #     module_name=PromptBuilder.CHINESE_NAME
            # )

        # âŒ å­—æ®µç¼ºå¤±æ ¡éªŒ
        if missing_fields:
            error_msg = f"{step_type} æ­¥éª¤ä¸­ç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}"
            logger.error(error_msg, module_name=PromptBuilder.CHINESE_NAME)
            raise ValueError(error_msg)

        # âœ… æˆåŠŸæ—¥å¿—
        logger.info(
            f"ğŸ”§ å·²ç”Ÿæˆ {step_type} prompts æ•°é‡: {len(prompts_with_fields)}",
            module_name=PromptBuilder.CHINESE_NAME
        )
        return prompts_with_fields

    @staticmethod
    def generate_description(context: dict, field_config: List[Tuple[str, bool, Any, str]], prefix="") -> str:
        def _is_effectively_empty(value) -> bool:
            if value is None:
                return True
            if isinstance(value, str) and not value.strip():
                return True
            if isinstance(value, (list, dict)) and len(value) == 0:
                return True
            return False

        def _format_simple_value(value):
            if isinstance(value, list):
                non_empty = [str(v) for v in value if not _is_effectively_empty(v)]
                return ", ".join(non_empty)
            return str(value)

        # é¢„å¤„ç†é€šé…è§„åˆ™ï¼šæå–æ‰€æœ‰ *. è·¯å¾„
        wildcard_rules = {}
        normal_rules = {}
        top_fields = []

        for path, required, typ, desc in field_config:
            if ".*." in path:
                prefix_path = path.split(".*.", 1)[0]  # å¦‚ "inference.events"
                field_name = path.split(".*.", 1)[1]  # å¦‚ "inference_type"
                if prefix_path not in wildcard_rules:
                    wildcard_rules[prefix_path] = []
                wildcard_rules[prefix_path].append((field_name, desc))
            elif "." not in path:
                top_fields.append((path, desc))
            else:
                normal_rules[path] = desc

        output_lines = []

        # å¦‚æœæ²¡æœ‰é¡¶å±‚å­—æ®µï¼Œfallback åˆ°å¹³é“ºæ¸²æŸ“
        if not top_fields:
            for path, desc in normal_rules.items():
                val = context.get(path)
                if not _is_effectively_empty(val):
                    output_lines.append(f"## {desc.rstrip('ï¼š:').strip()}")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")
            result = "\n".join(output_lines).strip()
            # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ï¼ˆæ— é¡¶å±‚ï¼‰:{result}", module_name=Prompter.CHINESE_NAME)
            return result

        # å¤„ç†æ¯ä¸ªé¡¶å±‚å­—æ®µï¼ˆæ”¯æŒå¤šä¸ªï¼‰
        for top_path, top_desc in top_fields:
            top_value = context.get(top_path)
            if _is_effectively_empty(top_value):
                continue

            clean_top_desc = top_desc.rstrip("ï¼š:").strip()
            output_lines.append(f"## {clean_top_desc}")

            if isinstance(top_value, dict):
                # æ¸²æŸ“å­—å…¸çš„æ¯ä¸ªå­å­—æ®µ
                for key, val in top_value.items():
                    if _is_effectively_empty(val):
                        continue
                    full_sub_path = f"{top_path}.{key}"
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ list[dict] ä¸”æœ‰é€šé…è§„åˆ™
                    if isinstance(val, list) and val and isinstance(val[0], dict):
                        if full_sub_path in wildcard_rules:
                            # è·å–è¯¥åˆ—è¡¨å­—æ®µçš„å®Œæ•´æè¿°ï¼ˆå¦‚ "eventsï¼ˆæ¨ç†äº‹ä»¶åˆ—è¡¨ï¼‰ï¼š"ï¼‰
                            list_desc = normal_rules.get(full_sub_path, f"{key}ï¼ˆåˆ—è¡¨ï¼‰ï¼š")
                            for item in val:
                                item_lines = []
                                for field_name, field_desc in wildcard_rules[full_sub_path]:
                                    item_val = item.get(field_name)
                                    if not _is_effectively_empty(item_val):
                                        item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                                if item_lines:
                                    output_lines.append(f"  - {list_desc}")
                                    output_lines.extend(item_lines)
                            continue  # å·²å¤„ç†ï¼Œè·³è¿‡é»˜è®¤é€»è¾‘

                    # é»˜è®¤ï¼šç®€å•æ ¼å¼åŒ–
                    desc = normal_rules.get(full_sub_path, f"{key}: ")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")

            elif isinstance(top_value, list):
                # é¡¶å±‚æ˜¯åˆ—è¡¨ï¼ˆå¦‚ participantsï¼‰
                if top_path in wildcard_rules:
                    for item in top_value:
                        if not isinstance(item, dict):
                            continue
                        item_lines = []
                        for field_name, field_desc in wildcard_rules[top_path]:
                            item_val = item.get(field_name)
                            if not _is_effectively_empty(item_val):
                                item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                        if item_lines:
                            output_lines.append("  - åˆ—è¡¨é¡¹ï¼š")
                            output_lines.extend(item_lines)
                else:
                    output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")
            else:
                output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")

        # æ¸…ç†ç©ºè¡Œ
        while output_lines and output_lines[-1] == "":
            output_lines.pop()

        result = "\n".join(output_lines).strip()
        # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡:{result}", module_name=Prompter.CHINESE_NAME)
        return result

    @staticmethod
    def extract_top_level_description(fields_spec: List[Tuple[str, bool, Any, str]]) -> Optional[str]:
        """
        ä»å­—æ®µè§„èŒƒåˆ—è¡¨ä¸­æå–é¡¶å±‚å­—æ®µï¼ˆè·¯å¾„ä¸­ä¸å« '.' çš„å­—æ®µï¼‰çš„æè¿°ã€‚
        è‹¥å­˜åœ¨å¤šä¸ªé¡¶å±‚å­—æ®µï¼ˆå¦‚ inference + context_clueï¼‰ï¼Œä¼˜å…ˆå–ç¬¬ä¸€ä¸ªéé€šé…ã€éåˆ—è¡¨é¡¹çš„ã€‚
        """
        for field_path, _, _, description in fields_spec:
            # è·³è¿‡å¸¦é€šé…ç¬¦çš„è·¯å¾„ï¼ˆå¦‚ participants.*.roleï¼‰
            if ".*." in field_path or field_path.startswith("*."):
                continue
            parts = field_path.split(".")
            if len(parts) == 1:
                # è¿™æ˜¯ä¸€ä¸ªé¡¶å±‚å­—æ®µï¼Œå¦‚ "participants", "inference", "context_clue"
                return description
        return None
