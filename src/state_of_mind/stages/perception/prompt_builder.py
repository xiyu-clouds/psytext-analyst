"""è´Ÿè´£åŠ¨æ€æ¸²æŸ“"""
import json
from typing import Any, Dict, List, Tuple, Optional, Set
from src.state_of_mind.prompt_templates.prompt_templates import LLM_PROMPTS_SCHEMA
from .constants import PARALLEL, SERIAL, PREPROCESSING, get_effective_policy, \
    render_iron_law_from_policy, COREFERENCE_RESOLUTION_BATCH, CATEGORY_SUGGESTION
# from src.state_of_mind.utils.ip_timezone import IPBasedTimezoneResolver
from src.state_of_mind.utils.logger import LoggerManager as logger
# from src.state_of_mind.utils.network import get_public_ip


class PromptBuilder:
    """
    Prompt æ„é€ å™¨
    """
    CHINESE_NAME = "Promptæ„é€ å™¨"

    def build_raw(self, template_name: str, **template_vars: Any) -> Dict[str, Any]:
        """
        æ„å»º Prompt å¹¶è¿”å›å®Œæ•´ä¸Šä¸‹æ–‡æ•°æ®
        """
        logger.info("ğŸ”„ å¼€å§‹æ„å»º build_raw Prompt", module_name=self.CHINESE_NAME)
        if "user_input" not in template_vars:
            error_msg = "ç¼ºå¤±å¿…éœ€å­—æ®µ: user_input"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        raw_schema = LLM_PROMPTS_SCHEMA.get(template_name)
        if not raw_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {template_name}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        schema_version = raw_schema.get("version")
        pipeline = raw_schema.get("pipeline")
        preprocessing_steps, parallel_steps, serial_steps = PromptBuilder._split_pipeline(pipeline)

        preprocessing_prompts = PromptBuilder._build_step_prompts(
            steps=preprocessing_steps,
            step_type=PREPROCESSING
        )

        parallel_prompts = PromptBuilder._build_step_prompts(
            steps=parallel_steps,
            step_type=PARALLEL
        )

        serial_prompts = PromptBuilder._build_step_prompts(
            steps=serial_steps,
            step_type=SERIAL
        )

        logger.info(
            f"âœ… Prompt æ„å»ºå®Œæˆ, preprocessing_count = {len(preprocessing_prompts)} | "
            f"parallel_count = {len(parallel_prompts)} | serial_count = {len(serial_prompts)} | ",
            module_name=PromptBuilder.CHINESE_NAME
        )
        return {
            "template_name": template_name,
            "preprocessing_prompts": preprocessing_prompts,
            "parallel_prompts": parallel_prompts,
            "serial_prompts": serial_prompts
        }

    def build_suggestion(self, template_name: str, user_input: str, suggestion_type: str) -> str:
        logger.info("ğŸ”„ å¼€å§‹æ„å»º build_suggestion Prompt", module_name=self.CHINESE_NAME)

        suggestion_schema = LLM_PROMPTS_SCHEMA.get(template_name)
        if not suggestion_schema:
            error_msg = f"æ¨¡æ¿æœªå®šä¹‰: {template_name}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        valid_types = LLM_PROMPTS_SCHEMA[CATEGORY_SUGGESTION].keys()
        if suggestion_type not in valid_types:
            error_msg = f"ä¸æ”¯æŒçš„å»ºè®®ç±»å‹: '{suggestion_type}'ã€‚å¯ç”¨ç±»å‹: {sorted(valid_types)}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        prompt_template = suggestion_schema.get(suggestion_type)
        if not prompt_template:
            error_msg = f"æ¨¡æ¿ '{template_name}' ä¸­ç¼ºå°‘å»ºè®®ç±»å‹ '{suggestion_type}' çš„å®šä¹‰"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        try:
            final_prompt = prompt_template.format(user_input=user_input)
        except KeyError as e:
            error_msg = f"æ¨¡æ¿ä¸­åŒ…å«æœªæä¾›çš„å­—æ®µ: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)
        except Exception as e:
            error_msg = f"æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {e}"
            logger.error(error_msg, module_name=self.CHINESE_NAME)
            raise ValueError(error_msg)

        logger.info("âœ… build_suggestion Prompt æ„å»ºæˆåŠŸ", module_name=self.CHINESE_NAME)
        return final_prompt

    @staticmethod
    def build_coref_prompt(
            user_input: str,
            legitimate_participants: Set[str],
            index_to_pronoun: Dict[int, str]
    ) -> str:
        """
        æ„é€ æŒ‡ä»£æ¶ˆè§£ promptã€‚
        :param user_input:
        :param legitimate_participants:
        :param index_to_pronoun: {0: "ä»–", 2: "å¥¹", ...} â€”â€” åŸå§‹äº‹ä»¶ä¸­çš„ç´¢å¼•åˆ°ä»£è¯æ˜ å°„
        """
        participant_list_str = "\n".join(f"- {p}" for p in sorted(legitimate_participants))

        pronoun_lines = []
        for idx in sorted(index_to_pronoun.keys()):  # æŒ‰ç´¢å¼•æ’åºï¼Œä¾¿äºé˜…è¯»
            pronoun_lines.append(f"{idx} -> â€œ{index_to_pronoun[idx]}â€")
        pronoun_mapping_str = "\n".join(pronoun_lines)

        template = LLM_PROMPTS_SCHEMA[COREFERENCE_RESOLUTION_BATCH]
        return template.format(
            user_input=user_input,
            participant_list_str=participant_list_str,
            pronoun_mapping_str=pronoun_mapping_str
        )

    @staticmethod
    def _split_pipeline(pipeline: List[Dict]) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        åˆ†ç¦» pipeline ä¸­çš„é¢„å¤„ç†ã€å¹¶è¡Œã€ä¸²è¡Œä»»åŠ¡
        è¿”å›: (preprocessing_steps, parallel_steps, serial_steps)
        """
        if not isinstance(pipeline, list):
            error_msg = "pipeline å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹"
            logger.error(error_msg)
            raise ValueError(error_msg)

        preprocessing = []
        parallel = []
        serial = []

        for idx, step in enumerate(pipeline):
            if not isinstance(step, dict):
                logger.warning(f"è·³è¿‡éæ³• pipeline æ­¥éª¤ï¼ˆéå­—å…¸ï¼‰: ç´¢å¼•={idx}")
                continue

            step_type = step.get("type", SERIAL)  # é»˜è®¤ä¸ºä¸²è¡Œ

            if step_type == PREPROCESSING:
                preprocessing.append(step)
            elif step_type == PARALLEL:
                parallel.append(step)
            else:
                serial.append(step)  # åŒ…æ‹¬ SERIAL å’Œ æœªçŸ¥ type éƒ½å½’ä¸ºä¸²è¡Œï¼ˆå®‰å…¨å…œåº•ï¼‰

        logger.info(
            f"ğŸ“Š pipeline ä¸‰è·¯åˆ†ç¦»å®Œæˆ, preprocessing_count = {len(preprocessing)} | parallel_count = {len(parallel)} | "
            f"serial_count = {len(serial)} | total_steps = {len(pipeline)}", module_name=PromptBuilder.CHINESE_NAME
        )
        return preprocessing, parallel, serial

    @staticmethod
    def _build_step_prompts(
            steps: List[Dict],
            step_type: str
    ) -> List[Tuple[str, str, str]]:
        """
        æ„å»ºæŒ‡å®šç±»å‹ï¼ˆå¹¶è¡Œ/ä¸²è¡Œï¼‰çš„ prompt åˆ—è¡¨ï¼Œè¿”å› (step_name, driven_by, full_prompt) å…ƒç»„åˆ—è¡¨ã€‚
        æ¯ä¸ª prompt ä¸¥æ ¼æŒ‰ä»¥ä¸‹é¡ºåºç»„ç»‡ï¼š
          1. roleï¼ˆç”±è°ƒç”¨æ–¹å®šä¹‰ã€å”¯ä¸€ä¿¡æºã€‘ä¸ã€å½“å‰æŒ‡ä»¤ã€‘è¾¹ç•Œï¼‰
          2. sole_missionï¼ˆå¼ºåŒ–é”šå®šè¦æ±‚ï¼‰
          3. ### ã€ç»å¯¹é“å¾‹ã€‘ï¼ˆä»…å«æŠ½è±¡ç­–ç•¥ï¼Œä¸æ¶‰åŠå…·ä½“è¾“å…¥å—æ ‡è¯†ï¼‰
          4. ### å½“å‰ä»»åŠ¡çš„æ ¸å¿ƒé“å¾‹ï¼ˆå­—æ®µè¾¹ç•Œã€å®ä½“è§„åˆ™ç­‰ï¼‰
          5. ### è¾“å‡ºæ ¼å¼ä¸ç»“æ„å¼ºåˆ¶è¦æ±‚
          6. fields schemaï¼ˆJSON è½¬ä¹‰åï¼‰
        """
        prompts_with_fields = []
        missing_fields = []

        for idx, step in enumerate(steps):
            try:
                step_name = step["step"]
                role = step["role"]
                sole_mission = step["sole_mission"]
                fields = step["fields"]
                driven_by = step.get("driven_by")
                constraint_profile = step.get("constraint_profile", "unknown")
                input_requirements = step.get("input_requirements", {})
            except KeyError as e:
                field = e.args[0]
                missing_fields.append(f"æ­¥éª¤{idx}.{field}")
                continue

            # æ¸²æŸ“é€šç”¨ç­–ç•¥é“å¾‹
            effective_policy = get_effective_policy(step_name)
            dynamic_iron_law = render_iron_law_from_policy(effective_policy)

            fields_json = json.dumps(fields, ensure_ascii=False, indent=2)
            fields_escaped = fields_json.replace('{', '{{').replace('}', '}}')

            # === æ„å»ºä¸‰å±‚é“å¾‹ï¼ˆæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰===
            iron_law_sections = []

            # 1ï¸âƒ£ é€šç”¨ç­–ç•¥é“å¾‹
            if dynamic_iron_law.strip():
                iron_law_sections.append(dynamic_iron_law.strip())

            # 2ï¸âƒ£ ä»»åŠ¡ä¸“å±æ•°æ®ä¸é”šå®šçº¦æŸ
            data_constraints = input_requirements.get("data_and_anchor_constraints")
            if data_constraints:
                iron_law_sections.append(
                    "### å½“å‰ä»»åŠ¡çš„æ ¸å¿ƒé“å¾‹ï¼ˆå¿…é¡»ç»å¯¹éµå®ˆï¼‰###\n" +
                    "\n".join(data_constraints)
                )

            # 3ï¸âƒ£ è¾“å‡ºç»“æ„ä¸æ ¼å¼å¼ºåˆ¶è¦æ±‚
            output_constraints = input_requirements.get("output_structure_constraints")
            if output_constraints:
                iron_law_sections.append(
                    "### è¾“å‡ºæ ¼å¼ä¸ç»“æ„å¼ºåˆ¶è¦æ±‚ ###\n" +
                    "\n".join(output_constraints)
                )

            combined_iron_law = "\n\n".join(iron_law_sections).strip()

            # === æ‹¼æ¥å®Œæ•´ prompt ===
            full_prompt_parts = [
                "### SYSTEM_INSTRUCTIONS BEGIN ###\n",
                role.strip(),
                sole_mission.strip()
            ]
            if combined_iron_law:
                full_prompt_parts.append(combined_iron_law)

            full_prompt_parts.append(fields_escaped.strip())
            full_prompt_parts.append("### SYSTEM_INSTRUCTIONS END ###")
            full_prompt = "\n".join(full_prompt_parts)
            prompts_with_fields.append((step_name, driven_by, full_prompt))
            logger.info(
                f"ğŸ“Œ æ­¥éª¤ {step_name} ä½¿ç”¨çº¦æŸé…ç½®: {constraint_profile}",
                module_name=PromptBuilder.CHINESE_NAME
            )

        # âŒ å­—æ®µç¼ºå¤±æ ¡éªŒ
        if missing_fields:
            error_msg = f"{step_type} æ­¥éª¤ä¸­ç¼ºå¤±å­—æ®µ: {', '.join(missing_fields)}"
            logger.error(error_msg, module_name=PromptBuilder.CHINESE_NAME)
            raise ValueError(error_msg)

        # âœ… æˆåŠŸæ—¥å¿—
        logger.info(
            f"ğŸ”§ å·²ç”Ÿæˆ {step_type} prompts æ•°é‡: {len(prompts_with_fields)}",
            module_name=PromptBuilder.CHINESE_NAME
        )
        return prompts_with_fields

    @staticmethod
    def generate_description(context: dict, field_config: List[Tuple[str, bool, Any, str]], prefix="") -> str:
        def _is_effectively_empty(value) -> bool:
            if value is None:
                return True
            if isinstance(value, str) and not value.strip():
                return True
            if isinstance(value, (list, dict)) and len(value) == 0:
                return True
            return False

        def _format_simple_value(value):
            if isinstance(value, list):
                non_empty = [str(v) for v in value if not _is_effectively_empty(v)]
                return ", ".join(non_empty)
            return str(value)

        # é¢„å¤„ç†é€šé…è§„åˆ™ï¼šæå–æ‰€æœ‰ *. è·¯å¾„
        wildcard_rules = {}
        normal_rules = {}
        top_fields = []

        for path, required, typ, desc in field_config:
            if ".*." in path:
                prefix_path = path.split(".*.", 1)[0]  # å¦‚ "inference.events"
                field_name = path.split(".*.", 1)[1]  # å¦‚ "inference_type"
                if prefix_path not in wildcard_rules:
                    wildcard_rules[prefix_path] = []
                wildcard_rules[prefix_path].append((field_name, desc))
            elif "." not in path:
                top_fields.append((path, desc))
            else:
                normal_rules[path] = desc

        output_lines = []

        # å¦‚æœæ²¡æœ‰é¡¶å±‚å­—æ®µï¼Œfallback åˆ°å¹³é“ºæ¸²æŸ“
        if not top_fields:
            for path, desc in normal_rules.items():
                val = context.get(path)
                if not _is_effectively_empty(val):
                    output_lines.append(f"## {desc.rstrip('ï¼š:').strip()}")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")
            result = "\n".join(output_lines).strip()
            # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡ï¼ˆæ— é¡¶å±‚ï¼‰:{result}", module_name=Prompter.CHINESE_NAME)
            return result

        # å¤„ç†æ¯ä¸ªé¡¶å±‚å­—æ®µï¼ˆæ”¯æŒå¤šä¸ªï¼‰
        for top_path, top_desc in top_fields:
            top_value = context.get(top_path)
            if _is_effectively_empty(top_value):
                continue

            clean_top_desc = top_desc.rstrip("ï¼š:").strip()
            output_lines.append(f"## {clean_top_desc}")

            if isinstance(top_value, dict):
                # æ¸²æŸ“å­—å…¸çš„æ¯ä¸ªå­å­—æ®µ
                for key, val in top_value.items():
                    if _is_effectively_empty(val):
                        continue
                    full_sub_path = f"{top_path}.{key}"
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ list[dict] ä¸”æœ‰é€šé…è§„åˆ™
                    if isinstance(val, list) and val and isinstance(val[0], dict):
                        if full_sub_path in wildcard_rules:
                            # è·å–è¯¥åˆ—è¡¨å­—æ®µçš„å®Œæ•´æè¿°ï¼ˆå¦‚ "eventsï¼ˆæ¨ç†äº‹ä»¶åˆ—è¡¨ï¼‰ï¼š"ï¼‰
                            list_desc = normal_rules.get(full_sub_path, f"{key}ï¼ˆåˆ—è¡¨ï¼‰ï¼š")
                            for item in val:
                                item_lines = []
                                for field_name, field_desc in wildcard_rules[full_sub_path]:
                                    item_val = item.get(field_name)
                                    if not _is_effectively_empty(item_val):
                                        item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                                if item_lines:
                                    output_lines.append(f"  - {list_desc}")
                                    output_lines.extend(item_lines)
                            continue  # å·²å¤„ç†ï¼Œè·³è¿‡é»˜è®¤é€»è¾‘

                    # é»˜è®¤ï¼šç®€å•æ ¼å¼åŒ–
                    desc = normal_rules.get(full_sub_path, f"{key}: ")
                    output_lines.append(f"  - {desc}{_format_simple_value(val)}")

            elif isinstance(top_value, list):
                # é¡¶å±‚æ˜¯åˆ—è¡¨ï¼ˆå¦‚ participantsï¼‰
                if top_path in wildcard_rules:
                    for item in top_value:
                        if not isinstance(item, dict):
                            continue
                        item_lines = []
                        for field_name, field_desc in wildcard_rules[top_path]:
                            item_val = item.get(field_name)
                            if not _is_effectively_empty(item_val):
                                item_lines.append(f"    - {field_desc}{_format_simple_value(item_val)}")
                        if item_lines:
                            output_lines.append("  - åˆ—è¡¨é¡¹ï¼š")
                            output_lines.extend(item_lines)
                else:
                    output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")
            else:
                output_lines.append(f"  - {top_desc}{_format_simple_value(top_value)}")

        # æ¸…ç†ç©ºè¡Œ
        while output_lines and output_lines[-1] == "":
            output_lines.pop()

        result = "\n".join(output_lines).strip()
        # logger.info(f"åŠ¨æ€ç”Ÿæˆä¸Šä¸‹æ–‡:{result}", module_name=Prompter.CHINESE_NAME)
        return result

    @staticmethod
    def extract_top_level_description(fields_spec: List[Tuple[str, bool, Any, str]]) -> Optional[str]:
        """
        ä»å­—æ®µè§„èŒƒåˆ—è¡¨ä¸­æå–é¡¶å±‚å­—æ®µï¼ˆè·¯å¾„ä¸­ä¸å« '.' çš„å­—æ®µï¼‰çš„æè¿°ã€‚
        è‹¥å­˜åœ¨å¤šä¸ªé¡¶å±‚å­—æ®µï¼ˆå¦‚ inference + context_clueï¼‰ï¼Œä¼˜å…ˆå–ç¬¬ä¸€ä¸ªéé€šé…ã€éåˆ—è¡¨é¡¹çš„ã€‚
        """
        for field_path, _, _, description in fields_spec:
            # è·³è¿‡å¸¦é€šé…ç¬¦çš„è·¯å¾„ï¼ˆå¦‚ participants.*.roleï¼‰
            if ".*." in field_path or field_path.startswith("*."):
                continue
            parts = field_path.split(".")
            if len(parts) == 1:
                # è¿™æ˜¯ä¸€ä¸ªé¡¶å±‚å­—æ®µï¼Œå¦‚ "participants", "inference", "context_clue"
                return description
        return None
